{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"neuralwork/arxiver\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier viewing\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Show the first 5 papers with title and abstract\n",
    "# print(df['markdown'].iloc[0])\n",
    "\n",
    "# df[['markdown']].to_csv('raw_paper_data/markdown_only.csv', index=False)\n",
    "type(df['markdown'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"# Image Completion via Dual-Path Cooperative Filtering\\\\n\\\\n###### Abstract\\\\n\\\\nGiven the recent advances with image-generating algorithms, deep image completion methods have made significant progress. However, state-of-art methods typically provide poor cross-scene generalization, and generated masked areas often contain blurry artifacts. Predictive filtering is a method for restoring images, which predicts the most effective kernels based on the input scene. Motivated by this approach, we address image completion as a filtering problem. Deep feature-level semantic filtering is introduced to fill in missing information, while preserving local structure and generating visually realistic content. In particular, a Dual-path Cooperative Filtering (DCF) model is proposed, where one path predicts dynamic kernels, and the other path extracts multi-level features by using Fast Fourier Convolution to yield semantically coherent reconstructions. Experiments on three challenging image completion datasets show that our proposed DCF outperforms state-of-art methods.\\\\n\\\\nPourya Shamsolmoali\\\\\\\\({}^{1}\\\\\\\\), Masoumeh Zareapoor\\\\\\\\({}^{2}\\\\\\\\), Eric Granger\\\\\\\\({}^{3}\\\\\\\\)\\\\\\\\({}^{1}\\\\\\\\)Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, China\\\\n\\\\n\\\\\\\\({}^{2}\\\\\\\\)School of Automation, Shanghai Jiao Tong University, China\\\\n\\\\n\\\\\\\\({}^{3}\\\\\\\\)Lab. d\\'imagerie, de vision et d\\'intelligence artificielle, Dept. of Systems Eng., ETS, Canada Image Completion, Image Inpainting, Deep Learning.\\\\n\\\\n## 1 Introduction\\\\n\\\\nThe objective of image completion (inpainting) is to recover images by reconstructing missing regions. Images with inpainted details must be visually and semantically consistent. Therefore, robust generation is required for inpainting methods. Generative adversarial networks (GANs) [2, 18] or auto-encoder networks [16, 20, 21] are generally used in current state-of-the-art models [10, 11, 19] to perform image completion. In these models, the input image is encoded into a latent space by generative network-based inpainting, which is then decoded to generate a new image. The quality of inpainting is entirely dependent on the data and training approach, since the procedure ignores priors (for example smoothness among nearby pixels or features). It should be noted that, unlike the generating task, image inpainting has its own unique challenges. First, image inpainting requires that the completed images be clean, high-quality, and natural. These constraints separate image completion from the synthesis tasks, which focuses only on naturalness. Second, missing regions may appear in different forms, and the backgrounds could be from various scenes. Given these constraints, it is important for the inpainting method to have a strong capacity to generalize across regions that are missing. Recent generative networks have made substantial progress in image completion, but they still have a long way to go before they can address the aforementioned problems.\\\\n\\\\nFor instance, RFRNet [7] uses feature reasoning on the auto-encoder architecture for the task of image inpainting. As shown in Fig. 1, RFRNet produces some artifacts in output images. JPGNet and MISF [5, 8] are proposed to address generative-based inpainting problems [7, 12, 15] by reducing artifacts using image-level predictive filtering. Indeed, image-level predictive filtering reconstructs pixels from neighbors, and filtering kernels are computed adaptively based on the inputs. JPGNet is therefore able to retrieve the local structure while eliminating artifacts. As seen in Fig. 1, JPGNet\\'s artifacts are more efficiently smoother than RFRNet\\'s. However, many details may be lost, and the actual structures are not reconstructed. LaMa [19] is a recent image inpainting approach that uses Fast Fourier Convolution (FFC) [3] inside their ResNet-based LaMa-Fourier model to address the lack of receptive field for producing repeated patterns in the missing areas. Previously, researchers struggled with global self-attention [22] and its computational complexity, and they were still unable to perform satisfactory recovery for repeated man-made structures as effectively as with LaMa. Nonetheless, as the missing regions get bigger and pass the object boundary, LaMa creates faded structures.\\\\n\\\\nFigure 1: Examples of an image completed with our DCF model compared to baseline methods on the Paris dataset. DCF generates high-fidelity and more realistic images.\\\\nIn [12], authors adopts LaMa as the base network, and can captures various types of missing information by utilizing additional types of masks. They use more damaged images in the training phase to improve robustness. However, such a training strategy is unproductive. Transformer-based approaches [20, 23] recently have attracted considerable interest, despite the fact that the structures can only be estimated within a low-resolution coarse image, and good textures cannot be produced beyond this point. Recent diffusion-based inpainting models [13, 17] have extended the limitations of generative models by using image information to sample the unmasked areas or use a score-based formulation to generate unconditional inpainted images, however, these approaches are not efficient in real-world applications.\\\\n\\\\nTo address this problem, we introduce a new neural network architecture that is motivated by the predictive filtering on adaptability and use large receptive field for producing repeating patterns. In particular, this paper makes two key contributions. First, semantic filtering is introduced to fill the missing image regions by expanding image-level filtering into a feature-level filtering. Second, a Dual-path Cooperative Filtering (DCF) model is introduced that integrates two semantically connected networks - a kernel prediction network, and a semantic image filtering network to enhance image details.\\\\n\\\\nThe semantic filtering network supplies multi-level features to the kernel prediction network, while the kernel prediction network provides dynamic kernels to the semantic filtering network. In addition, for efficient reuse of high-frequency features, FFC [3] residual blocks are utilized in the semantic filtering network to better synthesize the missing regions of an image, leading to improved performance on textures and structures. By linearly integrating neighboring pixels or features, DCF is capable of reconstructing them with a smooth prior across neighbors. Therefore, DCF utilizes both semantic and pixel-level filling for accurate inpainting. Following Fig. 1, the propose model produces high-fidelity and realistic images. Furthermore, in comparison with existing methods, our technique involves a dual-path network with a dynamic convolutional operation that modifies the convolution parameters based on different inputs, allowing to have strong generalization. A comprehensive set of experiments conducted on three challenging benchmark datasets (CelebA-HQ [6], Places2 [24], and Paris StreetView [4]), shows that our proposed method yields better qualitative and quantitative results than state-of-art methods.\\\\n\\\\n## 2 Methodology\\\\n\\\\nPredictive filtering is a popular method for restoring images that is often used for image denoising tasks [14]. We define image completion as pixel-wise predictive filtering:\\\\n\\\\n\\\\\\\\[I_{c}=I_{m}\\\\\\\\vartriangle T, \\\\\\\\tag{1}\\\\\\\\]\\\\n\\\\nin which \\\\\\\\(I_{c}\\\\\\\\in\\\\\\\\mathbb{R}^{(H\\\\\\\\times W\\\\\\\\times 3)}\\\\\\\\) represents a complete image, \\\\\\\\(I_{m}\\\\\\\\in\\\\\\\\mathbb{R}^{(H\\\\\\\\times W\\\\\\\\times 3)}\\\\\\\\) denotes the input image with missing regions from the ground truth image \\\\\\\\(I_{gr}\\\\\\\\in\\\\\\\\mathbb{R}^{(H\\\\\\\\times W\\\\\\\\times 3)}\\\\\\\\). The tensor \\\\\\\\(T\\\\\\\\in\\\\\\\\mathbb{R}^{(H\\\\\\\\times W\\\\\\\\times N^{2})}\\\\\\\\) has \\\\\\\\(HW\\\\\\\\) kernels for filtering each pixel and the pixel-wise filtering operation is indicated by the operation \\\\\\\\({}^{\\\\\\\\prime}\\\\\\\\vartriangle^{\\\\\\\\prime}\\\\\\\\). Rather than using image-level filtering, we perform the double-path feature-level filtering, to provides more context information. Our idea is that, even if a large portion of the image is destroyed, semantic information can be maintained. To accomplish semantic filtering, we initially use an auto-encoder network in which the encoder extracts features of the damaged image \\\\\\\\(I_{m}\\\\\\\\), and the decoder maps the extracted features to the complete image \\\\\\\\(I_{c}\\\\\\\\). Therefore, the encoder can be defined by:\\\\n\\\\n\\\\\\\\[f_{L}=\\\\\\\\rho(I_{m})=\\\\\\\\rho_{L}(...\\\\\\\\rho_{l}(...\\\\\\\\rho_{2}(\\\\\\\\rho_{1}(I_{m})))), \\\\\\\\tag{2}\\\\\\\\]\\\\n\\\\nin which \\\\\\\\(\\\\\\\\rho(.)\\\\\\\\) denotes the encoder while \\\\\\\\(f_{l}\\\\\\\\) represents the feature taken from the deeper layers (\\\\\\\\(l^{th}\\\\\\\\)), \\\\\\\\(f_{l}=\\\\\\\\rho_{l}(f_{l-1})\\\\\\\\). For instance, \\\\\\\\(f_{l}\\\\\\\\) shows the last layer\\'s result of \\\\\\\\(\\\\\\\\rho(.)\\\\\\\\).\\\\n\\\\nIn our encoder network, to create remarkable textures and semantic structures within the missing image regions, we adopt Fast Fourier Convolutional Residual Blocks (FFC-Res) [19]. The FFC-Res shown in Fig. 2 (b) has two FFC layers. The channel-wise Fast Fourier Transform (FFT) [1] is the core of the FFC layer [3] to provide a whole image-wide receptive field. As shown in Fig. 2 (c), the FFC layer divides channels into two branches: a) a local branch, which utilizes standard convolutions to capture spatial information, and b) a global branch, which employs a Spectral Transform module to analyze global structure and capture long-range context.\\\\n\\\\nFigure 2: Overview of the proposed architecture. (a) Our proposed DCF inpainting network with (b) FFC residual block to have a larger receptive field. (c) and (d) show the architecture of the FFC and Spectral Transform layers, respectively.\\\\nOutputs of the local and global branches are then combined. Two Fourier Units (FU) are used by the Spectral Transform layer (Fig. 2 (d)) in order to capture both global and semi-global features. The FU on the left represents the global context. In contrast, the Local Fourier Unit on the right side of the image takes in one-fourth of the channels and focuses on the semi-global image information. In a FU, the spatial structure is generally decomposed into image frequencies using a Real FFT2D operation, a frequency domain convolution operation, and ultimately recovering the structure via an Inverse FFT2D operation. Therefore, based on the encoder the network of our decoder is defined as:\\\\n\\\\n\\\\\\\\[I_{c}=\\\\\\\\rho^{-1}(f_{L}), \\\\\\\\tag{3}\\\\\\\\]\\\\n\\\\nin which \\\\\\\\(\\\\\\\\rho^{-1}(.)\\\\\\\\) denotes the decoder. Then, similar to image-level filtering, we perform semantic filtering on extracted features according to:\\\\n\\\\n\\\\\\\\[\\\\\\\\hat{f}_{l}[r]=\\\\\\\\sum_{s\\\\\\\\in\\\\\\\\mathcal{N}_{\\\\\\\\kappa}}T_{\\\\\\\\kappa}^{l}[s-r]f_{l}[s], \\\\\\\\tag{4}\\\\\\\\]\\\\n\\\\nin which \\\\\\\\(r\\\\\\\\) and \\\\\\\\(s\\\\\\\\) denote the image pixels\\' coordinates, whereas the \\\\\\\\(\\\\\\\\mathcal{N}_{\\\\\\\\kappa}\\\\\\\\) consist of \\\\\\\\(N^{2}\\\\\\\\) closest pixels. \\\\\\\\(T_{\\\\\\\\kappa}^{l}\\\\\\\\) signifies the kernel for filtering the \\\\\\\\(\\\\\\\\kappa^{th}\\\\\\\\) component of \\\\\\\\(T_{l}\\\\\\\\) through its neighbors \\\\\\\\(\\\\\\\\mathcal{N}_{\\\\\\\\kappa}\\\\\\\\). To incorporate every element-wise kernel, we use the matrix \\\\\\\\(T_{l}\\\\\\\\) as \\\\\\\\(T_{\\\\\\\\kappa}^{l}\\\\\\\\). Following this, Eq. (2) is modified by substituting \\\\\\\\(f_{l}\\\\\\\\) with \\\\\\\\(\\\\\\\\hat{f}_{l}\\\\\\\\). In addition, we use a predictive network to predict the kernels\\' behaviour in order to facilitate their adaptation for two different scenes.\\\\n\\\\n\\\\\\\\[T_{l}=\\\\\\\\varphi_{l}(I_{m}), \\\\\\\\tag{5}\\\\\\\\]\\\\n\\\\nin which \\\\\\\\(\\\\\\\\varphi_{l}(.)\\\\\\\\) denotes the predictive network to generate \\\\\\\\(T_{l}\\\\\\\\). In Fig. 2(a) and Table 2, we illustrate our image completion network which consist of \\\\\\\\(\\\\\\\\rho(.),\\\\\\\\rho^{-1},\\\\\\\\) and \\\\\\\\(\\\\\\\\varphi_{l}(.)\\\\\\\\). The proposed network is trained using the \\\\\\\\(L_{1}\\\\\\\\) loss, perceptual loss, adversarial loss, and style loss, similar to predictive filtering.\\\\n\\\\n## 3 Experiments\\\\n\\\\nIn this section, the performance of our DCF model is compared to state-of-the-art methods for image completion task. Experiments are carried out on three datasets, CelebA-HQ [6], Places2 [24], and Paris StreetView [4] at \\\\\\\\(256\\\\\\\\times 256\\\\\\\\) resolution images. With all datasets, we use the standard training and testing splits. In both training and testing we use the diverse irregular mask (20%-40% of images occupied by holes) given by PConv [9] and regular center mask datasets. The code is provided at _DCF_.\\\\n\\\\n**Performance Measures:** The structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and Frechet inception distance (FID) are used as the evaluation metrics.\\\\n\\\\n### Implementation Details\\\\n\\\\nOur proposed model\\'s framework is shown in Table 2.\\\\n\\\\n**Loss functions.** We follow [15] and train the networks using four loss functions, including \\\\\\\\(L_{1}\\\\\\\\) loss (\\\\\\\\(\\\\\\\\ell_{1}\\\\\\\\)), adversarial loss (\\\\\\\\(\\\\\\\\ell_{A}\\\\\\\\)), style loss (\\\\\\\\(\\\\\\\\ell_{S}\\\\\\\\)), and perceptual loss (\\\\\\\\(\\\\\\\\ell_{P}\\\\\\\\)), to obtain images with excellent fidelity in terms of quality as well as semantic levels. Therefore, we can write the reconstruction loss (\\\\\\\\(\\\\\\\\ell_{R}\\\\\\\\)) as:\\\\n\\\\n\\\\\\\\[\\\\\\\\ell_{R}=\\\\\\\\lambda_{1}\\\\\\\\ell_{1}+\\\\\\\\lambda_{a}\\\\\\\\ell_{A}+\\\\\\\\lambda_{p}\\\\\\\\ell_{P}+\\\\\\\\lambda_ {s}\\\\\\\\ell_{S}. \\\\\\\\tag{6}\\\\\\\\]\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l|c|c||c|c} \\\\\\\\hline \\\\\\\\multicolumn{4}{c||}{Feature extracting network} & \\\\\\\\multicolumn{2}{c}{Predicting network} \\\\\\\\\\\\\\\\ \\\\\\\\hline Layer & In. & Out/size & In. & Out/size \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline conv(7,3,64) & \\\\\\\\(I_{m}\\\\\\\\) & \\\\\\\\(f_{1}\\\\\\\\) / 256 & \\\\\\\\(I_{m}\\\\\\\\) & \\\\\\\\(e_{1}\\\\\\\\) / 256 \\\\\\\\\\\\\\\\ conv(4,64,128) & \\\\\\\\(f_{1}\\\\\\\\) & \\\\\\\\(f_{2}\\\\\\\\) / 128 & \\\\\\\\(e_{1}\\\\\\\\) & \\\\\\\\(e_{2}\\\\\\\\) / 128 \\\\\\\\\\\\\\\\ pooling & \\\\\\\\(f_{2}\\\\\\\\) & \\\\\\\\(f_{2}\\\\\\\\) / 64 & \\\\\\\\(e_{2}\\\\\\\\) & \\\\\\\\(e_{2}\\\\\\\\) / 64 \\\\\\\\\\\\\\\\ conv(4,128,256) & \\\\\\\\(f_{2}\\\\\\\\) & \\\\\\\\(f_{3}\\\\\\\\) / 64 & \\\\\\\\([f_{2}^{\\\\\\\\prime},e_{2}^{\\\\\\\\prime}]\\\\\\\\) & \\\\\\\\(e_{3}\\\\\\\\) / 64 \\\\\\\\\\\\\\\\ \\\\\\\\(f_{3}\\\\\\\\) \\\\\\\\(\\\\\\\\\\\\nin which \\\\\\\\(\\\\\\\\lambda_{1}=1\\\\\\\\), \\\\\\\\(\\\\\\\\lambda_{a}=\\\\\\\\lambda_{p}=0.1\\\\\\\\), and \\\\\\\\(\\\\\\\\lambda_{s}=250\\\\\\\\). More details on the loss functions can be found in [15].\\\\n\\\\n**Training setting.** We use Adam as the optimizer with the learning rate of \\\\\\\\(1e-4\\\\\\\\) and the standard values for its hyperparameters. The network is trained for 500k iterations and the batch size is 8. The experiments are conducted on the same machine with two RTX-3090 GPUs.\\\\n\\\\n### Comparisons to the Baselines\\\\n\\\\n**Qualitative Results.** The proposed DCF model is compared to relevant baselines such as RFRNet [7], JPGNet [5], and LaMa [19]. Fig. 3 and Fig. 4 show the results for the Places2 and CelebA-HQ datasets respectively. In comparison to JPGNet, our model preserves substantially better recurrent textures, as shown in Fig. 3. Since JPGNet lacks attention-related modules, high-frequency features cannot be successfully utilized due to the limited receptive field. Using FFC modules, our model expanded the receptive field and successfully project source textures on newly generated structures. Furthermore, our model generates superior object boundary and structural data compared to LaMa. Large missing regions over larger pixel ranges limit LaMa from hallucinating adequate structural information. However, ours uses the advantages of the coarse-to-fine generator to generate a more precise object with better boundary. Fig. 4 shows more qualitative evidence. While testing on facial images, RFRNet and LaMa produce faded forehead hairs and these models are not robust enough. The results of our model, nevertheless, have more realistic textures and plausible structures, such as forehead form and fine-grained hair.\\\\n\\\\n**Quantitative Results.** On three datasets, we compare our proposed model with other inpainting models. The results shown in Table 2 lead to the following conclusions: 1) Compared to other approaches, our method outperforms them in terms of PSNR, SSIM, and FID scores for the most of datasets and mask types. Specifically, we achieve 9% higher PNSR on the Places2 dataset\\'s irregular masks than RFRNet. It indicates that our model has advantages over existing methods. 2) We observe similar results while analyzing the FID. On the CelebA-HQ dataset, our method achieves 2.5% relative lower FID than LaMa under the center mask. This result indicates our method\\'s remarkable success in perceptual restoration. 3) The consistent advantages over several datasets and mask types illustrate that our model is highly generalizable.\\\\n\\\\n## 4 Conclusion\\\\n\\\\nDual-path cooperative filtering (DCF) was proposed in this paper for high-fidelity image inpainting. For predictive filtering at the image and deep feature levels, a predictive network is proposed. In particular, image-level filtering is used for details recovery, whereas deep feature-level filtering is used for semantic information completion. Moreover, in the image-level filtering the FFC residual blocks is adopted to recover semantic information and resulting in high-fidelity outputs. The experimental results demonstrate our model outperforms the state-of-art inpainting approaches.\\\\n\\\\n#### Acknowledgments\\\\n\\\\nThis research was supported in part by NSFC China. The corresponding author is Masoumeh Zareapoor.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l|l|c c|c c|c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{3}{*}{} & \\\\\\\\multirow{3}{*}{Method} & \\\\\\\\multicolumn{3}{c|}{CelebA-HQ} & \\\\\\\\multicolumn{3}{c|}{Places2} & \\\\\\\\multicolumn{3}{c}{Paris StreetView} \\\\\\\\\\\\\\\\ \\\\\\\\cline{3-8}  & & Irregular & Center & Irregular & Center & Irregular & Center \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\multirow{8}{*}{PSNR\\\\\\\\(\\\\\\\\uparrow\\\\\\\\)} & RFRNet [7] & 26.63 & 21.32 & 22.58 & 18.27 & 23.81 & 19.26 \\\\\\\\\\\\\\\\  & JPGNet [5] & 25.54 & 22.71 & 23.93 & 19.22 & 24.79 & 20.63 \\\\\\\\\\\\\\\\  & TFill [23] & 26.84 & 23.65 & 24.32 & 20.49 & 25.46 & 21.85 \\\\\\\\\\\\\\\\  & LaMa [19] & 27.31 & 24.18 & **25.27** & 21.67 & 25.84 & 22.59 \\\\\\\\\\\\\\\\  & GLaMa [12] & 28.17 & 25.13 & 25.08 & 21.83 & 26.23 & 22.87 \\\\\\\\\\\\\\\\  & DCF (ours) & **28.34** & **25.62** & 25.19 & **22.30** & **26.57** & **23.41** \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\multirow{8}{*}{SSIM\\\\\\\\(\\\\\\\\uparrow\\\\\\\\)} & RFRNet [7] & 0.934 & 0.912 & 0.819 & 0.801 & 0.862 & 0.849 \\\\\\\\\\\\\\\\  & JPGNet [5] & 0.927 & 0.904 & 0.825 & 0.812 & 0.873 & 0.857 \\\\\\\\\\\\\\\\  & TFill [23] & 0.933 & 0.907 & 0.826 & 0.814 & 0.870 & 0.857 \\\\\\\\\\\\\\\\  & LaMa [19] & 0.939 & 0.911 & 0.829 & 0.816 & 0.871 & 0.856 \\\\\\\\\\\\\\\\  & GLaMa [12] & 0.941 & 0.925 & **0.833** & 0.817 & 0.872 & 0.858 \\\\\\\\\\\\\\\\  & DCF (ours) & **0.943** & **0.928** & 0.832 & **0.819** & **0.876** & **0.861** \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\multirow{8}{*}{FID\\\\\\\\(\\\\\\\\downarrow\\\\\\\\)} & RFRNet [7] & 17.07 & 17.83 & 15.56 & 16.47 & 40.23 & 41.08 \\\\\\\\\\\\\\\\  & JPGNet [5] & 13.92 & 15.71 & 15.14 & 16.23 & 37.61 & 39.24 \\\\\\\\\\\\\\\\  & TFill [23] & 13.18 & 13.87 & 15.48 & 16.24 & 33.29 & 34.41 \\\\\\\\\\\\\\\\  & LaMa [19] & 11.28 & 12.95 & 14.73 & 15.46 & 32.30 & 33.26 \\\\\\\\\\\\\\\\  & GLaMa [12] & 11.21 & 12.91 & 14.70 & 15.35 & 32.12 & 33.07 \\\\\\\\\\\\\\\\ \\\\\\\\cline{2-8}  & DCF w.o. Sem-Fil & 14.34 & 15.24 & 17.56 & 18.11 & 42.57 & 44.38 \\\\\\\\\\\\\\\\  & DCF w.o. FFC & 13.52 & 14.26 & 15.83 & 16.98 & 40.54 & 41.62 \\\\\\\\\\\\\\\\  & DCF (ours) & **11.13** & **12.63** & **14.52** & **15.09** & **31.96** & **32.85** \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 2: Ablation study and quantitative comparison of our proposed and state-of-art methods on center and free form masked images from the CelebA-HQ, Places2, and Paris StreetView datasets.\"\\n \\'# High Sensitivity Beamformed Observations of the Crab Pulsar\\\\\\'s Radio Emission\\\\n\\\\n###### Abstract\\\\n\\\\nWe analyzed four epochs of beamformed EVN data of the Crab Pulsar at \\\\\\\\(1658.49\\\\\\\\rm\\\\\\\\,MHz\\\\\\\\). With the high sensitivity resulting from resolving out the Crab Nebula, we are able to detect even the faint high-frequency components in the folded profile. We also detect a total of \\\\\\\\(65951\\\\\\\\) giant pulses, which we use to investigate the rates, fluence, phase, and arrival time distributions. We find that for the main pulse component, our giant pulses represent about 80% of the total flux. This suggests we have a nearly complete giant pulse energy distribution, although it is not obvious how the observed distribution could be extended to cover the remaining 20% of the flux without invoking large numbers of faint bursts for every rotation. Looking at the difference in arrival time between subsequent bursts in single rotations, we confirm that the likelihood of finding giant pulses close to each other is increased beyond that expected for randomly occurring bursts - some giant pulses consist of causally related microbursts, with typical separations of \\\\\\\\(\\\\\\\\sim 30\\\\\\\\rm\\\\\\\\ \\\\\\\\mu s\\\\\\\\) - but also find evidence that at separations \\\\\\\\(\\\\\\\\gtrsim\\\\\\\\!100\\\\\\\\rm\\\\\\\\ \\\\\\\\mu s\\\\\\\\) the likelihood of finding another giant pulse is suppressed. In addition, our high sensitivity enabled us to detect weak echo features in the brightest pulses (at \\\\\\\\(\\\\\\\\sim\\\\\\\\!0.4\\\\\\\\%\\\\\\\\) of the peak giant pulse flux), which are delayed by up to \\\\\\\\(\\\\\\\\sim\\\\\\\\!300\\\\\\\\rm\\\\\\\\ \\\\\\\\mu s\\\\\\\\).\\\\n\\\\nPulsars (1306) -- Radio bursts (1339) -- Very long baseline interferometry (1769) 0000-0002-4818-2886]Rebecca Lin\\\\n\\\\n0000-0002-4882-0886]Marten H. van Kerkwijk\\\\n\\\\n0000-0002-4882-0886]D.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A. Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.\\\\nInvestigation of the emission from the Crab Pulsar is complicated by propagation effects along the line of sight, especially at lower frequencies, \\\\\\\\(\\\\\\\\lesssim 2\\\\\\\\ \\\\\\\\mathrm{GHz}\\\\\\\\). While dispersion can be removed using coherent de-dispersion (either during recording, or afterwards with baseband data), scattering effects are difficult to remove. This includes echoes due to propagation in the Crab Nebula itself, which sometimes are bright and obvious (Backer et al., 2000; Lyne et al., 2001), but can also be quite faint (Driessen et al., 2019), making it difficult to disentangle them from microbursts without having a good pulse sample to look for repeating structure.\\\\n\\\\nAnother complication in studying the emission of the Crab Pulsar is the radio-bright nebula in which the pulsar resides. This contributes noise and hence many previous studies relied on long integrations to observe both the weaker pulse components and echoes in the average profile. But the contribution to the noise can be reduced by resolving the nebula, using large dishes or arrays, such as the VLA, Arecibo, and Westerbork (Moffett & Hankins, 1996; Cordes et al., 2004; Karuppusamy et al., 2010; Lewandowska et al., 2022).\\\\n\\\\nIn this paper, we use the European VLBI Network (EVN) to resolve out the Crab Nebula and obtain high sensitivity data. In Section 2, we describe our observations and data reduction, and in Section 3, we present the resulting pulse profiles and the components that are detectable at our high sensitivity. We turn to an analysis of GPs in Section 4, investigating their rates, fluence, phase, and arrival time distributions, as well as weak echoes seen in the brightest GPs. We summarize our findings in Section 5.\\\\n\\\\n## 2 Observations and Data Reduction\\\\n\\\\nWe analyze observations of the Crab Pulsar taken by the EVN, projects EK036 A-D, at four epochs between 2015 Oct and 2017 May (see Table 1). Throughout these observations, calibrator sources were also observed resulting in breaks in our data. While many dishes participated in these observations, for our analysis we only use telescope data that had relatively clean signals across the frequency range of \\\\\\\\(1594.49-1722.49\\\\\\\\ \\\\\\\\mathrm{MHz}\\\\\\\\) in both circular polarizations. At each single dish, real-sampled data were recorded in either 2 bit MARK 5B or VDIF format1, covering the frequency range in either eight contiguous \\\\\\\\(16\\\\\\\\ \\\\\\\\mathrm{MHz}\\\\\\\\) wide bands or four contiguous \\\\\\\\(32\\\\\\\\ \\\\\\\\mathrm{MHz}\\\\\\\\) wide bands.\\\\n\\\\nFootnote 1: For specifications of MARK5B and VDIF, see [https://www.haystack.mit.edu/haystack-memo-series/mark-5-memos/](https://www.haystack.mit.edu/haystack-memo-series/mark-5-memos/) and [https://vlbi.org/wp-content/uploads/2019/03/VDIF_specification_Release_1.1.1.pdf](https://vlbi.org/wp-content/uploads/2019/03/VDIF_specification_Release_1.1.1.pdf), respectively.\\\\n\\\\nFor these datasets, single dish data were processed and then combined coherently to form a tied-array beam as described in Lin et al. (2023). The resulting RFI-removed, normalized, de-dispersed (using dispersion measures (DMs) listed in Table 1), parallactic angle corrected, and phased baseband data were squared to form intensity data. As in Lin et al. (2023), we estimate the system equivalent flux density (SEFD) for the phased EVN array as \\\\\\\\((S_{\\\\\\\\text{CN}}+\\\\\\\\langle S_{\\\\\\\\text{tel}}\\\\\\\\rangle)/N_{\\\\\\\\text{tel}}\\\\\\\\approx 140-160\\\\\\\\ \\\\\\\\mathrm{ Jy}\\\\\\\\), where \\\\\\\\(S_{\\\\\\\\text{CN}}\\\\\\\\approx 833\\\\\\\\ \\\\\\\\mathrm{Jy}\\\\\\\\) is the SEFD of the Crab Nebula at our observing frequency (Bietenholz et al., 1997), \\\\\\\\(\\\\\\\\langle S_{\\\\\\\\text{tel}}\\\\\\\\rangle\\\\\\\\simeq 300\\\\\\\\ \\\\\\\\mathrm{Jy}\\\\\\\\) is the average nominal SEFD of the telescopes2 and \\\\\\\\(N_{\\\\\\\\text{tel}}=7\\\\\\\\ \\\\\\\\mathrm{or}\\\\\\\\ 8\\\\\\\\) is the number of telescopes used. By combining the single dishes into a synthesized beam, we resolve out the radio-bright Crab Nebula and increase our sensitivity, thus allowing us to investigate the weaker radio emission of the Crab Pulsar.\\\\n\\\\nFootnote 2: [http://old.evlbi.org/cgi-bin/EVNcalc](http://old.evlbi.org/cgi-bin/EVNcalc).\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{c c c c c c c c c c} \\\\\\\\hline \\\\\\\\hline Observation & & \\\\\\\\(t_{\\\\\\\\text{sep}}\\\\\\\\)a  & & & DMc  & & & Giant Pulsesd  & & \\\\\\\\\\\\\\\\  & Date & (h) & Telescopes usedb  & & & & Giant Pulsesd  & \\\\\\\\\\\\\\\\  & Date & (h) & Telescopes usedb  & & & & & & \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 1: Observation and Giant Pulse Log.\\\\n## 3 Pulse Profiles\\\\n\\\\nFor each of the phased EVN datasets, we create folded pulse profiles using polyco files generated with tempo2(Hobbs and Edwards, 2012) from the monthly Jodrell Bank Crab Pulsar ephemerides3(Lyne et al., 1993) and DM from Table 1. We averaged over all frequencies and used \\\\\\\\(512\\\\\\\\) phase bins, rotating in phase such that the MP is at phase \\\\\\\\(0\\\\\\\\). We show the resulting profiles in Figure 1, with each profile scaled to its maximum to ease comparison. With our high sensitivity, we can see all five pulse components expected from the multifrequency overview of Hankins et al. (2015), corresponding to the LFC, MP, IP, HFC1 and HFC2 (with the latter two detected at \\\\\\\\(\\\\\\\\sim\\\\\\\\!1.66\\\\\\\\ \\\\\\\\mathrm{GHz}\\\\\\\\) for the first time).\\\\n\\\\nFootnote 3: [http://www.jb.man.ac.uk/~pulsar/crab.html](http://www.jb.man.ac.uk/~pulsar/crab.html).\\\\n\\\\nWe fit the pulse components in the EKO36 datasets with five Gaussians to look for possible changes, both between our epochs and relative to the compilation from Hankins et al. (2015). Our fitted parameters are presented in Table 2, together with the values inferred from Hankins et al. (2015). One sees that the results for our four observations are all consistent. At \\\\\\\\(1.4\\\\\\\\ \\\\\\\\mathrm{GHz}\\\\\\\\), Lyne et al. (2013) found that the separations between the MP and IP and between the MP and LFC increase at a rate of \\\\\\\\(0\\\\\\\\fdg 5\\\\\\\\pm 0\\\\\\\\fdg 2\\\\\\\\) per century and \\\\\\\\(11\\\\\\\\arcdeg\\\\\\\\pm 2\\\\\\\\arcdeg\\\\\\\\) per century, respectively. Using these rates, we expect pulse phase changes for the IP and LFC of \\\\\\\\(\\\\\\\\sim\\\\\\\\!0\\\\\\\\fdg 008\\\\\\\\) and \\\\\\\\(\\\\\\\\sim\\\\\\\\!0\\\\\\\\fdg 17\\\\\\\\), respectively, which are not detectable within our uncertainties.\\\\n\\\\nComparing with Hankins et al. (2015), we find good agreement in pulse phase for all components (though now we do need to take into account the drift in pulse phase). We noticed, however, that while the widths of our LFC, HFC1 and HFC2 are consistent with those given by Hankins et al. (2015), the widths of the MP and IP seem smaller, even if they are still within the nominal, rather large uncertainties of Hankins et al. (2015). Looking in more detail at their Figure 3 with measurements, one sees considerable scatter for the MP and IP, even though those strong, narrow peaks should be the easiest to measure. This might suggest that some profiles were slightly smeared (e.g., because the data were not dedispersed to exactly the right DM, which is known to vary for the Crab Pulsar, or because of changes in scattering timescale at lower frequencies, see McKee et al., 2018). For a comparison with recent data, we estimated widths from the \\\\\\\\(2-4\\\\\\\\) and \\\\\\\\(4-6\\\\\\\\ \\\\\\\\mathrm{GHz}\\\\\\\\) pulse profiles in Figure 1 of Lewandowska et al. (2022), which were taken using the VLA in D configuration to resolve out the Crab Nebula and thus have high signal-to-noise ratio; we find these are all consistent with ours.\\\\n\\\\nFigure 1: Folded pulse profile of the Crab Pulsar at \\\\\\\\(1658.49\\\\\\\\ \\\\\\\\mathrm{MHz}\\\\\\\\) from EK036 observations in \\\\\\\\(512\\\\\\\\) phase bins centered on the MP. At this frequency, 5 components: LFC, MP, IP, HFC1 and HFC2 are visible. In the left panel, the profiles are normalized to their peak MP component. As the HFC1 and HFC2 components (indicated by arrows) are very faint, we show the grey region of the left panel zoomed in by a factor of \\\\\\\\(15\\\\\\\\) in the right panel, with vertical lines marking the peak of these components.\\\\nAt lower frequencies, the pulse profiles often show echo features (e.g., Driessen et al., 2019). At our frequencies, those are expected to be too weak at delays where they might be seen in the folded pulse profile, and indeed we see none. However, at frequencies like ours, echoes can still be seen in individual pulses. For instance, at \\\\\\\\(1.4\\\\\\\\;\\\\\\\\mathrm{GHz}\\\\\\\\), Crossley et al. (2004) saw that individual bright pulses all had an echo delayed at \\\\\\\\(\\\\\\\\sim\\\\\\\\!50\\\\\\\\;\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\) (which had no counterpart at \\\\\\\\(4.9\\\\\\\\;\\\\\\\\mathrm{GHz}\\\\\\\\)). From aligning GPs before stacking them in our datasets, Lin et al. (2023) also saw hints of echo features within \\\\\\\\(\\\\\\\\sim\\\\\\\\!25\\\\\\\\;\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\) of the peaks of GPs in EK036 B and D. In Section 4.6, we confirm echoes in our data using a more careful analysis, finding that for EK036 D faint echoes are visible out to to \\\\\\\\(\\\\\\\\sim\\\\\\\\!300\\\\\\\\;\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\).\\\\n\\\\n## 4 Giant Pulses\\\\n\\\\n### Search\\\\n\\\\nIn Lin et al. (2023), we searched for GPs by flagging peaks above \\\\\\\\(8\\\\\\\\sigma\\\\\\\\) in a \\\\\\\\(16\\\\\\\\;\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\) wide running average of the intensity time stream. While we reliably found GPs, the long time window meant we could not distinguish between bursts arriving in quick succession within that time window. Hence, the previous technique was unsuitable for one of our goals, of measuring arrival time differences between bursts, including between the microbursts that GPs sometimes are composed of. Below, we describe a revised technique, which allows us to more reliably identify multiple bursts (see Figure 2). Unsurprisingly, with our new technique we detected more multiple bursts than we had previously, as can be seen by comparing numbers listed in Section 6.3 of Lin et al. 2023) with those in Table 3.\\\\n\\\\nFor every pulsar period in the EK036 dataset, we take \\\\\\\\(2.0\\\\\\\\;\\\\\\\\mathrm{ms}\\\\\\\\) snippets of baseband data centered at the MP and\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l l l l l} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{1}{c}{\\\\n\\\\\\\\begin{tabular}{c} Pulse \\\\\\\\\\\\\\\\ Comp. \\\\\\\\\\\\\\\\ \\\\\\\\end{tabular} } & Obs./ & Amplitude & Pulse Phase & FWHM \\\\\\\\\\\\\\\\  & Ref. & (\\\\\\\\%) & (deg.) & (deg.) \\\\\\\\\\\\\\\\ \\\\\\\\hline LFC\\\\\\\\(\\\\\\\\dots\\\\\\\\) & A & 3.6(3) & \\\\\\\\(-38.0(3)\\\\\\\\) & 7.5(6) \\\\\\\\\\\\\\\\  & B & 3.35(17) & \\\\\\\\(-37.67(19)\\\\\\\\) & 7.7(4) \\\\\\\\\\\\\\\\  & C & 3.7(2) & \\\\\\\\(-37.2(3)\\\\\\\\) & 7.7(6) \\\\\\\\\\\\\\\\  & D & 3.9(2) & \\\\\\\\(-37.8(2)\\\\\\\\) & 8.1(5) \\\\\\\\\\\\\\\\  & H15 & \\\\\\\\(\\\\\\\\dots\\\\\\\\) & \\\\\\\\(-35.78(14)\\\\\\\\) & 7.2(12) \\\\\\\\\\\\\\\\ MP \\\\\\\\(\\\\\\\\dots\\\\\\\\) & A & & & 2.786(11) \\\\\\\\\\\\\\\\  & B & & & 2.708(7) \\\\\\\\\\\\\\\\  & C & & & 2.756(11) \\\\\\\\\\\\\\\\  & D & & & 2.836(9) \\\\\\\\\\\\\\\\  & H15 & & & 3.9(11) \\\\\\\\\\\\\\\\ IP\\\\\\\\(\\\\\\\\dots\\\\\\\\) & A & 15.2(4) & 145.38(4) & 3.48(10) \\\\\\\\\\\\\\\\  & B & 15.2(2) & 145.28(3) & 3.59(7) \\\\\\\\\\\\\\\\  & C & 15.3(4) & 145.25(4) & 3.46(10) \\\\\\\\\\\\\\\\  & D & 14.4(3) & 145.28(4) & 3.59(8) \\\\\\\\\\\\\\\\  & H15 & \\\\\\\\(\\\\\\\\dots\\\\\\\\) & 145.25(4) & 5.4(11) \\\\\\\\\\\\\\\\ HFC1\\\\\\\\(\\\\\\\\dots\\\\\\\\) & A & 0.58(13) & 203(3) & 28(7) \\\\\\\\\\\\\\\\  & B & 0.88(9) & 198.4(13) & 25(3) \\\\\\\\\\\\\\\\  & C & 0.68(12) & 194(3) & 34(7) \\\\\\\\\\\\\\\\  & D & 0.94(11) & 196.2(15) & 36(5) \\\\\\\\\\\\\\\\  & H15 & \\\\\\\\(\\\\\\\\dots\\\\\\\\) & 198.2(8) & 25(5) \\\\\\\\\\\\\\\\ HFC2\\\\\\\\(\\\\\\\\dots\\\\\\\\) & A & 1.5(2) & 259.7(8) & 11.8(19) \\\\\\\\\\\\\\\\  & B & 1.19(14) & 259.2(7) & 11.7(16) \\\\\\\\\\\\\\\\  & C & 1.23(19) & 257.7(9) & 12(2) \\\\\\\\\\\\\\\\  & D & 1.51(15) & 259.8(7) & 14.8(16) \\\\\\\\\\\\\\\\  & H15 & \\\\\\\\(\\\\\\\\dots\\\\\\\\) & 259.1(4) & 11.6(12) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\end{tabular} Note. –Amplitudes and phases are relative to the MP. H15 refers to Hankins et al. (2015), and corresponding values are from evaluating the fits presented in his Tables 2 and 3 at our central observing frequency of \\\\\\\\(1658.49\\\\\\\\;\\\\\\\\mathrm{MHz}\\\\\\\\). The phases for the LFC and IP have been extrapolated to MJD 57607 (midway between EK036 A and D) using \\\\\\\\(d\\\\\\\\phi/dt\\\\\\\\) values from Lyne et al. (2013). Numbers in parentheses are \\\\\\\\(1\\\\\\\\sigma\\\\\\\\) uncertainties in the last digit.\\\\n\\\\n\\\\\\\\end{table}\\\\nTable 2: Properties of the Pulse Profile Components.\\\\n\\\\nFigure 2: Sample MP pulse rotations with GPs as detected by our algorithm (see Section 4.1 for details), shown at a time resolution of \\\\\\\\(1.25\\\\\\\\;\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\). _Top_: Single pulse with scattering tail. _Middle_: Two pulses, each with their own scattering tail. _Bottom_: A profile showing the difficulties inherent in classifying pulses: our algorithm found three pulses, but if another algorithm were to classify this as two or four pulses, that would also seem reasonable.\\\\nIP component phase windows (roughly \\\\\\\\(2\\\\\\\\) times the size of the pulse component determined from the folded pulse profile) and create pulse intensity stacks for each component4. We average these stack across the eight frequency bands and bin over 10 time samples, or \\\\\\\\(0.625~{}\\\\\\\\mu\\\\\\\\)s, a value chosen to be large enough for a reliable GP detection yet well less than the scattering timescale of \\\\\\\\(\\\\\\\\sim\\\\\\\\)\\\\\\\\(5~{}\\\\\\\\mu\\\\\\\\)s during these observations (Lin et al., 2023). To detect GPs, we first subtract the off-pulse region (determined from the \\\\\\\\(0.5~{}\\\\\\\\mathrm{ms}\\\\\\\\) region on either side of each pulse stack), then filter with a uniform filter of size \\\\\\\\(5\\\\\\\\) (\\\\\\\\(3.125~{}\\\\\\\\mu\\\\\\\\)s), and finally record all samples above a detection threshold of \\\\\\\\(5\\\\\\\\sigma\\\\\\\\).\\\\n\\\\nFootnote 4: We only search for GPs inside these windows since Lin et al. (2023) found none outside for the same dataset.\\\\n\\\\nTo turn these sets of above-the-noise locations into detections of individual GPs, we use the following three-step process5. First, we connect detections within \\\\\\\\(8\\\\\\\\) samples (\\\\\\\\(5~{}\\\\\\\\mu\\\\\\\\)s, i.e., of order the scattering time), since those are likely related. Second, we remove detections spanning \\\\\\\\(4\\\\\\\\) samples (\\\\\\\\(2.5~{}\\\\\\\\mu\\\\\\\\)s) or less, since these are likely spurious. Third, we increase the width of a detection by \\\\\\\\(4\\\\\\\\) samples (\\\\\\\\(2.5~{}\\\\\\\\mu\\\\\\\\)s) on either side, mostly to ensure that if we integrate over the mask, we will capture most of the flux independent of pulse strength. With this procedure, the minimum final pulse width is \\\\\\\\(8.125~{}\\\\\\\\mu\\\\\\\\)s, slightly larger than the scattering timescale, and we confidently detect pulses above a threshold of \\\\\\\\(\\\\\\\\sim\\\\\\\\)\\\\\\\\(0.15~{}\\\\\\\\mathrm{kJy}~{}\\\\\\\\mu\\\\\\\\)s. The brightest GP we detect has a fluence of \\\\\\\\(\\\\\\\\sim 560~{}\\\\\\\\mathrm{kJy}~{}\\\\\\\\mu\\\\\\\\)s. With our relatively high initial detection threshold, we do not find any GPs outside our pulse windows, suggesting that we have no false detections in our sample. Nevertheless, as can be seen from the overall pulse statistics in Table 1, we find many GPs, about \\\\\\\\(2-3\\\\\\\\) per second or about one for every dozen pulsar rotations.\\\\n\\\\nFootnote 5: Using the binary_closing, binary_opening and binary_dilation functions, respectively, from scipy’s multidimensional image processing functions (Virtanen et al., 2020).\\\\n\\\\nIn some pulse rotations, we detect more than one distinct GP, where \"distinct\" means that the pulse is separated by at least \\\\\\\\(5~{}\\\\\\\\mu\\\\\\\\)s (roughly the scattering timescale) from another pulse at our detection threshold. Here, we note that whether or not a GP is detected as single or multiple depends on the detection threshold: a GP classified as a single one at our threshold might be classified as separated at a higher threshold if it has two bright peaks with some flux in between (e.g., because the scattering tail of the first peak overlaps with the start of the next one, or a weaker burst fills in the space in between). This dependence on detection threshold may explain why Bhat et al. (2008) found no pulses wider than \\\\\\\\(10~{}\\\\\\\\mu\\\\\\\\)s, as they took a high detection cutoff, of \\\\\\\\(3~{}\\\\\\\\mathrm{kJy}~{}\\\\\\\\mu\\\\\\\\)s. This kind of arbitrariness seems unavoidable given the variety in pulse shapes that we see; it often is a rather subjective decision on what to take as a single bursts. To give a sense, we show in Figure 2 an example of a pulse rotation with a single burst as well as two examples of rotations with multiple bursts. In Section 4.5, we estimate the fraction of multiple bursts that is causally related from the statistics of pulse separations.\\\\n\\\\n### Rates\\\\n\\\\nWith the high sensitivity of the phased EVN array, we detected a total of \\\\\\\\(65951\\\\\\\\) GPs over \\\\\\\\(7.32~{}\\\\\\\\mathrm{hr}\\\\\\\\), implying an average detection rate of \\\\\\\\(2.5~{}\\\\\\\\mathrm{s}^{-1}\\\\\\\\). From Table 1, one sees that the rates are not the same for each epoch. Comparable detection rates are seen for both MP and IP GPs in EK036 A and C, but those are about a factor \\\\\\\\(2\\\\\\\\) smaller than the rates for EK036 B and D (which are comparable to each other).\\\\n\\\\nSimilar changes in detection rate were found for bright pulses by Lundgren et al. (1995) at \\\\\\\\(800~{}\\\\\\\\mathrm{MHz}\\\\\\\\), Bera & Chengalur (2019) at \\\\\\\\(1330~{}\\\\\\\\mathrm{GHz}\\\\\\\\) and by Kazantsev et al. (2019) at \\\\\\\\(111~{}\\\\\\\\mathrm{MHz}\\\\\\\\). Lundgren et al. (1995) suggests that almost\\\\n\\\\nFigure 3: GP pulse detection rates in each EK036 observation. Times when the telescope was not observing the Crab Pulsar are shaded grey. The MP (blue) and IP (orange) detection rates appear to scale together and are relatively constant across each observation.\\\\ncertainly, these are due to changes in the scattering screen, which are known to cause changes in the scattering time on similar timescales and are expected to cause changes in magnification as well. To verify that there are no variations at shorter timescales, we calculated rates at roughly \\\\\\\\(5\\\\\\\\,\\\\\\\\mathrm{min}\\\\\\\\) intervals. As can be seen in Figure 3, we find that in a given epoch, the rates are indeed steady.\\\\n\\\\n### Fluences\\\\n\\\\nThe fluence distribution of the Crab Pulsar\\\\\\'s GPs is typically described by power-law approximations to the reverse cumulative distribution,\\\\n\\\\n\\\\\\\\[N_{\\\\\\\\mathrm{GP}}(E>E_{0})=CE_{0}^{\\\\\\\\alpha}, \\\\\\\\tag{1}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\alpha\\\\\\\\) is the power-law index, \\\\\\\\(C\\\\\\\\) a proportionality constant, and \\\\\\\\(E_{0}\\\\\\\\) the GP fluence such that \\\\\\\\(N_{\\\\\\\\mathrm{GP}}(E>E_{0})\\\\\\\\) is the occurrence rate of GPs above \\\\\\\\(E_{0}\\\\\\\\). For our data, one sees in Figure 4, that for all observations the distributions indeed appear power-law like at high fluence, with \\\\\\\\(\\\\\\\\alpha\\\\\\\\approx-2.0\\\\\\\\) and \\\\\\\\(-1.6\\\\\\\\) for MP and IP, respectively. These values are roughly consistent with values found at similar frequencies: e.g., Popov & Stappers (2007) find \\\\\\\\(-1.7\\\\\\\\) to \\\\\\\\(-3.2\\\\\\\\) for MP GPs and \\\\\\\\(-1.6\\\\\\\\) for IP GPs at \\\\\\\\(1197\\\\\\\\,\\\\\\\\mathrm{MHz}\\\\\\\\), and Majid et al. (2011) finds \\\\\\\\(\\\\\\\\alpha=-1.9\\\\\\\\) for the combined MP and IP distribution at \\\\\\\\(1664\\\\\\\\,\\\\\\\\mathrm{MHz}\\\\\\\\).\\\\n\\\\nHowever, as noted by Hankins et al. (2015) already, the power-law indices show large scatter and should be taken as roughly indicative only, showing, e.g., that at higher frequencies, very bright pulses are relatively rare. Indeed, in our data, like in more sensitive previous studies (e.g., Lundgren et al., 1995; Popov & Stappers, 2007; Bhat et al., 2008; Karuppusamy et al., 2010), the fluence distribution clearly flattens at lower fluences. At the very low end, this is because our detection method misses more pulses, but the changes above \\\\\\\\(\\\\\\\\sim 0.2\\\\\\\\,\\\\\\\\mathrm{kJy}\\\\\\\\,\\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\) are real. This turnover may at least partially explain why a variety of power-law indices was found previously, as the measured index will depend on what part of the fluence distribution is fit (which will depend also on the magnification by scattering), as well as why for very high fluences, well away from the turn-over, the power-law index seems fairly stable (Bera & Chengalur, 2019).\\\\n\\\\nComparing the distributions for the different epochs, one sees that they are very similar except for a shift left or right in the figure. This confirms that the differences in rates seen between the epochs are due differences in magnification due to scintillation (and not due to the Crab Pulsar varying the rate at which pulses are emitted, which would, to first order, shift the distributions up and down).\\\\n\\\\nAs the fluence distributions looked roughly parabolic in log-log space, we also show cumulative log-normal distributions in Figure 4, of the form,\\\\n\\\\n\\\\\\\\[N_{\\\\\\\\mathrm{GP}}(E>E_{0})=\\\\\\\\frac{A}{2}\\\\\\\\left[\\\\\\\\mathrm{erfc}\\\\\\\\left(\\\\\\\\frac{\\\\\\\\ln E_{0}- \\\\\\\\mu}{\\\\\\\\sigma\\\\\\\\sqrt{2}}\\\\\\\\right)\\\\\\\\right], \\\\\\\\tag{2}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(A\\\\\\\\) is a scale factor, \\\\\\\\(\\\\\\\\mu\\\\\\\\) and \\\\\\\\(\\\\\\\\sigma\\\\\\\\) are the mean and standard deviation of \\\\\\\\(\\\\\\\\ln E_{0}\\\\\\\\), and \\\\\\\\(\\\\\\\\mathrm{erfc}\\\\\\\\) is the complementary error function. One sees that these describe the observed cumulative distributions quite well.\\\\n\\\\nFigure 4: Reverse cumulative GP fluence distribution showing the occurrence rates of GPs. For comparison, power-law distributions (solid black lines) and log-normal distributions (dashed black line) are shown, with indices \\\\\\\\(\\\\\\\\alpha\\\\\\\\) and widths \\\\\\\\(\\\\\\\\sigma\\\\\\\\) as listed in the legend.\\\\nIf the intrinsic distributions were log-normal, it would imply that especially for the MP, most of the flux is already captured and that the total rate of GPs is not much larger than our detection rate. For the log-normal distribution shown in Figure 4, for the MP, \\\\\\\\(A=2.7\\\\\\\\ \\\\\\\\mathrm{s}^{-1}\\\\\\\\) and the mean GP fluence is \\\\\\\\(\\\\\\\\langle E\\\\\\\\rangle=\\\\\\\\exp(\\\\\\\\mu+\\\\\\\\frac{1}{2}\\\\\\\\sigma^{2})=1.2\\\\\\\\ \\\\\\\\mathrm{kJy\\\\\\\\,\\\\\\\\mu s}\\\\\\\\) and only 1.5% of the total flux is below \\\\\\\\(0.15\\\\\\\\ \\\\\\\\mathrm{kJy\\\\\\\\,\\\\\\\\mu s}\\\\\\\\), while for the IP, \\\\\\\\(A=1.6\\\\\\\\ \\\\\\\\mathrm{s}^{-1}\\\\\\\\) and \\\\\\\\(\\\\\\\\langle E\\\\\\\\rangle=0.24\\\\\\\\ \\\\\\\\mathrm{kJy\\\\\\\\,\\\\\\\\mu s}\\\\\\\\), and 13% of the flux is below.\\\\n\\\\nWe can verify whether our MP GPs account for most of the flux by calculating pulse profiles with and without removing pulse rotations where GPs are detected. As can be seen in Figure 5, significant flux remains in both MP and IP. For the MP, even though the remaining signal is brighter in epochs B and D, the fraction is lower: about 18% in B and D, in comparison with 23% in A and C. This again can be understood if the larger detection rate is due to an overall magnification: a larger fraction of the pulses - and hence of the total flux - is detected.\\\\n\\\\nOur result is similar (but more constraining) than that of Majid et al. (2011), who showed that at least \\\\\\\\(54\\\\\\\\%\\\\\\\\) of overall pulsed energy flux for the Crab Pulsar is emitted in the form of GPs. But it is in contrast for what is seen by Abbate et al. (2020) for PSR J1823\\\\\\\\(-\\\\\\\\)3021A, where the detected GPs make up only a small fraction of the integrated pulse emission (\\\\\\\\(4\\\\\\\\%\\\\\\\\) and \\\\\\\\(2\\\\\\\\%\\\\\\\\) for their C1 and C2 components, respectively), and by Geyer et al. (2021) for PSR J0540\\\\\\\\(-\\\\\\\\)6919, where the detected GPs only make up \\\\\\\\(7\\\\\\\\%\\\\\\\\) of the total flux. This might indicate a difference in the emission process. As these authors noted, however, a larger population of undetected GPs may still be hidden below their detection threshold.\\\\n\\\\nFor our observations, for both MP and IP, the residual flux is much larger than expected based on the log-normal distribution, thus indicating that the true fluence distribution has more pulses at low fluence (many more for the IP); if additional pulses were emitted also in rotations that we do not detect them, their typical fluence would be the residual flux integrated over one cycle, which is \\\\\\\\(\\\\\\\\sim 25\\\\\\\\ \\\\\\\\mathrm{Jy\\\\\\\\,\\\\\\\\mu s}\\\\\\\\) for MP and a little less for IP. This is well below our detection limit, so consistent in that sense, but from the distributions shown in Figure 4, one would expect a much smaller rate than once per pulse period at \\\\\\\\(25\\\\\\\\ \\\\\\\\mathrm{Jy\\\\\\\\,\\\\\\\\mu s}\\\\\\\\). This might suggest that there are even more but typically fainter bursts (note that it cannot be fainter bursts accompanying the GPs we already detect, since we excluded the full rotations in calculating the resid\\\\n\\\\nFigure 5: Mean and median MP and IP pulse profiles obtained using all pulse rotations (in blue and orange, respectively) and using only those in which no GPs were detected (green and red, respectively) in \\\\\\\\(6.25\\\\\\\\ \\\\\\\\mathrm{\\\\\\\\mu s}\\\\\\\\) bins. Note that because the noise in an individual profile is not normally distributed, but rather follows a \\\\\\\\(\\\\\\\\chi_{k}^{2}\\\\\\\\) distribution, the median is slightly below zero in the off-pulse region, by \\\\\\\\((1-2/3k)^{3}-1\\\\\\\\simeq-6/9k\\\\\\\\simeq-0.0002\\\\\\\\) of the SEFD of \\\\\\\\(\\\\\\\\sim\\\\\\\\!150\\\\\\\\ \\\\\\\\mathrm{Jy}\\\\\\\\) (Section 2), or \\\\\\\\(\\\\\\\\sim\\\\\\\\!-0.03\\\\\\\\ \\\\\\\\mathrm{Jy}\\\\\\\\) given \\\\\\\\(k=3200\\\\\\\\) degrees of freedom (complex dedispersed timestream squared, averaged over 2 polarizations, 8 bands, and 100 time bins).\\\\nual emission), or that there is some steady underlying emission. It would be worthwhile to test this with more sensitive future observations.\\\\n\\\\n### Pulse Phases\\\\n\\\\nDefining the time of arrival of a GP as the time when an increase in flux is first detected, the longitude windows where MP and IP GPs occur have total widths of \\\\\\\\(\\\\\\\\sim 680\\\\\\\\)\\\\\\\\(\\\\\\\\mu\\\\\\\\)s and \\\\\\\\(860\\\\\\\\)\\\\\\\\(\\\\\\\\mu\\\\\\\\)s (or \\\\\\\\(\\\\\\\\sim\\\\\\\\!7\\\\\\\\fdg 3\\\\\\\\) and \\\\\\\\(\\\\\\\\sim\\\\\\\\!9\\\\\\\\fdg 2\\\\\\\\)), respectively (averaged over the four epoch). As can be seen in Figure 6, the majority of GPs occur within much narrower windows: the root-mean-square deviations around the mean arrival phases are \\\\\\\\(\\\\\\\\sim\\\\\\\\!100\\\\\\\\)\\\\\\\\(\\\\\\\\mu\\\\\\\\)s and \\\\\\\\(\\\\\\\\sim\\\\\\\\!130\\\\\\\\)\\\\\\\\(\\\\\\\\mu\\\\\\\\)s (or \\\\\\\\(\\\\\\\\sim\\\\\\\\!1\\\\\\\\fdg 1\\\\\\\\) and \\\\\\\\(\\\\\\\\sim\\\\\\\\!1\\\\\\\\fdg 4\\\\\\\\)), respectively. The number distribution is roughly Gaussian, with a slightly negative skewness (i.e., a longer tail toward earlier phases and thus with a mode towards later phases). This was also observed by Majid et al. (2011) at a similar frequency of \\\\\\\\(1664\\\\\\\\)\\\\\\\\(\\\\\\\\mathrm{MHz}\\\\\\\\). In EKO36 D, a few MP pulses are detected beyond the range found in the other epochs. As we will discuss in Section 4.6, these \"outlier\" detections are due to echoes (hence, they are are omitted in our determinations of widths above).\\\\n\\\\nIn Figure 6, we also show the flux distributions as a function of pulse phase, including the median flux of the GPs detected in any given phase bin. One sees no obvious variation, i.e., no hint of, e.g., brighter pulses having an intrinsically narrower phase distribution. This suggests that only the probability of seeing a pulse depends on pulse phase. In our earlier work on these data, where we studied how the pulse spectra and their correlations are affected by scattering (Lin et al., 2023), we concluded that we resolved the regions from which the nanoshots that comprise individual GPs are emitted, and that this is most easily understood if the emitting plasma is ejected highly relativistically, with \\\\\\\\(\\\\\\\\gamma\\\\\\\\simeq 10^{4}\\\\\\\\) (as was already suggested by Bij et al., 2021). If so, the emission would be beamed to angles much smaller than the width of the phase windows, and the range of phases over which we observe GPs would reflect the range of angles over which plasma is ejected.\\\\n\\\\n### Arrival Times\\\\n\\\\nSeveral studies (e.g., Karuppusamy et al., 2010; Majid et al., 2011) have found that GPs in different rotations are not correlated, and that there is no correlation between MP and IP GPs, but that instead the distribution of the time delays between successive GPs follows an exponential distribution, as expected for a Poissonian process. Within a given cycle, though, multiple correlated microbursts can occur (Sallmen et al., 1999; Hankins and Eilek, 2007).\\\\n\\\\nWith our high sensitivity, we can investigate this in more detail. In Table 3 we show the number of rotations in which we detect multiple MP or IP bursts (i.e., double, triple etc.), as well as the number expected (listed only where larger than 0) for the case where all events are independent,\\\\n\\\\n\\\\\\\\[N_{n}=p_{n}N_{r}=\\\\\\\\begin{pmatrix}N_{\\\\\\\\mathrm{p}}\\\\\\\\\\\\\\\\ n\\\\\\\\end{pmatrix}\\\\\\\\left(\\\\\\\\frac{1}{N_{r}}\\\\\\\\right)^{n}\\\\\\\\left(1-\\\\\\\\frac{1}{N_{r}}\\\\\\\\right)^{ N_{\\\\\\\\mathrm{p}}-n}N_{r}, \\\\\\\\tag{3}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(p_{n}\\\\\\\\) is the probability of a given rotation to have \\\\\\\\(n\\\\\\\\) bursts (assuming a binomial distribution), \\\\\\\\(N_{r}\\\\\\\\) is the total number of rotations observed, and \\\\\\\\(N_{\\\\\\\\mathrm{p}}\\\\\\\\) is the total number of bursts found (and where for numerical values we inserted numbers from Table 1: \\\\\\\\(N_{\\\\\\\\mathrm{p}}=N_{\\\\\\\\mathrm{MP}}\\\\\\\\) or \\\\\\\\(N_{\\\\\\\\mathrm{IP}}\\\\\\\\) and \\\\\\\\(N_{r}=t_{\\\\\\\\mathrm{exp}}/P_{\\\\\\\\mathrm{Crab}}\\\\\\\\), where \\\\\\\\(P_{\\\\\\\\mathrm{Crab}}=33.7\\\\\\\\)\\\\\\\\(\\\\\\\\mathrm{ms}\\\\\\\\) is the rotation period of the pulsar). One sees that we detect significantly more multiples than expected by chance6, i.e., some of the detected pulses are composed of multiple, causally related microbursts.\\\\n\\\\nFootnote 6: In Lin et al. (2023), we wrongly concluded the multiples were consistent with arising by chance. Sadly, we used incorrect estimates of \\\\\\\\(N_{n}\\\\\\\\).\\\\n\\\\nIn principle, one could estimate the number of independent bursts, \\\\\\\\(N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}\\\\\\\\), in each epoch by subtracting from \\\\\\\\(N_{\\\\\\\\mathrm{p}}\\\\\\\\) the excess pulses from Table 3, but this would not be quite correct since the excess would be relative to estimates made using the total number of observed pulses \\\\\\\\(N_{\\\\\\\\mathrm{p}}\\\\\\\\), not the (lower) number of independent pulses \\\\\\\\(N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}\\\\\\\\). One could iterate, but an easier, unbiased estimate of \\\\\\\\(N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}\\\\\\\\) can be made using the observed fraction of rotations in which we do not see any bursts, which should equal \\\\\\\\(N_{0}/N_{r}=p_{0}=\\\\\\\\left(1-1/N_{r}\\\\\\\\right)^{N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}}\\\\\\\\). Solving for \\\\\\\\(N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}\\\\\\\\), we find that \\\\\\\\(N_{\\\\\\\\mathrm{p}}^{\\\\\\\\mathrm{ind}}=fN_{\\\\\\\\mathrm{p}}\\\\\\\\) with fractions \\\\\\\\(f\\\\\\\\) that are consistent between all epochs, at \\\\\\\\(91.8\\\\\\\\pm 0.2\\\\\\\\) and \\\\\\\\(95.2\\\\\\\\pm 0.5\\\\\\\\)% for MP and IP, respectively. Hence, about 8 and 5% of the detected MP and IP pulses, respectively, are extra components. Or, as fractions of independent MP and IP pulses, \\\\\\\\((6,1,0.12)\\\\\\\\) and \\\\\\\\((4,0.3,0.0)\\\\\\\\%\\\\\\\\), respectively, are causally related double, triple, or quadruple microbursts.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{c c c c c c c c} \\\\\\\\hline \\\\\\\\hline Observation & \\\\\\\\multicolumn{3}{c}{MP} & \\\\\\\\multicolumn{3}{c}{\\\\\\\\(\\\\\\\\dots\\\\\\\\)} & IP & \\\\\\\\multicolumn{3}{c}{\\\\\\\\(\\\\\\\\dots\\\\\\\\)} \\\\\\\\\\\\\\\\ Code & 2 & 3 & 4 & 5 & 6 & 2 & 3 & 4 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline EK036 A & 1820(599) & 200(12) & 24 & 0 & 0 & 144(17) & 4 & 2 \\\\\\\\\\\\\\\\ EK036 B & 1431(611) & 170(18) & 22 & 3 & 1 & 237(43) & 16 & 2 \\\\\\\\\\\\\\\\ EK036 C & 611(213) & 67 (4) & 6 & 0 & 0 & 54( 7) & 4 & 0 \\\\\\\\\\\\\\\\ EK036 D & 934(395) & 117(10) & 23 & 6 & 1 & 116(19) & 9 & 0 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\end{tabular} Note. – Numbers in parentheses are those expected if bursts occur randomly; for that case, one does not expect to find any rotations with 4 or more MP bursts or 3 or more IP bursts. Note that our GP detection method does not differentiate between microbursts and echoes, which becomes important for a few very bright pulses in EKO36 D, for which echoes were present. In addition, we are not able to distinguish microbursts that occur very close together in time. The number of detections differ from Lin et al. (2023) as a different, more robust, search algorithm is implemented here (see Section 4.1).\\\\n\\\\n\\\\\\\\end{table}\\\\nTable 3: Number of Rotations with Multiple Bursts.\\\\nTo investigate the distributions further, we show histograms of the time delay between pulses in Figure 7. Overdrawn are expectations for randomly arriving, independent pulses. We constructed these by bootstrapping, where we repeatedly reassign new random pulse cycles to our observed sets of pulses, and then recalculate the time delay distributions. Note that in our bootstraps, we do not randomize pulse phase, so that the observed phase distribution is correctly reflected in the time delays. One sees that as a function of pulse cycle (right column panels for MP and IP GPs in Fig. 7), the time delay distributions are not well defined.\\\\n\\\\nFigure 6: MP GP and IP GP fluence and count distributions as a function of pulse phase for each EK036 observation. We used pulse phase bins of \\\\\\\\(0.1\\\\\\\\%\\\\\\\\) and fluence bins of \\\\\\\\(0.1\\\\\\\\ \\\\\\\\mathrm{dex}\\\\\\\\). The light purple line in the fluence panels show the median for bins with more than \\\\\\\\(2\\\\\\\\) detected pulses.\\\\nure 7), the observed histograms follow the expected exponential distribution (although the observed counts are slightly lower than the expected ones because not all pulses are independent, as is implicitly assumed in the bootstraps).\\\\n\\\\nFor the time delays between pulses that occur in the same cycle (left column panels for MP and IP GPs in Figure 7), the observed distributions are very different from those expected for randomly occurring bursts. One sees a large peak at short delays, representing the excess microbursts from Table 3, following a roughly exponential distribution with a mean time between bursts of \\\\\\\\(\\\\\\\\sim 30\\\\\\\\;\\\\\\\\mu\\\\\\\\)s or so. Intriguingly, at somewhat larger time difference, there seem to be fewer bursts than expected for independent events. This suggests that while a given detection has an enhanced probability of being in a group of causally related microbursts, the occurrence of a burst also suppresses the likelihood of another, independent, burst being produced in the same rotation. Thus, our results confirm that GPs are often composed of multiple microbursts, and they indicate that another, independent GP is less likely to occur right after.\\\\n\\\\n### Scattering Features\\\\n\\\\nIn Figure 6, one sees that in EK036 D, several MP GPs were detected at pulse phases quite far from the median phase. To investigate this, we looked at the arrival times of all GPs detected in EK036 D (see left panel of Figure 8). We found that the outliers occurred in two pulse rotations, which turned out to contain the brightest GPs in EK036 D. Looking at the pulse profiles of these brightest GPs, one sees that they are very similar (see right panels of Figure 8). In fact, closer\\\\n\\\\nFigure 7: Time delays between successive GPs for the MP (in blue) and IP (in orange) components for each EK036 observation. On the left MP and IP columns, time delays within a pulse rotation are shown with bins of \\\\\\\\(10\\\\\\\\;\\\\\\\\mu\\\\\\\\)s and \\\\\\\\(20\\\\\\\\;\\\\\\\\mu\\\\\\\\)s for the MP and IP respectively; the low counts in the first bin reflect the minimum separation of \\\\\\\\(8.75\\\\\\\\;\\\\\\\\mu\\\\\\\\)s between detected pulses. On the right MP and IP columns, time delays in pulse rotations are shown with bins of \\\\\\\\(1\\\\\\\\) rotation and \\\\\\\\(4\\\\\\\\) rotations for the MP and IP respectively. The red lines show the average time delay histograms for \\\\\\\\(1000\\\\\\\\) bootstrap iterations, in which we randomized the rotation in which a pulse was seen (but not the phase, to keep the observed phase distribution).\\\\nexamination reveals that all of the brightest GPs detected in EK036 D show similar pulse profiles. This implies that the pulses far from the median pulse phase arrive late because they are actually weak echoes of the main burst, with amplitudes down to \\\\\\\\(\\\\\\\\sim 0.4\\\\\\\\%\\\\\\\\) of the peak flux and delays up to \\\\\\\\(\\\\\\\\sim 300~{}\\\\\\\\mu\\\\\\\\)s.\\\\n\\\\nIn Figure 9, we show singular value decomposition (SVD) approximations of the average MP GP profile for each epoch (for the IP, too few bright pulses were available). This was created from MP GP rotations with peak intensities greater than \\\\\\\\(200~{}\\\\\\\\mathrm{Jy}\\\\\\\\) and seemingly single peaks, aligned using time offsets found by correlation with a reference pulse. To avoid giving too much weight to the brightest pulses, and thus risking that remaining substructure enters the average profile, we normalized each rotation by the intensity at the correlation maximum before doing the SVD. One sees that all profiles are fairly sharply peaked, but sit on top of a base, which has the expected asymmetric part extending to later time due to scattering, as well as a more symmetric component, likely resulting from the collective effect of faint microbursts. Comparing the epochs, one sees that for EK036 A-C, the profile dropoff is relatively smooth and becomes undetectable after \\\\\\\\(\\\\\\\\sim\\\\\\\\!200~{}\\\\\\\\mu\\\\\\\\)s, while in EK036 D, the tail is much longer, extending to \\\\\\\\(\\\\\\\\sim\\\\\\\\!400~{}\\\\\\\\mu\\\\\\\\)s, and is much more bumpy.\\\\n\\\\nAlmost certainly, all bumps are echoes, including those at shorter delay in EK036 B (more clearly seen in the linear-scale plots in Lin et al.2023), Indeed, looking carefully at the stack of profiles in Figure 9, one sees that the echoes in EK036 D drift in time, moving slightly further away from the MP during the observation, with perhaps even a hint that echoes further away from the main bursts drift faster than those closer in. (Note that this stack is not completely linear in time, although given that the GP detection rate is roughly constant throughout, it is not far off.) This change in time is expected for echoes off a structure with changing distance from the line of sight, and indeed has been seen for a very prominent echo by Backer et al. (2000); Lyne et al. (2001). Overall, our observations suggests echoes are common, as also concluded from daily monitoring at \\\\\\\\(600~{}\\\\\\\\mathrm{MHz}\\\\\\\\) by Serafin-Nadeau et al. (2023, in prep.).\\\\n\\\\nFigure 8: _Left_: MP GPs and IP GPs detected in the EK036 D data. The gray shaded regions indicate when the telescope was not observing the Crab Pulsar and the black vertical lines mark our MP GP and IP GP windows. In the inset, we show two pulse rotations containing the brightest GPs “A” and “B”, in red and orange respectively. _Right, Top_: Waterfalls of the two brightest pulses in EK036 D with \\\\\\\\(1~{}\\\\\\\\mu\\\\\\\\)s time resolution and \\\\\\\\(1~{}\\\\\\\\mathrm{MHz}\\\\\\\\) frequency resolution. _Right, Bottom_: Pulse profile of the two brightest pulses in EK036 D with \\\\\\\\(1~{}\\\\\\\\mu\\\\\\\\)s time resolution scaled to the peak of each pulse. Pulses “A” and “B” show similar features and we conclude that during the EK036 D observations, weak echoes were present at large delays.\\\\n## 5 Summary of Conclusions\\\\n\\\\nThe fine time resolution and high sensitivity in our beam-formed EVN data allowed us to confidently detect \\\\\\\\(65951\\\\\\\\) GPs with fluences above \\\\\\\\(\\\\\\\\sim 150\\\\\\\\ \\\\\\\\mathrm{Jy\\\\\\\\ \\\\\\\\mu s}\\\\\\\\) over a short period of \\\\\\\\(7.32\\\\\\\\mathrm{hr}\\\\\\\\). Within each of our four observations, we found that the GP detection rates are fairly constant, but that between epochs they differ by a factor of \\\\\\\\(\\\\\\\\sim\\\\\\\\!2\\\\\\\\). Similar changes were seen previously, and were suggested by Lundgren et al. (1995) to reflect changes in overall magnification of the scattering screens along the line of sight.\\\\n\\\\nThe changes in magnification are consistent with the pulse fluence distributions, which are power-law like at high fluence, but with a flattening at lower fluences; the distributions from the different epochs can be shifted to each other with a change in fluence scale. We noted that the fluence distributions are similar to what is expected for log-normal distributions, but found that the residual signals seen in the GP phase windows after removing the GPs we detected were larger than expected if the log-normal distribution continued also below our detection limit. Nevertheless, it suggests that with only somewhat more sensitive observations, it should be possible to get a fairly complete sampling of all GPs that contribute to the average flux, at least for the MP component.\\\\n\\\\nAnalyzing the pulse phase distributions, we confirm previous observations showing that the majority of GPs occur within very narrow phase windows. Furthermore, we observe no significant variations in the median flux distributions as a function of pulse phase. This suggests that it is the probability of observing a pulse that depends on pulse phase, not its energy, implying that the angle within which a pulse is emitted is much narrower than the rotational phase window, as expected if the plasma causing them is travelling highly relativistically (Bij et al., 2021; Lin et al., 2023).\\\\n\\\\nWith our high detection rates, we were able to investigate the distribution of time delays between successive bursts within the same pulse rotation. We detect a larger number than expected if all bursts were due to a Poissonian process, and infer that \\\\\\\\(\\\\\\\\sim\\\\\\\\!5\\\\\\\\%\\\\\\\\) of bursts come in groups of 2 or 3 causally related microbursts, with a typical separation in time of \\\\\\\\(\\\\\\\\sim\\\\\\\\!30\\\\\\\\ \\\\\\\\mu\\\\\\\\)s.\\\\n\\\\nAdditionally, our high sensitivity revealed weak echo features for individual bright pulses, which drift slightly but sig\\\\n\\\\nFigure 9: _Line plots_: SVD approximation of the MP pulse profile for all observations. In EK036 B, echoes are seen close to the profile’s peak (see Lin et al., 2023 for more details). The profile for EK036 D shows multiple weak echoes up to \\\\\\\\(\\\\\\\\sim\\\\\\\\!300\\\\\\\\ \\\\\\\\mu\\\\\\\\)s. _Image_: The MP pulse stack for EK036 D, using a logarithmic colour scale to bring out faint features. Each pulse is aligned by correlating with the rotation with the brightest pulse in EK036 D (which is appears to be a simple single microburst) and then normalized by the intensity at time \\\\\\\\(0\\\\\\\\) (the black dashed line). The echoes appear to move out over time, as one can see by comparing the location of the most prominent faint echo with the dashed white vertical line near it (time is increasing both upwards and to the right in this image).\\\\nnificantly even over our timescales of just a few hours. We infer that echo events are not rare.\\\\n\\\\nGiven our findings, we believe even more sensitive follow-up studies of the Crab Pulsar would be very useful. This would be possible using more small dishes (spaced sufficiently far apart that the Crab Nebula is well-resolved) and by recording a larger bandwidth.\\\\n\\\\n## Acknowledgements\\\\n\\\\nWe thank the anonymous referee for their comments, which improved the clarity of this manuscript. We thank the Toronto Scintillometry group, and in particular Nikhil Mahajan, for useful discussion on GP statistics. Computations were performed on the Niagara supercomputer at the SciNet HPC Consortium (Loken et al., 2010; Ponce et al., 2019). SciNet is funded by: the Canada Foundation for Innovation; the Government of Ontario; Ontario Research Fund - Research Excellence; and the University of Toronto. M.Hv.K. is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) via discovery and accelerator grants, and by a Killam Fellowship.\\\\n\\\\nThe European VLBI Network (EVN) is a joint facility of independent European, African, Asian, and North American radio astronomy institutes. Scientific results from data presented in this publication are derived from the following EVN project codes: EK036 A-D.\\\\n\\\\nastropy (Astropy Collaboration et al., 2013, 2018, 2022), Baseband (Van Kerkwijk et al., 2020), CALC10 (Ryan & Vandenberg, 1980), numpy (Harris et al., 2020), matplotlib (Hunter, 2007), pulsarbat (Mahajan & Lin, 2023), scipy (Virtanen et al., 2020), tempo2 (Hobbs & Edwards, 2012).\\\\n\\'\\n \\'# Maybe, Maybe Not: A Survey on Uncertainty in Visualization\\\\n\\\\n###### Abstract\\\\n\\\\nUnderstanding and evaluating uncertainty play a key role in decision-making. When a viewer studies a visualization that demands inference, it is necessary that uncertainty is portrayed in it. This paper showcases the importance of representing uncertainty in visualizations. It provides an overview of uncertainty visualization and the challenges authors and viewers face when working with such charts. I divide the visualization pipeline into four parts, namely data collection, preprocessing, visualization, and inference, to evaluate how uncertainty impacts them. Next, I investigate the authors\\\\\\' methodologies to process and design uncertainty. Finally, I contribute by exploring future paths for uncertainty visualization.\\\\n\\\\n## 1 Introduction\\\\n\\\\nWith a rise in the complexity and dimensionality of data, analyzing and modeling data becomes more challenging. When most of our decisions are data-driven, it becomes imperative that we know the nature of the data and the patterns it contains. As a result, analyzing the inherent uncertainty in the data is gaining more significance. In various fields, uncertainty can signify different things. For instance, data bias, random or systematic error, and statistical variance are all factors that contribute to data uncertainty. Without understanding the underlying uncertainty in our data, we cannot make accurate predictions. Similarly, to observe the true structure of our data and as well as identify patterns in it, we need to visualize it. Today, we can no longer undermine the significance of uncertainty nor ignore the importance of visualizations for data analysis.\\\\n\\\\nAs mentioned before, uncertainty is bound to exist whenever there is data. Therefore representation of uncertainty in data visualizations is crucial. Consider the example of hurricane path maps, as shown in Figure 1. The increase in the width of the predicted path with time is not due to an increase in the size of the hurricane. Instead, it is representing the inherent uncertainty in the data. In other words, the visualization indicates that compared to Friday, Sunday\\\\\\'s hurricane path is more difficult to predict with any degree of accuracy.\\\\n\\\\nInformation tends to be withheld from the viewer when one does not portray uncertainty in the visualization. Therefore the viewer might occasionally be ignorant of this exclusion. This breach of trust can have significant consequences for both the author and the viewer. Given this significance, it is reasonable to assume that visualizations frequently include uncertainty. But how often do we encounter charts that represent uncertainty? How frequently do we check for bias in graphs that represent public surveys? As it turns out, not frequently.\\\\n\\\\nIn a recent study [9], 121 journalism articles, social science surveys, and economic estimates were examined. Out of 449 visualizations created for inference, the study demonstrates that only 14 accurately depict uncertainty. \"What\\\\\\'s Going on in This Graph?\" is a New York Times (NYT) initiative to increase graphical literacy, especially among students. Different categories of charts, such as maps, parts-to-whole, and associations, are published for students to explore and analyze. When I looked into the distribution of these charts, I found that only 6 out of the 136 charts show uncertainty.\\\\n\\\\nThe question I ask is, do we actually examine uncertainty representations when we come across them in order to make decisions, or do we simply ignore them? Does uncertainty offer value or just clutter these visualizations? I try to investigate these questions in this paper. Visualizations are an integral part of newspapers, government bills, and business earnings reports to name a few. The public uses them to gain insights, spot trends, and make decisions.\\\\n\\\\nHence, when we visualize data, it becomes critical to support those visualizations with information about uncertainty. People frequently use visualizations to examine data and make observations. A lack of uncertainty representation could result in incorrect and erroneous interpretations. However, it can be challenging to visualize uncertainty. There are limited standard guidelines or protocols that authors can follow when they create such charts. Given these drawbacks, uncertainty visualization is considered one of the top research problems in data visualization [13]. With the help of a few uncertainty visualization examples, this survey studies how uncertainty contributes to every phase in visualization. Most research in this area focuses on creating charts with uncertainty and how viewers may perceive them. However, uncertainty is also influential in the other parts of the data visualization process, such as during data collection and preprocessing.\\\\n\\\\n**The objectives of this paper are as follows:**\\\\n\\\\n* Provide an entry point for anyone who wants to learn about uncertainty visualization\\\\n* Delineate the significance of uncertainty visualizations\\\\n* Explore how uncertainty influences every phase of the data visualization process\\\\n\\\\nFigure 1: An example chart for Matthew showing its five-day forecast track [5]\\\\n* Understand the challenges authors and viewers face when interacting with it\\\\n* Discuss the open problems and future research directions in the field\\\\n\\\\nThis work is divided into the following sections. Section 2 defines uncertainty and describes the relationship between uncertainty and visualization. In Section 3, I classify the data visualization pipeline into four phases, analyzing the involvement of uncertainty in each phase. The classification helps look at each phase individually, focusing on the challenges and bottlenecks authors and viewers face when working with uncertainty visualization. Finally, I study some state-of-the-art methods to visualize uncertainty and discuss future directions for research. I conclude the paper in Section 4.\\\\n\\\\n## 2 Uncertainty and Visualization\\\\n\\\\nVisualizations are incredibly important for examining, analyzing, and interpreting data in the era of big data. Visualizations are evidence that a picture really does say a thousand words. They aid viewers in seeing trends, background noise, and outliers. Asking the correct questions can be quite challenging when there is an abundance of data. Through visualizations, viewers can determine what questions the data can help answer. With improvements in hardware, software, and graphics theory, data visualizations are adopted more frequently and widely [26]. Viewers use visualizations to make decisions. However, making decisions and drawing observations by looking at visualizations can be complex due to the statistical variance and uncertainty present in these visualizations.\\\\n\\\\nAs mentioned previously, uncertainty can have different definitions based on different scenarios [3]. Broadly speaking, uncertainty is classified into two types, aleatory and epistemic. Aleatory uncertainty rises from random fluctuation and unknown outcomes when an experiment is run multiple times in a consistent environment. For example, in a drug trial, a participant\\\\\\'s blood pressure can vary due to stress and anxiety. There might also be measurement errors in the sphygmomanometer. Aleatory uncertainty can be minimized by controlling individual factors and increasing the number of readings. Epistemic uncertainty, on the other hand, rises from a lack of knowledge, like predicting the outcome of the same experiment in a completely different, unknown environment. For example, predicting the effect of a drug on a new disease. Uncertainty can be measured, like risks but can also be unquantified, like bias. While aleatory uncertainty is more widely represented in the visualizations [25], both types can be represented with distribution graphs.\\\\n\\\\nUncertainty and visualizations are interweaved, and working with one often requires working with the other. In 1644, Michael Florent van Langren was one of the first researchers to use visualization for statistical analysis [25]. He used a 1D line graph to present the 12 known estimated longitudinal distances between Toledo and Rome, as shown in Figure 2. Instead of using a table to show this data, Langren used this graph to showcase the wide range of variation. Even though all the distances were over-estimated (actual distance, in longitude, is shown using the arrow), the graph remains classic in demonstrating the power of visualization.\\\\n\\\\nThe popular Anscombe\\\\\\'s quartet [1] is a perfect example of how data with similar statistics might have a very different distribution which is observed when visualized. The quartet consists of four datasets with 11 points having nearly the same mean, sample variance, correlation, linear regression, and coefficient of determination. The four datasets may appear very similar to viewers looking at the data and the descriptive statistics. However, when one visualizes them, the difference in their distribution is very evident, as shown in Figure 3. Looking at data in tabular form may hide insightful observations and can lead to erroneous conclusions. Today, researchers across all domains use extensive libraries such as [12, 19, 22, 4, 11] to analyze data uncertainty.\\\\n\\\\nUsing visualizations to represent and study uncertainty in data is widely adopted. However, uncertainty in visualizations is often not communicated [9]. One of the earliest instances of uncertainty being presented can be traced back to the 18th century. Joseph Priestley, a British scientist, created \"A Chart of Biography\" to present the lifespans of famous people as shown in Figure 4. He used horizontal lines to portray the lifetime of about 2000 people and used dots before or after the lines to communicate uncertainty.\\\\n\\\\nVisualizations of uncertainty, however, are not common. Numerous factors influence why authors decide against visualizing uncertainty. Since they do not know all the information about the dataset, viewers may draw inaccurate conclusions in the absence of uncertainty representation. Nevertheless, introducing more uncertainty could also make the audience feel too overwhelmed to pay attention to it. The study of why visualizing uncertainty is rare is\\\\n\\\\nFigure 4: Priestley’s Chart of Biography [21]\\\\n\\\\nFigure 3: Anscombe’s quartet represents four datasets with similar statistics but very different distributions.\\\\n\\\\nFigure 2: Langren’s line graph is one of the first visualizations to present uncertainty\\\\nstill in its early stages. In the section that follows, I go through each of these issues in more detail and look at how uncertainty affects every stage of data visualization.\\\\n\\\\n## 3 Uncertainty in Visualization\\\\n\\\\nPrevious works in the field have attempted to classify the data visualization process differently. [14] considers sampling, modeling, visualization, and decision-making as the primary sources of uncertainty. This paper follows a similar classification. I divide the visualization pipeline into **data collection, preprocessing, visualization and inference** as shown in Figure 5. Pang et al. [18] classify the process into data collection, derivation, and visualization and discuss how uncertainty is introduced in each stage.\\\\n\\\\nUnder the data collection phase, the paper mainly discusses the uncertainty added due to measurement errors. However, there are other sources, such as bias and sampling error, that the paper fails to describe. I investigate these uncertainties in Section 3.3.1. The authors then discuss the change data undergoes when it is preprocessed. These changes include converting one unit to another, rescaling, and resampling. However, they do not mention other vital issues such as missing data, approximation, and interpolation that I examine in Section 3.3.2. Next, the authors highlight how uncertainty also influences the data visualization stage itself. They mainly focus on radiosity and volume rendering, while this paper delves more into 2D visualizations. Finally, I explore how viewers infer these visualizations and the challenges they face while making a decision from these charts.\\\\n\\\\nUncertainty is presented at every phase of this classification. However, understanding and evaluating uncertainty in each of these phases is unique. Therefore, authors are required to approach these uncertainties based on their type and complexity, understand their abstraction, and then present them in visualizations in a way that is easy to grasp.\\\\n\\\\nGiven the interdisciplinary nature of visualizations, the format, quantity, and type of data used to create them vary immensely. Different data implies different data collection processes and uncertainties. Uncertainty is intertwined with data acquisition and can arise from random variables and modeling errors [14]. Pang et al. [18] explain how almost all acquired data has statistical variation. Collected data can have errors, bias, and variance. [23] study how bias can be introduced during the process of collecting data. Datasets are prone to various biases that include but are not limited to selection bias, volunteer bias, admission bias, survivor bias, and misclassification bias.\\\\n\\\\nIt is imperative that datasets resemble the true population as closely as possible. Data can also contain different types of errors, such as coverage error, sampling error, nonresponse error, and measurement error [7]. Missing data points is another common challenge researchers face during data collection.\\\\n\\\\nCorrecting these errors is not always possible, but they can be mentioned in the visualization to inform the viewer. However, uncertainty is often ignored when authors create visualizations. Other times this uncertainty in data is not communicated to them [9]. For example, when I analyze a piece called \"Free Speech\" (as shown in Figure 6) published in the What\\\\\\'s Going On in This Graph section of the NYT. [16], we can see how information about uncertainty from the data source is not mentioned directly in the graph. The bars of the graph do not sum to 100 percent since they are missing the no-response segment. The article mentions that the margin of error for the sample is +/- 3.1%, but the graph makes no mention of it.\\\\n\\\\nEfforts are being made by researchers to improve the way uncertainty in the data collection phase is captured, processed, and communicated. Athawale et al. [2] propose using statistical summary maps to represent uncertainty in scalar field data caused by data acquisition.\\\\n\\\\n### _Data Preprocessing_\\\\n\\\\nRaw data is imperfect and can consist of noise and error. Once data is collected, it undergoes processing for accuracy and standardization. However, this phase adds uncertainty to the data that may not be immediately evident. For example, fundamental transformations like rounding off values, converting data from one unit to another, rescaling, resampling, and quantizing can add uncertainty [1]. Even though this might seem minor, the impact can be significant. For example, based on whether we take the value of pi as 22/7(3.14285) or 3.14159, the area of the Sun can vary by a difference of 239x106 sq. miles.\\\\n\\\\nA significant setback that most datasets suffer from is missing data. Data can have missing values for many reasons, such as instrument malfunction, incomplete observations, and lost data. Missing values leave a gap in the dataset, which makes room for uncertainty. Working with such uncertainty requires the authors to take extra measures during preprocessing. Authors attempt to find close estimates of the missing values to provide the viewers with a complete picture. One way to tackle this problem is by deleting the complete entry that has the missing value. This leads to a loss of data and insights. Another option is to make an educated guess about the missing value. However, this is highly unreliable and often not recommended. Using interpolation, imputation, or other techniques can induce errors [3].\\\\n\\\\nSometimes, authors choose to encode these estimated values differently in their designs to inform the viewer about the gap in the dataset. However, how authors choose to visualize this encoding becomes very influential in how viewers perceive these graphs. Whether authors highlight, downplay, annotate or remove the missing values determines how much confidence and credibility the\\\\n\\\\nFigure 5: The data visualization process divided into four stages to show how uncertainty affects each stage\\\\n\\\\nFigure 6: Free Speech, a graph by the New York Times based on a national poll including 1,507 U.S residents [16]\\\\nviewer shows in the visualization [24].\\\\n\\\\n### Visualization Creation\\\\n\\\\nSince uncertainty isgrained in different parts of the data collection process, it is not easy to identify and control it. However, once the data is cleaned and processed, the authors face a new problem. Creating visualizations requires authors to make various decisions on behalf of the viewer. Authors are expected to choose the type of visualization based on data type, which may lead them to choose the scaling, sorting, ordering, and aesthetics [27]. Compelling visualizations are accurate and suggest an understanding and interpretation of data. Hence, it is the author\\\\\\'s responsibility to analyze data correctly before creating any visualizations. Midway [15] describes ten design principles authors can follow to create charts. However, none of those principles discuss how uncertainty can be presented. Creating effective visualizations is hard. However, when we add uncertainty representation, the task becomes much more complex [17]. The data visualization community of researchers, designers, journalists, etc., has been reluctant to add uncertainty to their charts. Authors are aware of how significant uncertainty visualization is. Yet, they choose to exclude uncertainty when they design their charts for various reasons discussed below.\\\\n\\\\n#### 3.2.1 Uncertainty is hard to represent\\\\n\\\\nThough data is replete with uncertainty, the difficulty lies in determining if it should be represented and how. If the uncertainty has no direct relationship to the goal of the visualization, then it may not be included in the visualization. But this is not a conclusion that authors can quickly draw. The rise in techniques of visualizing uncertainty can make it harder for authors to decide which one to choose from. One of the biggest challenges in visualizing uncertainty is discovering and communicating the relationship and impact that the uncertainty has on the data. Data visualization is often a preferred choice for analysis due to its ability to present high-dimensional data. However, uncertainty also has dimensions, generally classified into scalar, vector, and tensor [20]. While scalar and vector fields of uncertainty are depicted in charts, tensor fields are often avoided. Mapping these dimensions of uncertainty along with the dimensions of data is challenging and often overlooked when creating charts. Instead, authors tend to simplify uncertainty to align with the dimensionality of the data.\\\\n\\\\n#### 3.2.2 Uncertainty is hard to calculate and verify\\\\n\\\\nAnother reason why authors choose to exclude uncertainty from their charts is that calculating uncertainty is complex [9]. It is well known that even mathematicians and statisticians sometimes find it challenging to calculate the error or variance in a dataset. Verifying if the presented uncertainty is correct is challenging. Moreover, if the authors make an error while designing their charts, they end up providing wrong information to the viewers and losing their trust.\\\\n\\\\n#### 3.2.3 Viewers may be overwhelmed\\\\n\\\\n[9] explains why the inclusion of uncertainty in graphs is not widely adopted. Authors believe that uncertainty can be challenging for the viewers to perceive and understand. As a result, viewers may choose to either look at an alternative graph that does not contain any uncertainty representation or overlook the uncertainty in their graph altogether.\\\\n\\\\n#### 3.2.4 Uncertainty can add clutter to the visualization\\\\n\\\\nAuthors can be unsure of how effective communicating uncertainty is. They also worry about adding more information to an already visually complex visualization. For many authors, the goal of a chart is to express a signal [9] that can be useful to their viewers. This signal tends to present a single point or a single source of truth. Uncertainty tends to challenge that notion by obfuscating the signal. Additionally, expressing the intricacy of uncertainty through a visual abstraction is challenging. The dimensionality of the data also plays a vital role in deciding whether uncertainty should be represented or not. An increase in the dimensionality of data makes it harder for the human visual system to perceive it effectively. Sometimes even two-dimensional charts can be overwhelming for the viewer. In such a case, representing uncertainty adds visual overload [20].\\\\n\\\\n### Visualization Inference\\\\n\\\\nUncertainty is hard to understand and analyze. When faced with perceiving an uncertain visualization, viewers can get confused or derive inaccurate information from it. One easy method viewers tend to use is to ignore the uncertainty in the graph altogether. Another way is to substitute tricky calculations with easy ones or use heuristics to make decisions. However, this may not always give a correct observation. The most common approach to show uncertainty is by using box plots and error bars. Though widely used, viewers may find them challenging to analyze [6]. Sometimes visualizing uncertainty as frequency instead of distribution provide a better understanding.\\\\n\\\\nCurrently, research is being done to create visualizations that help understand uncertainty more intuitively. For example, hypothetical outcome plots (HOPs) represent uncertainty by animating a finite set of individual draws [10]. This approach expects no prior knowledge of the domain from the viewer. However, using HOPs in physical media might be challenging. Bubble treemaps [8] are another approach for visualizing uncertainty. These circular treemaps encode additional information about uncertainty by allocating additional space for visuals.\\\\n\\\\nWhile uncertainty is still underrepresented in visualizations, more researchers are slowly adding it to their designs. One of the significant setbacks in uncertainty visualizations for authors is calculating uncertainty, while for viewers, it is graphical literacy. Efforts can be taken to increase this literacy through different programs gradually. Furthermore, work should be done to understand what visualization type best suits a given uncertainty type. This relationship can also depend on the type of data being represented and the target audience viewing the graph. For example, it is necessary for graphs published in newspapers and reports to be easily understandable by the public. Hence, studies focusing on visualizing uncertainty with no prior knowledge or information can be very insightful.\\\\n\\\\n## 4 Conclusion\\\\n\\\\nUncertainty visualization is one of the most complex research areas in data visualization today. This work provided an overview of uncertainty visualization and the relationship between uncertainty and visualization. I divided the visualization pipeline into four phases and surveyed papers to study how uncertainty interacts with each phase of the process. The work also investigated why the representation of uncertainty is not widely practiced by the data visualization community and the challenges viewers face when inferring from such a graph. Lastly, I discussed a few state-of-the-art methods to design uncertainty visualization and offered a glance into the interesting future research this field has to offer.\\\\n\\'\\n ...\\n \\'# Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models\\\\n\\\\n###### Abstract\\\\n\\\\nWe study the problem of privately estimating the parameters of \\\\\\\\(d\\\\\\\\)-dimensional Gaussian Mixture Models (GMMs) with \\\\\\\\(k\\\\\\\\) components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in the sample complexity and running time. As the main application of our framework, we develop an \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-differentially private algorithm to learn GMMs using the non-private algorithm of Moitra and Valiant [14] as a blackbox. Consequently, this gives the first sample complexity upper bound and first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters.\\\\n\\\\n## 1 Introduction\\\\n\\\\nThe problem of learning the parameters of a Gaussian Mixture Model (GMM) is a fundamental problem in statistics, dating back to the early work of Karl Pearson in 1894 [10]. A GMM with \\\\\\\\(k\\\\\\\\) components in \\\\\\\\(d\\\\\\\\) dimensions can be represented as \\\\\\\\((w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})_{i=1}^{k}\\\\\\\\), where \\\\\\\\(w_{i}\\\\\\\\) is a mixing weight (\\\\\\\\(w_{i}\\\\\\\\geq 0\\\\\\\\), and \\\\\\\\(\\\\\\\\sum_{i\\\\\\\\in[k]}w_{i}=1\\\\\\\\)), \\\\\\\\(\\\\\\\\mu_{i}\\\\\\\\in\\\\\\\\mathbb{R}^{d}\\\\\\\\) is a mean, and \\\\\\\\(\\\\\\\\Sigma_{i}\\\\\\\\in\\\\\\\\mathbb{R}^{d\\\\\\\\times d}\\\\\\\\) is a covariance matrix (of the \\\\\\\\(i\\\\\\\\)-th Gaussian component). To draw a random instance from this GMM, one first samples an index \\\\\\\\(i\\\\\\\\in[k]\\\\\\\\) (with probability \\\\\\\\(w_{i}\\\\\\\\)) and then returns a random sample from the Gaussian distribution \\\\\\\\(\\\\\\\\mathcal{N}(\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})\\\\\\\\). In this work we consider the problem of parameter estimation in the probably approximately correct (PAC) model, where the goal is to \"approximately recover\"1 the parameters of an unknown GMM given only independent samples from it.\\\\n\\\\nFootnote 1: See Definition 1.4 for the precise notion of distance.\\\\n\\\\nThe sample complexity and computational complexity of learning the parameters of GMMs has been studied extensively. A notable breakthrough in this line of work was the development of polynomial-time methods (with respect to \\\\\\\\(d\\\\\\\\)) for learning GMMs [14, 2]. The running time and sample complexity of these methods is exponential \\\\\\\\(k\\\\\\\\), which is necessary for parameter estimation [14].\\\\n\\\\nThe above approaches, however, may not maintain privacy of the individuals whose data has been used for the estimation. To address this issue, we adopt the rigorous and widely accepted notion of differential privacy (DP) [13]. At a high-level, DP ensures that the contribution of each individual has only a small (indistinguishable) effect on the output of the estimator. The classical notion of \\\\\\\\(\\\\\\\\varepsilon\\\\\\\\)-DP (pure DP) is, however, quite restrictive. For instance, even estimating the mean of an unbounded univariate Gaussian random variable in this model is impossible. Therefore, in line with recent work on private estimation in unbounded domains, we consider the \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP (i.e. approximate differential privacy [1]) model.\\\\nFor the simpler case of multivariate Gaussians (without any boundedness assumptions on the parameters), it has been shown that learning with a finite number of samples is possible in the \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP model [1]. More recently, computationally efficient estimators have been devised for the same task [1, 2, 3]. This begs answering the corresponding question for GMMs.\\\\n\\\\nIs there an \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP estimator with a bounded sample complexity for learning unbounded GMMs? Is there a polynomial time estimator (in terms of \\\\\\\\(d\\\\\\\\)) for the same task?\\\\n\\\\nNote that if additional boundedness2 and strong separation3 assumptions are made about the GMM, then the work of [3] offers a positive answer to the above question in the \\\\\\\\(\\\\\\\\varepsilon\\\\\\\\)-DP model. Our aim is, however, learning unbounded GMMs and also with minimal separation assumptions.\\\\n\\\\nFootnote 2: They assume there are known quantities \\\\\\\\(R,\\\\\\\\sigma_{max},\\\\\\\\sigma_{min}\\\\\\\\) such that \\\\\\\\(\\\\\\\\forall i\\\\\\\\in[k],\\\\\\\\|\\\\\\\\mu_{i}\\\\\\\\|_{2}\\\\\\\\leq R\\\\\\\\) and \\\\\\\\(\\\\\\\\sigma_{min}^{2}\\\\\\\\leq||\\\\\\\\Sigma_{i}||\\\\\\\\leq\\\\\\\\sigma_{max}^{2}\\\\\\\\).\\\\n\\\\nFootnote 3: They assume \\\\\\\\(\\\\\\\\forall i\\\\\\\\neq j,||\\\\\\\\mu_{i}-\\\\\\\\mu_{j}||_{2}\\\\\\\\geq\\\\\\\\widehat{\\\\\\\\Omega}\\\\\\\\left(\\\\\\\\sqrt{k}+ \\\\\\\\sqrt{\\\\\\\\frac{1}{w_{i}}+\\\\\\\\frac{1}{w_{j}}}\\\\\\\\right)\\\\\\\\cdot\\\\\\\\max\\\\\\\\left\\\\\\\\{||\\\\\\\\Sigma_{i}^{1/ 2}||,||\\\\\\\\Sigma_{j}^{1/2}||\\\\\\\\right\\\\\\\\}\\\\\\\\).\\\\n\\\\nTo approach this problem, it is natural to ask if there is a general reduction from the private learning of GMMs to its non-private counterpart. If so, this would enable us to easily reuse existing results for non-private learning of GMMs.\\\\n\\\\nIs there a reduction from private to non-private learning of GMMs that incurs only a polynomial time and polynomial sample overhead?\\\\n\\\\nThe main result of this paper is the existence of such a reduction; see Theorem 6.2 for a rigorous version.\\\\n\\\\n**Theorem 1.1** (**Private to Non-private Reduction for GMMs, Informal**).: _There is a reduction from learning the parameters of a GMM in the \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP model to its non-private counterpart. Moreover, this reduction adds only polynomial time and sample overhead in terms of \\\\\\\\(d\\\\\\\\) and \\\\\\\\(k\\\\\\\\)._\\\\n\\\\nThis reduction, along with the non-private learner of [14] gives the first finite sample complexity upper bound for learning the parameters of unbounded GMMs in the \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP model. Moreover, the resulting estimator essentially inherits all the properties of the non-private estimator of [14]; it runs in time that is polynomial in \\\\\\\\(d\\\\\\\\) (for fixed \\\\\\\\(k\\\\\\\\)) and shares the advantage of requiring provably minimal separability assumptions on the components of the GMM.\\\\n\\\\nConcurrent work.In an independent work, [11] offer an \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP method for learning GMMs, removing the boundedness and strong separation requirements of [3]. However, they assume Gaussian components are spherical. We do not make that assumption, and learn the covariance matrices as well.\\\\n\\\\n### Related Work\\\\n\\\\nPrivate Learning of a Single Gaussian.Karwa and Vadhan [10] established polynomial time and sample efficient methods for learning the mean and variance of a univariate Gaussian in both the pure and approximate-DP settings. Namely, in the \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP setting, they can recover the mean and variance of the Gaussian without any boundedness assumption on the parameters. This result can be generalized to the multivariate setting [1, 2], where one finds Gaussians that approximate the underlying Gaussian in terms of total variation distance. However, the sample complexity of these methods depends on the condition number of the covariance matrix, and requires a priori bounds on the range of the parameters. The first finite sample complexity bound for privately learning unbounded Gaussians appeared in [1], nearly matching the sample complexity lower bound of [3]. The work of [1] relies on\\\\na private version of the minimum distance estimator [23] and is based on ideas from the private hypothesis selection method [16]. However, this method is not computationally efficient. Recently, several papers offered \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-DP and computationally efficient algorithms for learning unbounded Gaussians [1, 15, 14], where the work of [1] achieved a near-optimal sample complexity for this task. Part of the approach of [1] is a sub-sample-and-aggregate scheme which we modify and use in this paper. FriendlyCore [13] is an alternative sample-and-aggregate framework that can be used for privately learning unbounded Gaussians. It is noteworthy that the approaches of [1, 12] work in the robust setting as well albeit with sub-optimal sample complexities. The recent work of [1] offers a robust and private learner with near-optimal sample requirements in terms of dimension. Finally, [11] ticks all the boxes by offering a sample near-optimal, robust, and efficient learner for unbounded Gaussians.\\\\n\\\\nAnother related result is a sample-efficient and computationally efficient method for learning bounded and high-dimensional Gaussians in the \\\\\\\\(\\\\\\\\varepsilon\\\\\\\\)-DP model [11]. There is also work on the problem of private mean estimation with respect to Mahalanobis distance [1, 12]. Finding private and robust estimators [13] and also the interplay between robustness and privacy [1, 1, 1, 15, 16] are subjects of a few recent papers.\\\\n\\\\nParameter Learning for GMMs with PAC Guarantees.Given i.i.d. samples from a GMM, can we approximately recover its parameters? There has been an extensive amount of research in developing sample efficient and computationally efficient methods for learning the parameters of a GMM [1, 1, 1, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 222, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 323, 334, 335, 35, 36, 37, 38, 39, 31, 33, 34, 36, 38, 39, 32, 33, 34, 36, 39, 33, 35, 37, 38, 39, 39, 30, 31, 32, 33, 34, 36, 39, 33, 36, 39, 31, 33, 32, 35, 37, 38, 39, 32, 36, 39, 33, 37, 39, 38, 39, 39, 31, 32, 33, 34, 36, 39, 32, 35, 38, 39, 33, 36, 39, 37, 38, 39, 31, 33, 34, 36, 39, 32, 37, 39, 33, 38, 39, 32, 39, 33, 34, 35, 36, 39, 37, 38, 39, 39, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 42, 44, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 74, 75, 76, 77, 78, 79, 80, 81, 82, 82, 83, 84, 85, 86, 87, 88, 89, 90, 82, 84, 85, 86, 87, 88, 89, 91, 80, 83, 84, 85, 87, 89, 80, 84, 86, 88, 89, 80, 85, 87, 88, 89, 81, 82, 84, 85, 86, 88, 87, 89, 82, 88, 89, 80, 83, 84, 85, 88, 89, 80, 84, 86, 89, 81, 83, 85, 87, 88, 89, 80, 84, 88, 89, 82, 85, 86, 87, 88, 89, 82, 89, 83, 84, 85, 88, 89, 80, 86, 87, 88, 88, 89, 80, 87, 88, 89, 80, 88, 89, 81, 82, 83, 84, 85, 86, 88, 89, 82, 87, 88, 88, 89, 80, 88, 89, 82, 89, 83, 84, 85, 86, 87, 88, 89, 80, 88, 89, 81, 84, 86, 88, 89, 82, 85, 87, 88, 82, 86, 89, 83, 88, 89, 80, 84, 88, 85, 89, 86, 87, 88, 89, 82, 89, 83, 84, 86, 88, 87, 89, 80, 88, 85, 89, 80, 86, 87, 88, 89, 82, 89, 80, 88, 89, 80, 89, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 82, 89, 80, 83, 84, 87, 88, 85, 89, 86, 88, 89, 80, 87, 88, 89, 82, 89, 83, 85, 89, 80, 86, 88, 89, 80, 87, 88, 89, 80, 88, 89, 80, 89, 81, 80, 82, 83, 84, 85, 86, 87, 88, 89, 82, 83, 88, 89, 80, 84, 88, 85, 89, 86, 87, 88, 88, 89, 80, 88, 89, 82, 89, 80, 83, 84, 86, 88, 89, 85, 87, 88, 89, 80, 88, 89, 80, 89, 82, 89, 80, 81, 83, 84, 85, 86, 87, 88, 89, 82, 83, 85, 88, 89, 83, 86, 89, 84, 87, 88, 89, 85, 89, 86, 89, 87, 88, 89, 80, 89, 80, 88, 89, 82, 89, 80, 89, 80, 89, 80, 81, 82, 83, 84, 86, 88, 89, 80, 81, 82, 84, 85, 86, 87, 88, 89, 83, 82, 85, 87, 89, 84, 88, 89, 85, 86, 89, 87, 88, 89, 80, 89, 80, 82, 89, 80, 83, 8\\\\n### Preliminaries\\\\n\\\\nWe use \\\\\\\\(\\\\\\\\|v\\\\\\\\|_{2}\\\\\\\\) to denote the Euclidean norm of a vector \\\\\\\\(v\\\\\\\\in\\\\\\\\mathbb{R}^{d}\\\\\\\\) and \\\\\\\\(\\\\\\\\|A\\\\\\\\|_{F}\\\\\\\\) (resp. \\\\\\\\(\\\\\\\\|A\\\\\\\\|\\\\\\\\)) to denote the Frobenius (resp. spectral) norm of a matrix \\\\\\\\(A\\\\\\\\in\\\\\\\\mathbb{R}^{d\\\\\\\\times d}\\\\\\\\).\\\\n\\\\nIn this paper, we write \\\\\\\\(\\\\\\\\mathcal{S}^{d}\\\\\\\\) to denote the positive-definite cone in \\\\\\\\(\\\\\\\\mathbb{R}^{d\\\\\\\\times d}\\\\\\\\). Let \\\\\\\\(\\\\\\\\mathcal{G}(d)=\\\\\\\\{\\\\\\\\mathcal{N}(\\\\\\\\mu,\\\\\\\\Sigma)\\\\\\\\,:\\\\\\\\,\\\\\\\\mu\\\\\\\\in\\\\\\\\mathbb{R}^{d},\\\\\\\\Sigma\\\\\\\\in \\\\\\\\mathcal{S}^{d}\\\\\\\\}\\\\\\\\) be the family of \\\\\\\\(d\\\\\\\\)-dimensional Gaussians. We can now define the class \\\\\\\\(\\\\\\\\mathcal{G}(d,k)\\\\\\\\) of mixtures of Gaussians as follows.\\\\n\\\\n**Definition 1.2** (Gaussian Mixtures).: The class of mixtures of \\\\\\\\(k\\\\\\\\) Guassians in \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\) is defined by \\\\\\\\(\\\\\\\\mathcal{G}(d,k)\\\\\\\\coloneqq\\\\\\\\left\\\\\\\\{\\\\\\\\sum\\\\\\\\limits_{i=1}^{k}w_{i}G_{i}\\\\\\\\,:\\\\\\\\,G_{i}\\\\\\\\in \\\\\\\\mathcal{G}(d),w_{i}\\\\\\\\geq 0,\\\\\\\\sum_{i=1}^{k}w_{i}=1\\\\\\\\right\\\\\\\\}\\\\\\\\).\\\\n\\\\nWe represent the Gaussian Mixture Model (GMM) by a set of \\\\\\\\(k\\\\\\\\) tuples \\\\\\\\(\\\\\\\\left(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i}\\\\\\\\right)_{i=1}^{k}\\\\\\\\), where each tuple represents the mean, covariance matrix, and mixing weight of one of its components. Note that the order of the components is important in our notation, since the order of the output may have an impact on the privacy.\\\\n\\\\nIn the following definition and the remainder of the paper, we may abuse terminology and refer to a distribution via its probability density function (p.d.f.).\\\\n\\\\n**Definition 1.3** (Total Variation Distance).: Given two absolutely continuous probability measures \\\\\\\\(f(x),g(x)\\\\\\\\) on \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\), the total variation (TV) distance between \\\\\\\\(f\\\\\\\\) and \\\\\\\\(g\\\\\\\\) is defined as \\\\\\\\(d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(f(x),g(x)\\\\\\\\right)=\\\\\\\\frac{1}{2}\\\\\\\\int_{\\\\\\\\mathbb{R}^{d}}|f(x)-g (x)|\\\\\\\\,\\\\\\\\mathrm{d}x\\\\\\\\).\\\\n\\\\nA standard way to define the distance between two GMMs is as follows [10][Definition 2].\\\\n\\\\n**Definition 1.4** (The Distance between Two GMMs).: The \\\\\\\\(\\\\\\\\mathrm{dist}_{\\\\\\\\mathrm{GMM}}\\\\\\\\) distance between two GMMs is defined by\\\\n\\\\n\\\\\\\\[\\\\\\\\mathrm{dist}_{\\\\\\\\mathrm{GMM}}\\\\\\\\left(\\\\\\\\left(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i}\\\\\\\\right)_{i=1}^ {k},\\\\\\\\left(w^{\\\\\\\\prime}_{i},\\\\\\\\mu^{\\\\\\\\prime}_{i},\\\\\\\\Sigma^{\\\\\\\\prime}_{i}\\\\\\\\right)_{i=1}^{k }\\\\\\\\right)\\\\\\\\,=\\\\\\\\min_{\\\\\\\\pi}\\\\\\\\max_{i\\\\\\\\in[k]}\\\\\\\\max\\\\\\\\left\\\\\\\\{\\\\\\\\left|w_{i}-w^{{}^{\\\\\\\\prime}}_{ \\\\\\\\pi(i)}\\\\\\\\right|,d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(\\\\\\\\mathcal{N}(\\\\\\\\mu_{i},\\\\\\\\Sigma_{i}),\\\\\\\\mathcal{ N}(\\\\\\\\mu^{{}^{\\\\\\\\prime}}_{\\\\\\\\pi(i)},\\\\\\\\Sigma^{{}^{\\\\\\\\prime}}_{\\\\\\\\pi(i)})\\\\\\\\right)\\\\\\\\,\\\\\\\\right\\\\\\\\}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\pi\\\\\\\\) is chosen from the set of all permutations over \\\\\\\\([k]\\\\\\\\).\\\\n\\\\nIf \\\\\\\\(X\\\\\\\\) (resp. \\\\\\\\(Y\\\\\\\\)) is a random variable distributed according to \\\\\\\\(f\\\\\\\\) (resp. \\\\\\\\(g\\\\\\\\)), we write \\\\\\\\(d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(X,Y\\\\\\\\right)=d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(f,g\\\\\\\\right)\\\\\\\\). We drop the reference to the p.d.f. of the random variable when it is clear or implicit from context.\\\\n\\\\n### Differential Privacy Basics\\\\n\\\\nAt a high-level, an algorithm is differentially private if, given two datasets that differ only in a single element, the output distribution of the algorithm are nearly the same4.\\\\n\\\\nFootnote 4: For sake of simplicity, we consider data sets to be ordered and therefore the neighboring data sets are defined based on their Hamming distances. However, one can easily translate guarantees proven for the ordered setting to the unordered one; see Proposition D.6 in [1].\\\\n\\\\n**Definition 1.5** (Neighbouring Datasets).: Let \\\\\\\\(\\\\\\\\mathcal{X},\\\\\\\\mathcal{Y}\\\\\\\\) denote sets and \\\\\\\\(n\\\\\\\\in\\\\\\\\mathbb{N}\\\\\\\\). Two datasets \\\\\\\\(D=(X_{1},\\\\\\\\ldots,X_{n}),D^{\\\\\\\\prime}=(X_{1},\\\\\\\\ldots,X_{n})\\\\\\\\in\\\\\\\\mathcal{X}^{n}\\\\\\\\) are said to be _neighbouring_ if \\\\\\\\(d_{H}(D,D^{\\\\\\\\prime})\\\\\\\\leq 1\\\\\\\\) where \\\\\\\\(d_{H}\\\\\\\\) denotes Hamming distance, i.e., \\\\\\\\(d_{H}(D,D^{\\\\\\\\prime})=|\\\\\\\\{i\\\\\\\\in[n]\\\\\\\\,:\\\\\\\\,X_{i}\\\\\\\\neq X^{\\\\\\\\prime}_{i}\\\\\\\\}|\\\\\\\\).\\\\n\\\\n**Definition 1.6** (\\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-Indistinguishable).: Let \\\\\\\\(D,D^{\\\\\\\\prime}\\\\\\\\) be two distributions defined on a set \\\\\\\\(\\\\\\\\mathcal{Y}\\\\\\\\). Then \\\\\\\\(D,D^{\\\\\\\\prime}\\\\\\\\) are said to be \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-indistinguishable if for all measurable \\\\\\\\(S\\\\\\\\subseteq\\\\\\\\mathcal{Y}\\\\\\\\), \\\\\\\\(\\\\\\\\mathbb{P}_{Y\\\\\\\\sim D}\\\\\\\\left[Y\\\\\\\\in S\\\\\\\\right]\\\\\\\\leq e^{\\\\\\\\varepsilon}\\\\\\\\mathbb{P}_{Y\\\\\\\\sim D ^{\\\\\\\\prime}}\\\\\\\\left[Y\\\\\\\\in S\\\\\\\\right]+\\\\\\\\delta\\\\\\\\) and \\\\\\\\(\\\\\\\\mathbb{P}_{Y\\\\\\\\sim D^{\\\\\\\\prime}}\\\\\\\\left[Y\\\\\\\\in S\\\\\\\\right]\\\\\\\\leq e^{\\\\\\\\varepsilon}\\\\\\\\mathbb{P} _{Y\\\\\\\\sim D}\\\\\\\\left[Y\\\\\\\\in S\\\\\\\\right]+\\\\\\\\delta\\\\\\\\).\\\\n\\\\n**Definition 1.7** (\\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-Differential Privacy [16]).: A randomized mechanism \\\\\\\\(\\\\\\\\mathcal{M}\\\\\\\\colon\\\\\\\\mathcal{X}^{n}\\\\\\\\to\\\\\\\\mathcal{Y}\\\\\\\\) is said to be \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-differentially private if for all neighbouring datasets \\\\\\\\(D,D^{\\\\\\\\prime}\\\\\\\\in\\\\\\\\mathcal{X}^{n}\\\\\\\\), \\\\\\\\(\\\\\\\\mathcal{M}(D)\\\\\\\\) and \\\\\\\\(\\\\\\\\mathcal{M}(D^{\\\\\\\\prime})\\\\\\\\) are \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-indistinguishable.\\\\n### Techniques\\\\n\\\\nThe techniques in this paper are inspired by the techniques in [1] which are based on the Propose-Test-Release framework [14] and the Subsample-And-Aggregate framework [20]. Given a dataset \\\\\\\\(D\\\\\\\\), we first split \\\\\\\\(D\\\\\\\\) into \\\\\\\\(t\\\\\\\\) sub-datasets and run a non-private algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) on each of the sub-datasets. Next, we privately check if most of the outputs of \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) are \"well-clustered\" (i.e., are close to each other). If not, then the algorithm fails as this suggests that the outputs of the non-private algorithm is not very stable (either due to lack of data or simply that the non-private algorithm is sensitive to its input). On the other hand, if most of the outputs are well-clustered then we can aggregate these clustered outputs and release a noisy version of it. There are, however, multiple additional technical challenges that need to be addressed.\\\\n\\\\nOne core difficulty is the issue of the ordering of the Gaussian components. Namely, the non-private GMM learners may output GMM components in different orders. Therefore, aggregating these non-private solutions (e.g., by taking their weighted average in the style of [1]) seems impossible. We therefore propose to skip the aggregation step all together by simply picking an arbitrary solution from the cluster. Therefore, our private populous mean estimator (PPE) simplifies and generalizes the private populous mean estimator (PPME) framework of [1], making it applicable to general semimetric spaces (and therefore GMMs). A precise discussion of this framework is presented in Subsection 2.1.\\\\n\\\\nAnother challenge is designing an appropriate mechanism for adding noise to GMMs. As discussed above, our framework requires that we are able to release a noisy output of a candidate output. More precisely, given two neighbouring datasets \\\\\\\\(Y_{1},Y_{2}\\\\\\\\), we want to design a mechanism \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) such that \\\\\\\\(\\\\\\\\mathcal{B}(Y_{1})\\\\\\\\), \\\\\\\\(\\\\\\\\mathcal{B}(Y_{2})\\\\\\\\) are indistinguishable whenever \\\\\\\\(Y_{1},Y_{2}\\\\\\\\) are sufficiently close. As in [1], we refer to such a mechanism as a \"masking mechanism\". In the context of mixture distributions with \\\\\\\\(k\\\\\\\\) components, a candidate output corresponds to a \\\\\\\\(k\\\\\\\\)-tuple where each element of the tuple contain the parameters and the mixing weight of a single component. We prove that, if one can design a masking mechanism for a _single_ component then it is possible to use this masking mechanism as a blackbox to design a masking mechanism for the \\\\\\\\(k\\\\\\\\)-tuple with only a \\\\\\\\(\\\\\\\\text{poly}(k)\\\\\\\\) overhead in the running time. One important ingredient is that we randomly shuffle the components, making the output invariant to the order of the components.\\\\n\\\\nAnother challenge related to the order of components is that computing the distance between two GMMs based on Definition 1.4 requires minimizing over all permutations. A naive method for computing this distance could require exponential time but we show this task can be done in polynomial time using a simple reduction to bipartite matching.\\\\n\\\\nTo showcase the utility of the above framework, we show that it is straightforward to apply the framework to privately learning mixtures of Gaussians. We design a masking mechanism of a single Gaussian component which consists of mixing the weight, the mean, and the covariance matrix. Masking the mixing weight is fairly standard while masking the mean and the covariance matrix can be done using known results (e.g. by using [1][Lemma 5.2] for the covariance matrix and a similar technique for the mean).\\\\n\\\\nFinally, we note that, in some of the literature for Gaussian mixtures, the results usually assert that for each Gaussian component \\\\\\\\(\\\\\\\\mathcal{N}(\\\\\\\\mu,\\\\\\\\Sigma)\\\\\\\\), the algorithm returns \\\\\\\\(\\\\\\\\hat{\\\\\\\\mu},\\\\\\\\hat{\\\\\\\\Sigma}\\\\\\\\) such that \\\\\\\\(\\\\\\\\mathcal{N}(\\\\\\\\mu,\\\\\\\\Sigma)\\\\\\\\) and \\\\\\\\(\\\\\\\\mathcal{N}(\\\\\\\\hat{\\\\\\\\mu},\\\\\\\\hat{\\\\\\\\Sigma})\\\\\\\\) are close in _total variation_ distance (e.g. [13]). Our framework requires that \\\\\\\\(\\\\\\\\hat{\\\\\\\\mu}\\\\\\\\) (resp. \\\\\\\\(\\\\\\\\hat{\\\\\\\\Sigma}\\\\\\\\)) is close to \\\\\\\\(\\\\\\\\mu\\\\\\\\) (resp. \\\\\\\\(\\\\\\\\Sigma\\\\\\\\)) for some appropriate norm. Intuitively, this ought to be the case but no tight characterization was previously known unless the Gaussians had the same mean [13][Theorem 1.1]. In this paper, we prove the following tight characterization between the TV distance of a Gaussian and its parameters. We believe that such a result may be of independent interest.\\\\n\\\\n**Theorem 1.8**.: _Let \\\\\\\\(\\\\\\\\mu_{1},\\\\\\\\mu_{2}\\\\\\\\in\\\\\\\\mathbb{R}^{d}\\\\\\\\) and \\\\\\\\(\\\\\\\\Sigma_{1},\\\\\\\\Sigma_{2}\\\\\\\\) be \\\\\\\\(d\\\\\\\\times d\\\\\\\\) positive-definite matrices. Suppose that we have \\\\\\\\(d_{\\\\\\\\mathrm{TV}}(\\\\\\\\mathcal{N}(\\\\\\\\mu_{1},\\\\\\\\Sigma_{1}),\\\\\\\\mathcal{N}(\\\\\\\\mu_{2},\\\\\\\\Sigma_{ 2}))<\\\\\\\\frac{1}{600}\\\\\\\\). Let_\\\\n\\\\n\\\\\\\\[\\\\\\\\Delta=\\\\\\\\max\\\\\\\\left\\\\\\\\{\\\\\\\\|\\\\\\\\Sigma_{1}^{-1/2}\\\\\\\\Sigma_{2}\\\\\\\\Sigma_{1}^{-1/2}-I_{d}\\\\\\\\|_{F}, \\\\\\\\|\\\\\\\\Sigma_{1}^{-1}(\\\\\\\\mu_{1}-\\\\\\\\mu_{2})\\\\\\\\|_{2}\\\\\\\\right\\\\\\\\}.\\\\\\\\]\\\\n_Then_\\\\n\\\\n\\\\\\\\[\\\\\\\\frac{1}{200}\\\\\\\\Delta\\\\\\\\leq d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(\\\\\\\\mathcal{N}(\\\\\\\\mu_{1},\\\\\\\\Sigma_{1}), \\\\\\\\mathcal{N}(\\\\\\\\mu_{2},\\\\\\\\Sigma_{2})\\\\\\\\right)\\\\\\\\leq\\\\\\\\frac{1}{\\\\\\\\sqrt{2}}\\\\\\\\Delta.\\\\\\\\]\\\\n\\\\n## 2 Private Populous Estimator\\\\n\\\\nIn this section, we describe our main framework which we call the \"private populous estimator\" (PPE). Before that, we need a few definitions.\\\\n\\\\nSemimetric spaces.In our application, we need to deal with distance functions which only satisfy an _approximate_ triangle inequality that hold only when the points are sufficiently close together. To that end, we first define the notion of a semimetric space.\\\\n\\\\n**Definition 2.1** (Semimetric Space).: We say \\\\\\\\((\\\\\\\\mathcal{F},\\\\\\\\mathrm{dist})\\\\\\\\) is a semimetric space if for every \\\\\\\\(F,F_{1},F_{2},F_{3}\\\\\\\\in\\\\\\\\mathcal{F}\\\\\\\\), the following conditions hold.\\\\n\\\\n1. **Non-negativity.**\\\\\\\\(\\\\\\\\mathrm{dist}(F,F)=0\\\\\\\\); \\\\\\\\(\\\\\\\\mathrm{dist}(F_{1},F_{2})\\\\\\\\geq 0\\\\\\\\).\\\\n2. **Symmetry.**\\\\\\\\(\\\\\\\\mathrm{dist}(F_{1},F_{2})=\\\\\\\\mathrm{dist}(F_{2},F_{1})\\\\\\\\).\\\\n3. \\\\\\\\(z\\\\\\\\)**-approximate \\\\\\\\(r\\\\\\\\)-restricted triangle inequality.** Let \\\\\\\\(r>0\\\\\\\\) and \\\\\\\\(z\\\\\\\\geq 1\\\\\\\\). If \\\\\\\\(\\\\\\\\mathrm{dist}(F_{1},F_{2}),\\\\\\\\mathrm{dist}(F_{2},F_{3})\\\\\\\\leq r\\\\\\\\) then \\\\\\\\(\\\\\\\\mathrm{dist}(F_{1},F_{3})\\\\\\\\leq z\\\\\\\\cdot(\\\\\\\\mathrm{dist}(F_{1},F_{2})+\\\\\\\\mathrm{dist} (F_{2},F_{3}))\\\\\\\\).\\\\n\\\\nMasking mechanism.Intuitively, a masking mechanism \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is a random function that returns a noisy version of its input, with the goal of making close inputs indistinguishable. Formally, we define a masking mechanism as follows.\\\\n\\\\n**Definition 2.2** (Masking Mechanism [2], Definition 3.3).: Let \\\\\\\\((\\\\\\\\mathcal{F},\\\\\\\\mathrm{dist})\\\\\\\\) be a semimetric space. A randomized function \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\colon\\\\\\\\mathcal{F}\\\\\\\\to\\\\\\\\mathcal{F}\\\\\\\\) is a \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism for \\\\\\\\((\\\\\\\\mathcal{F},\\\\\\\\mathrm{dist})\\\\\\\\) if for all \\\\\\\\(F,F^{\\\\\\\\prime}\\\\\\\\in\\\\\\\\mathcal{F}\\\\\\\\) satisfying \\\\\\\\(\\\\\\\\mathrm{dist}(F,F^{\\\\\\\\prime})\\\\\\\\leq\\\\\\\\gamma\\\\\\\\), we have that \\\\\\\\(\\\\\\\\mathcal{B}(F),\\\\\\\\mathcal{B}(F^{\\\\\\\\prime})\\\\\\\\) are \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-indistinguishable. Further, \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is said to be \\\\\\\\((\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\)-concentrated if for all \\\\\\\\(F\\\\\\\\in\\\\\\\\mathcal{F}\\\\\\\\), \\\\\\\\(\\\\\\\\mathbb{P}[\\\\\\\\mathrm{dist}(\\\\\\\\mathcal{B}(F),F)>\\\\\\\\alpha]\\\\\\\\leq\\\\\\\\beta\\\\\\\\).\\\\n\\\\n### The Private Populous Estimator (PPE)\\\\n\\\\nIn this section, we define the PPE framework which allows us to use non-private algorithms to design private algorithms. We represent the non-private algorithm by \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\colon\\\\\\\\mathcal{X}^{*}\\\\\\\\to\\\\\\\\mathcal{Y}\\\\\\\\) which takes elements from a dataset as inputs and outputs an element in \\\\\\\\(\\\\\\\\mathcal{Y}\\\\\\\\). PPE requires two assumptions. Firstly, we assume that \\\\\\\\((\\\\\\\\mathcal{Y},\\\\\\\\mathrm{dist})\\\\\\\\) is a semimetric space. Secondly, we assume that we have access to an efficient masking mechanism for \\\\\\\\((\\\\\\\\mathcal{Y},\\\\\\\\mathrm{dist})\\\\\\\\).\\\\n\\\\nThe PPE framework we introduce in this section can be seen as a somewhat generalized version of the framework used in [2] and requires fewer assumptions. Given a dataset \\\\\\\\(D\\\\\\\\) as inputs, we partition \\\\\\\\(D\\\\\\\\) into \\\\\\\\(t\\\\\\\\) disjoint subsets. Next, we run the non-private algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) on each of these subsets to produce \\\\\\\\(t\\\\\\\\) outputs \\\\\\\\(Y_{1},\\\\\\\\ldots,Y_{t}\\\\\\\\). We then privately check if most of the \\\\\\\\(t\\\\\\\\) outputs are close to each other. If not, PPE fails. Otherwise, it chooses a \\\\\\\\(Y_{j}\\\\\\\\) that is close to more than \\\\\\\\(60\\\\\\\\%\\\\\\\\) of other \\\\\\\\(Y_{i}\\\\\\\\)\\\\\\'s. It then adds noise to \\\\\\\\(Y_{j}\\\\\\\\) using a masking mechanism \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\), and returns the masked version of \\\\\\\\(Y_{j}\\\\\\\\). The formal details of the algorithm can be found in Algorithm 1.\\\\n\\\\nThe following theorem establishes the privacy and accuracy of Algorithm 1. The proof can be found in Appendix D.1.\\\\n\\\\n**Theorem 2.3**.: _Suppose that \\\\\\\\((\\\\\\\\mathcal{Y},\\\\\\\\mathrm{dist})\\\\\\\\) satisfies a \\\\\\\\(z\\\\\\\\)-approximate \\\\\\\\(r\\\\\\\\)-restricted triangle inequality. Further, suppose that \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is a \\\\\\\\((r,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism._\\\\n\\\\n* **Privacy.** _For_ \\\\\\\\(t>5\\\\\\\\)_, Algorithm_ 1 _is_ \\\\\\\\((2\\\\\\\\varepsilon,4e^{\\\\\\\\varepsilon}\\\\\\\\delta)\\\\\\\\)_-DP._\\\\n* **Utility.**_Suppose \\\\\\\\(\\\\\\\\alpha\\\\\\\\leq r/2z\\\\\\\\) and \\\\\\\\(t\\\\\\\\geq(\\\\\\\\frac{20}{\\\\\\\\varepsilon}\\\\\\\\ln\\\\\\\\left(1+\\\\\\\\frac{\\\\\\\\varepsilon^{\\\\\\\\varepsilon}-1}{2 \\\\\\\\delta}\\\\\\\\right)\\\\\\\\). Let \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) be \\\\\\\\((\\\\\\\\alpha/2z,\\\\\\\\beta)\\\\\\\\)-concentrated. If there exists \\\\\\\\(Y^{*}\\\\\\\\) with the property that for all \\\\\\\\(i\\\\\\\\in[t],\\\\\\\\mathrm{dist}(Y^{*},Y_{i})<\\\\\\\\alpha/2z\\\\\\\\), then \\\\\\\\(\\\\\\\\mathbb{P}\\\\\\\\left[\\\\\\\\mathrm{dist}(\\\\\\\\widetilde{Y},Y^{*})>\\\\\\\\alpha\\\\\\\\right]\\\\\\\\leq\\\\\\\\beta\\\\\\\\)._\\\\n\\\\nThe utility guarantee asserts that if the outcome of all non-private procedures are close to each other, then the output of the PPE will be close to those non-private outcomes.\\\\n\\\\n**Remark 2.4**.: _Let \\\\\\\\(T_{\\\\\\\\mathcal{A}}\\\\\\\\) be the running time of the algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) in Line 2, \\\\\\\\(T_{\\\\\\\\mathrm{dist}}\\\\\\\\) be the time to compute \\\\\\\\(\\\\\\\\mathrm{dist}(Y_{i},Y_{j})\\\\\\\\) for any \\\\\\\\(Y_{i},Y_{j}\\\\\\\\in\\\\\\\\mathcal{Y}\\\\\\\\) in Line 3, and \\\\\\\\(T_{\\\\\\\\mathcal{B}}\\\\\\\\) be the time to compute \\\\\\\\(\\\\\\\\widetilde{Y}\\\\\\\\) in Line 9. Then Algorithm 1 runs in time \\\\\\\\(O(t\\\\\\\\cdot T_{\\\\\\\\mathcal{A}}+t^{2}\\\\\\\\cdot T_{\\\\\\\\mathrm{dist}}+T_{\\\\\\\\mathcal{B}})\\\\\\\\). We will see that \\\\\\\\(T_{\\\\\\\\mathcal{A}}\\\\\\\\), \\\\\\\\(T_{\\\\\\\\mathcal{B}}\\\\\\\\), and \\\\\\\\(T_{\\\\\\\\mathrm{dist}}\\\\\\\\) can be polynomially bounded for GMMs._\\\\n\\\\nTo apply Algorithm 1 for private learning of GMMs, we need to introduce a masking mechanism for them. In order to do that, we start by defining a masking mechanism for a single Gaussian component (presented in Section 4). We then show how one can convert a masking mechanism for a component to one for mixtures (Section 3). Finally, we apply this to come up with a masking mechanism for GMMs as shown in Section 5.\\\\n\\\\n## 3 Masking Mixtures\\\\n\\\\nThe goal of this section is to show how to \"lift\" a masking mechanism for a single component to a masking mechanism for mixtures. We can do this by adding noise to each of the components and randomly permute the output components.\\\\n\\\\nFormally, let \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) denote a space and let \\\\\\\\(\\\\\\\\mathcal{F}^{k}=\\\\\\\\mathcal{F}\\\\\\\\times\\\\\\\\ldots\\\\\\\\times\\\\\\\\mathcal{F}\\\\\\\\) (\\\\\\\\(k\\\\\\\\) times). The following definition is useful in defining the distance between two mixtures, as it is invariant to the order of components.\\\\n\\\\n**Definition 3.1**.: Let \\\\\\\\(\\\\\\\\mathrm{dist}\\\\\\\\) denote a distance function on \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\). We define \\\\\\\\(\\\\\\\\mathrm{dist}^{k}\\\\\\\\colon\\\\\\\\mathcal{F}^{k}\\\\\\\\times\\\\\\\\mathcal{F}^{k}\\\\\\\\to\\\\\\\\mathbb{R}_{ \\\\\\\\geq 0}\\\\\\\\) as\\\\n\\\\n\\\\\\\\[\\\\\\\\mathrm{dist}^{k}((F_{1},\\\\\\\\ldots,F_{k}),(F_{1}^{\\\\\\\\prime},\\\\\\\\ldots,F_{k}^{\\\\\\\\prime}) )\\\\\\\\coloneqq\\\\\\\\min_{\\\\\\\\pi}\\\\\\\\max_{i\\\\\\\\in[k]}\\\\\\\\mathrm{dist}(F_{i},F_{\\\\\\\\pi(i)}^{\\\\\\\\prime}),\\\\\\\\]\\\\n\\\\nwhere the minimization is taken over all permutations \\\\\\\\(\\\\\\\\pi\\\\\\\\).\\\\n\\\\nNote that computing \\\\\\\\(\\\\\\\\mathrm{dist}^{k}\\\\\\\\) requires computing a minimum over all permutations \\\\\\\\(\\\\\\\\pi\\\\\\\\). Naively, one might assume that this requires exponential time to try all permutations. However, it turns out that one can reduce the problem of computing \\\\\\\\(\\\\\\\\mathrm{dist}^{k}\\\\\\\\) to deciding whether a perfect matching exists in a weighted bipartite graph. The details of this argument can be found in Appendix E.3.\\\\n**Lemma 3.2**.: _If \\\\\\\\(T_{\\\\\\\\rm dist}\\\\\\\\) is the running time to compute \\\\\\\\({\\\\\\\\rm dist}\\\\\\\\) then \\\\\\\\({\\\\\\\\rm dist}^{k}\\\\\\\\) can be computed in time \\\\\\\\(O(k^{2}T_{\\\\\\\\rm dist}+k^{3}\\\\\\\\log k)\\\\\\\\)._\\\\n\\\\nThe following definition is useful for extending a masking mechanism for a component to a masking mechanism for a mixture. The important thing is that the components are shuffled randomly in this mechanism, making the outcome independent of the original order of the components.\\\\n\\\\n**Definition 3.3**.: Suppose that \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is a \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism for \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\). We define the mechanism \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) as \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}(F_{1},\\\\\\\\ldots,F_{k})=(\\\\\\\\mathcal{B}(F_{\\\\\\\\sigma(1)}), \\\\\\\\ldots,\\\\\\\\mathcal{B}(F_{\\\\\\\\sigma(k)}))\\\\\\\\), where \\\\\\\\(\\\\\\\\sigma\\\\\\\\) is a uniform random permutation.\\\\n\\\\nWe also note that \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) can be computed with only polynomial overhead. The proof can be found in Appendix E.2.\\\\n\\\\n**Lemma 3.4**.: _If \\\\\\\\(T_{\\\\\\\\mathcal{B}}\\\\\\\\) is the running time of \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) then \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) can be computed in time \\\\\\\\(O(k\\\\\\\\cdot T_{\\\\\\\\mathcal{B}}+k\\\\\\\\log k)\\\\\\\\)._\\\\n\\\\nThe next lemma shows that \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) is indeed a masking mechanism w.r.t. \\\\\\\\((\\\\\\\\mathcal{F}^{k},{\\\\\\\\rm dist}^{k})\\\\\\\\) and that \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) is accurate provided that \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is accurate.\\\\n\\\\n**Lemma 3.5**.: _If \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is an \\\\\\\\((\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\)-concentrated \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism for \\\\\\\\((\\\\\\\\mathcal{F},{\\\\\\\\rm dist})\\\\\\\\) then, for any \\\\\\\\(\\\\\\\\delta^{\\\\\\\\prime}>0\\\\\\\\), \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) is an \\\\\\\\((\\\\\\\\alpha,k\\\\\\\\beta)\\\\\\\\)-concentrated \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon^{\\\\\\\\prime},k\\\\\\\\delta+\\\\\\\\delta^{\\\\\\\\prime})\\\\\\\\)-masking mechanism for \\\\\\\\((\\\\\\\\mathcal{F}^{k},{\\\\\\\\rm dist}^{k})\\\\\\\\) where_\\\\n\\\\n\\\\\\\\[\\\\\\\\varepsilon^{\\\\\\\\prime}=\\\\\\\\sqrt{2k\\\\\\\\ln(1/\\\\\\\\delta^{\\\\\\\\prime})}\\\\\\\\varepsilon+k\\\\\\\\varepsilon( e^{\\\\\\\\varepsilon}-1).\\\\\\\\]\\\\n\\\\nProof.: First, we prove privacy. Let \\\\\\\\(F=(F_{1},\\\\\\\\ldots,F_{k})\\\\\\\\in\\\\\\\\mathcal{F}^{k}\\\\\\\\) and \\\\\\\\(F^{\\\\\\\\prime}=(F^{\\\\\\\\prime}_{1},\\\\\\\\ldots,F^{\\\\\\\\prime}_{k})\\\\\\\\in\\\\\\\\mathcal{F}_{k}\\\\\\\\) be such that \\\\\\\\({\\\\\\\\rm dist}^{k}(F,F^{\\\\\\\\prime})\\\\\\\\leq\\\\\\\\gamma\\\\\\\\). In other words, there exists a permutation \\\\\\\\(\\\\\\\\pi\\\\\\\\) such that \\\\\\\\({\\\\\\\\rm dist}(F_{i},F^{\\\\\\\\prime}_{\\\\\\\\pi(i)})\\\\\\\\leq\\\\\\\\gamma\\\\\\\\) for all \\\\\\\\(i\\\\\\\\in[k]\\\\\\\\). Since \\\\\\\\(\\\\\\\\mathcal{B}\\\\\\\\) is a \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism, we know that \\\\\\\\(\\\\\\\\mathcal{B}(F_{i}),\\\\\\\\mathcal{B}(F^{\\\\\\\\prime}_{\\\\\\\\pi(i)})\\\\\\\\) are \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-indistinguishable. Thus, by advanced composition (see Theorem C.7), \\\\\\\\((\\\\\\\\mathcal{B}(F_{1}),\\\\\\\\ldots,\\\\\\\\mathcal{B}(F_{k}))\\\\\\\\) and \\\\\\\\((\\\\\\\\mathcal{B}(F^{\\\\\\\\prime}_{\\\\\\\\pi(1)}),\\\\\\\\ldots,\\\\\\\\mathcal{B}(F^{\\\\\\\\prime}_{\\\\\\\\pi(k)}))\\\\\\\\) are \\\\\\\\((\\\\\\\\varepsilon^{\\\\\\\\prime},k\\\\\\\\delta+\\\\\\\\delta^{\\\\\\\\prime})\\\\\\\\)-indistinguishable with \\\\\\\\(\\\\\\\\varepsilon^{\\\\\\\\prime}\\\\\\\\) as stated in the lemma. Since \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}((F^{\\\\\\\\prime}_{1},\\\\\\\\ldots,F^{\\\\\\\\prime}_{k}))\\\\\\\\) has the same distribution has \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}((F^{\\\\\\\\prime}_{\\\\\\\\pi(1)},\\\\\\\\ldots,F^{\\\\\\\\prime}_{\\\\\\\\pi(k)}))\\\\\\\\), we conclude, using the fact that permutation preserves privacy (see Lemma C.8), that \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}(F)\\\\\\\\) and \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}(F^{\\\\\\\\prime})\\\\\\\\) are \\\\\\\\((\\\\\\\\varepsilon^{\\\\\\\\prime},k\\\\\\\\delta+\\\\\\\\delta^{\\\\\\\\prime})\\\\\\\\)-indistinguishable.\\\\n\\\\nFinally, it remains to prove accuracy (i.e. that \\\\\\\\(\\\\\\\\mathcal{B}^{k}_{\\\\\\\\sigma}\\\\\\\\) is \\\\\\\\((\\\\\\\\alpha,k\\\\\\\\beta)\\\\\\\\)-concentrated). Indeed, given \\\\\\\\(F=(F_{1},\\\\\\\\ldots,F_{k})\\\\\\\\in\\\\\\\\mathcal{F}^{k}\\\\\\\\), we know that \\\\\\\\({\\\\\\\\rm dist}(\\\\\\\\mathcal{B}(F_{i}),F_{i})\\\\\\\\leq\\\\\\\\alpha\\\\\\\\) with probability at least \\\\\\\\(1-\\\\\\\\beta\\\\\\\\). Thus, by a union bound \\\\\\\\({\\\\\\\\rm dist}(\\\\\\\\mathcal{B}(F_{i}),F_{i})\\\\\\\\leq\\\\\\\\alpha\\\\\\\\) for all \\\\\\\\(i\\\\\\\\in[k]\\\\\\\\) with probability at least \\\\\\\\(1-k\\\\\\\\beta\\\\\\\\). We conclude that \\\\\\\\({\\\\\\\\rm dist}(\\\\\\\\mathcal{B}(F),F)\\\\\\\\leq\\\\\\\\alpha\\\\\\\\) with probability at least \\\\\\\\(1-k\\\\\\\\beta\\\\\\\\). \\\\n\\\\nRecall that Theorem 2.3 requires that the distance function satisfies an \\\\\\\\(r\\\\\\\\)-restricted \\\\\\\\(z\\\\\\\\)-approximate. The following lemma shows that \\\\\\\\({\\\\\\\\rm dist}^{k}\\\\\\\\) indeed does satisfy this property provided that \\\\\\\\({\\\\\\\\rm dist}\\\\\\\\) does. The proof can be found in Appendix E.1.\\\\n\\\\n**Lemma 3.6**.: _If \\\\\\\\({\\\\\\\\rm dist}\\\\\\\\) satisfies an \\\\\\\\(r\\\\\\\\)-restricted \\\\\\\\(z\\\\\\\\)-approximate triangle inequality then so does \\\\\\\\({\\\\\\\\rm dist}^{k}\\\\\\\\)._\\\\n\\\\n## 4 Masking a Single Gaussian Component\\\\n\\\\nIn this section, we develop a masking mechanism for a single Gaussian component. In the following section, we utilize this masking mechanism combined with the general purpose mechanism in Section 3 to develop a masking mechanism for GMMs.\\\\n\\\\nLet \\\\\\\\(\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Comp}}=\\\\\\\\mathbb{R}\\\\\\\\times\\\\\\\\mathbb{R}^{d}\\\\\\\\times\\\\\\\\mathbb{R}^{d \\\\\\\\times d}\\\\\\\\) (corresponding to the weight \\\\\\\\(w\\\\\\\\), mean \\\\\\\\(\\\\\\\\mu\\\\\\\\), and covariance matrix \\\\\\\\(\\\\\\\\Sigma\\\\\\\\), respectively). Define \\\\\\\\({\\\\\\\\rm dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\colon\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Comp}}\\\\\\\\times\\\\\\\\mathcal{F}_ {\\\\\\\\textsc{Comp}}\\\\\\\\to\\\\\\\\mathbb{R}_{\\\\\\\\geq 0}\\\\\\\\) as\\\\n\\\\n\\\\\\\\[{\\\\\\\\rm dist}_{\\\\\\\\textsc{Comp}}((w_{1},\\\\\\\\mu_{1},\\\\\\\\Sigma_{1}),(w_{2},\\\\\\\\mu_{2},\\\\\\\\Sigma_{2}) )=\\\\\\\\max\\\\\\\\{|w_{1}-w_{2}|,{\\\\\\\\rm dist}_{\\\\\\\\textsc{Mean}}((\\\\\\\\mu_{1},\\\\\\\\Sigma_{1}),(\\\\\\\\mu_{2},\\\\\\\\Sigma_{2})),{\\\\\\\\rm dist}_{\\\\\\\\textsc{Cov}}(\\\\\\\\Sigma_{1},\\\\\\\\Sigma_{2})\\\\\\\\},\\\\\\\\]\\\\nwhere\\\\n\\\\n\\\\\\\\[\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Conv}}(\\\\\\\\Sigma_{1},\\\\\\\\Sigma_{2})=\\\\\\\\max\\\\\\\\{\\\\\\\\|\\\\\\\\Sigma_{1}^{1/ 2}\\\\\\\\Sigma_{2}^{-1}\\\\\\\\Sigma_{1}^{1/2}-I_{d}\\\\\\\\|_{F},\\\\\\\\|\\\\\\\\Sigma_{2}^{1/2}\\\\\\\\Sigma_{1}^{-1} \\\\\\\\Sigma_{2}^{1/2}-I_{d}\\\\\\\\|_{F}\\\\\\\\}\\\\\\\\]\\\\n\\\\nand\\\\n\\\\n\\\\\\\\[\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Mean}}((\\\\\\\\mu_{1},\\\\\\\\Sigma_{1}),(\\\\\\\\mu_{2},\\\\\\\\Sigma_{2}) )=\\\\\\\\max\\\\\\\\{\\\\\\\\|\\\\\\\\mu_{1}-\\\\\\\\mu_{2}\\\\\\\\|_{\\\\\\\\Sigma_{1}},\\\\\\\\|\\\\\\\\mu_{1}-\\\\\\\\mu_{2}\\\\\\\\|_{\\\\\\\\Sigma_{2}}\\\\\\\\}.\\\\\\\\]\\\\n\\\\nFirst, we show that \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\) satisfies an approximate triangle inequality; this is useful in order to use Theorem 2.3.\\\\n\\\\n**Lemma 4.1**.: \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\) _satisfies a \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality._\\\\n\\\\nProof.: For any positive-definite matrix \\\\\\\\(\\\\\\\\Sigma\\\\\\\\), \\\\\\\\(\\\\\\\\|\\\\\\\\cdot\\\\\\\\|_{\\\\\\\\Sigma}\\\\\\\\) is a metric and thus, \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Mean}}\\\\\\\\) is a metric (and therefore satisfies the \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality). Next, \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Cov}}\\\\\\\\) satisfies the \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality (see Lemma A.8). A straightforward calculation concludes that, as a result, \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\) also satisfies a \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality. \\\\n\\\\nThe following lemma gives a masking mechanism for a single Gaussian mechanism. The proof can be found Appendix F. The mechanism essentially noises the mixing weight, the mean, and the covariance matrix separately. For noising the mixing weight, one can do this using the Gaussian mechanism. Care must be taken to noise the mean and the covariance matrix. In both cases, we use the empirical covariance matrix itself to re-scale both the mean and the covariance matrix. Pseudocode for the various pieces can be found in Algorithm 2 (the first four functions). Note that the parameters \\\\\\\\(\\\\\\\\eta_{\\\\\\\\textsc{N}},\\\\\\\\eta_{\\\\\\\\textsc{Mean}},\\\\\\\\eta_{\\\\\\\\textsc{Cov}}\\\\\\\\) must be set correctly to ensure privacy and accuracy but these details are relegated to Appendix F.\\\\n\\\\n**Lemma 4.2**.: _For \\\\\\\\(\\\\\\\\gamma\\\\\\\\leq\\\\\\\\frac{\\\\\\\\varepsilon\\\\\\\\alpha}{C_{2}\\\\\\\\sqrt{d(d+\\\\\\\\ln(4/\\\\\\\\beta))\\\\\\\\cdot\\\\\\\\ln(2/ \\\\\\\\delta)}}\\\\\\\\), there exists a \\\\\\\\((\\\\\\\\gamma,3\\\\\\\\varepsilon,3\\\\\\\\delta)\\\\\\\\)-masking mechanism, \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Comp}}\\\\\\\\), for \\\\\\\\((\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Comp}},\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}})\\\\\\\\) that is \\\\\\\\((\\\\\\\\alpha,3\\\\\\\\beta)\\\\\\\\)-concentrated, where \\\\\\\\(C_{2}\\\\\\\\) is a universal constant._\\\\n\\\\n## 5 A Masking Mechanism for GMMs\\\\n\\\\nIn this section, we show how to mask a mixture of \\\\\\\\(k\\\\\\\\) Gaussians. Let \\\\\\\\(\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Gmm}}=\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Comp}}\\\\\\\\times\\\\\\\\ldots\\\\\\\\times \\\\\\\\mathcal{F}_{\\\\\\\\textsc{Comp}}\\\\\\\\) (\\\\\\\\(k\\\\\\\\) times). Note we drop \\\\\\\\(k\\\\\\\\) from \\\\\\\\(\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) (and related notation below) since \\\\\\\\(k\\\\\\\\) is fixed and implied from context. Let \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\) be as defined in Eq. (4) and define the distance\\\\n\\\\n\\\\\\\\[\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}(\\\\\\\\{(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})\\\\\\\\}_{i\\\\\\\\in[k]}, \\\\\\\\{(w^{\\\\\\\\prime}_{i},\\\\\\\\mu^{\\\\\\\\prime}_{i},\\\\\\\\Sigma^{\\\\\\\\prime}_{i})\\\\\\\\}_{i\\\\\\\\in[k]})=\\\\\\\\min_{ \\\\\\\\pi}\\\\\\\\max_{i\\\\\\\\in[k]}\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}((w_{\\\\\\\\pi(i)},\\\\\\\\mu_{\\\\\\\\pi(i)}, \\\\\\\\Sigma_{\\\\\\\\pi(i)}),(w^{\\\\\\\\prime}_{i},\\\\\\\\mu^{\\\\\\\\prime}_{i},\\\\\\\\Sigma^{\\\\\\\\prime}_{i})),\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\pi\\\\\\\\) is chosen from the set of all permutations over \\\\\\\\([k]\\\\\\\\). Now define the masking mechanism\\\\n\\\\n\\\\\\\\[\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}(\\\\\\\\{(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})\\\\\\\\}_{i\\\\\\\\in[k]})=\\\\\\\\{ \\\\\\\\mathcal{B}_{\\\\\\\\textsc{Comp}}(w_{\\\\\\\\sigma(i)},\\\\\\\\mu_{\\\\\\\\sigma(i)},\\\\\\\\Sigma_{\\\\\\\\sigma(i)}) \\\\\\\\}_{i\\\\\\\\in[k]},\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Comp}}\\\\\\\\) is the masking mechanism from Lemma 4.2 and \\\\\\\\(\\\\\\\\sigma\\\\\\\\) is a permutation chosen uniformly at random from the set of all permutations over \\\\\\\\([k]\\\\\\\\). In words, \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) applies the masking mechanism \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Comp}}\\\\\\\\) from Section 4 to each component separately and then permutes the components. To summarize the entire masking mechanism for GMMs, we provide pseudocode in Algorithm 2.\\\\n\\\\nThe following lemma asserts that \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) is indeed a masking mechanism. At a high-level, it follows by combining Lemma 4.2 with Lemma 3.5. The details can be found in Appendix G.1.\\\\n\\\\n**Lemma 5.1**.: _Let \\\\\\\\(\\\\\\\\varepsilon<\\\\\\\\ln(2)/3\\\\\\\\). There is a sufficiently large constant \\\\\\\\(C_{2}\\\\\\\\) such that for \\\\\\\\(\\\\\\\\gamma\\\\\\\\leq\\\\\\\\frac{\\\\\\\\varepsilon\\\\\\\\alpha}{C_{2}\\\\\\\\sqrt{k\\\\\\\\ln(2/\\\\\\\\delta)}\\\\\\\\sqrt{d(d+\\\\\\\\ln(12k/ \\\\\\\\beta))\\\\\\\\cdot\\\\\\\\ln(12k/\\\\\\\\delta)}}\\\\\\\\), \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) is a \\\\\\\\((\\\\\\\\gamma,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism with respect to \\\\\\\\((\\\\\\\\mathcal{F}_{\\\\\\\\textsc{Gmm}},\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}})\\\\\\\\). Moreover, \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) is \\\\\\\\((\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\)-concentrated._\\\\nNote that \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}\\\\\\\\) also satisfies a \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality since \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Comp}}\\\\\\\\) does (see Appendix G.2 for a proof).\\\\n\\\\n**Lemma 5.2**.: \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}\\\\\\\\) _satisfies a \\\\\\\\(1\\\\\\\\)-restricted \\\\\\\\((3/2)\\\\\\\\)-approximate triangle inequality._\\\\n\\\\n**Input:** GMM given by \\\\\\\\(\\\\\\\\{(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})\\\\\\\\}_{i\\\\\\\\in[k]}\\\\\\\\) and parameters \\\\\\\\(\\\\\\\\eta_{\\\\\\\\textsc{W}},\\\\\\\\eta_{\\\\\\\\textsc{Mean}},\\\\\\\\eta_{\\\\\\\\textsc{Cov}}>0\\\\\\\\)\\\\n\\\\n```\\\\n1:function\\\\\\\\(\\\\\\\\mathcal{R}_{\\\\\\\\textsc{W}}(w)\\\\\\\\)\\\\\\\\(\\\\\\\\triangleright\\\\\\\\) Noise mixing weights\\\\n2:Return\\\\\\\\(\\\\\\\\max(0,w+\\\\\\\\eta_{\\\\\\\\textsc{W}}g)\\\\\\\\) where \\\\\\\\(g\\\\\\\\sim\\\\\\\\mathcal{N}(0,1)\\\\\\\\).\\\\n3:endfunction\\\\n4:function\\\\\\\\(\\\\\\\\mathcal{R}_{\\\\\\\\textsc{Mean}}(\\\\\\\\mu,\\\\\\\\Sigma)\\\\\\\\)\\\\\\\\(\\\\\\\\triangleright\\\\\\\\) Noise mean\\\\n5:Return\\\\\\\\(\\\\\\\\mu+\\\\\\\\eta_{\\\\\\\\textsc{Mean}}g\\\\\\\\) where \\\\\\\\(g\\\\\\\\sim\\\\\\\\mathcal{N}(0,\\\\\\\\Sigma)\\\\\\\\)\\\\n6:endfunction\\\\n7:function\\\\\\\\(\\\\\\\\mathcal{R}_{\\\\\\\\textsc{Cov}}(\\\\\\\\Sigma)\\\\\\\\)\\\\\\\\(\\\\\\\\triangleright\\\\\\\\) Noise covariance\\\\n8: Let \\\\\\\\(G\\\\\\\\in\\\\\\\\mathbb{R}^{d\\\\\\\\times d}\\\\\\\\) matrix with independent \\\\\\\\(\\\\\\\\mathcal{N}(0,1)\\\\\\\\) entries.\\\\n9:Return\\\\\\\\(\\\\\\\\Sigma^{1/2}(I_{d}+\\\\\\\\eta_{\\\\\\\\textsc{Cov}}G)(I_{d}+\\\\\\\\eta_{\\\\\\\\textsc{Cov}}G)^{\\\\\\\\top} \\\\\\\\Sigma^{1/2}\\\\\\\\)\\\\n10:endfunction\\\\n11:function\\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Comp}}(w,\\\\\\\\mu,\\\\\\\\Sigma)\\\\\\\\)\\\\\\\\(\\\\\\\\triangleright\\\\\\\\) Mask component\\\\n12:Return\\\\\\\\((\\\\\\\\mathcal{R}_{\\\\\\\\textsc{W}}(w),\\\\\\\\mathcal{R}_{\\\\\\\\textsc{Mean}}(\\\\\\\\mu,\\\\\\\\Sigma), \\\\\\\\mathcal{R}_{\\\\\\\\textsc{Cov}}(\\\\\\\\Sigma))\\\\\\\\)\\\\n13:endfunction\\\\n14:function\\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{Gmm}}(\\\\\\\\{(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})\\\\\\\\}_{i\\\\\\\\in[k]})\\\\\\\\)\\\\\\\\(\\\\\\\\triangleright\\\\\\\\) Mask GMM\\\\n15: Let \\\\\\\\(\\\\\\\\sigma\\\\\\\\) be uniformly random permutation.\\\\n16:\\\\\\\\(\\\\\\\\{(\\\\\\\\hat{w}_{i},\\\\\\\\hat{\\\\\\\\mu},\\\\\\\\hat{\\\\\\\\Sigma}_{i})\\\\\\\\}\\\\\\\\leftarrow\\\\\\\\{\\\\\\\\mathcal{B}_{\\\\\\\\textsc{ Comp}}(w_{\\\\\\\\sigma(i)},\\\\\\\\mu_{\\\\\\\\sigma(i)},\\\\\\\\Sigma_{\\\\\\\\sigma(i)})\\\\\\\\}\\\\\\\\).\\\\n17: Normalize: \\\\\\\\(\\\\\\\\hat{w}_{i}\\\\\\\\leftarrow\\\\\\\\hat{w}_{i}/\\\\\\\\sum_{i\\\\\\\\in[k]}\\\\\\\\hat{w}_{i}\\\\\\\\).\\\\n18:Return\\\\\\\\(\\\\\\\\{(\\\\\\\\hat{w}_{i},\\\\\\\\hat{\\\\\\\\mu},\\\\\\\\hat{\\\\\\\\Sigma}_{i})\\\\\\\\}_{i\\\\\\\\in[k]}\\\\\\\\).\\\\n19:endfunction\\\\n```\\\\n\\\\n**Algorithm 2** GMM Masking Mechanism\\\\n\\\\n## 6Privately Learning GMMs\\\\n\\\\nAt this point, we have everything we need to develop a private algorithm for learning the parameters of a GMM. First, we define the problem more formally.\\\\n\\\\n**Definition 6.1** (PAC Learning of Parameters of GMMs).: Let \\\\\\\\(\\\\\\\\mathcal{F}=\\\\\\\\left\\\\\\\\{\\\\\\\\left(w^{j}_{i},\\\\\\\\mu^{j}_{i},\\\\\\\\Sigma^{j}_{i}\\\\\\\\right)_{i=1}^{k} \\\\\\\\right\\\\\\\\}^{j}\\\\\\\\) be a class of \\\\\\\\(d\\\\\\\\)-dimensional GMMs with \\\\\\\\(k\\\\\\\\) components5. Let \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) be function that receives a sequence \\\\\\\\(S\\\\\\\\) of instances in \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\) and outputs a mixture \\\\\\\\(\\\\\\\\hat{F}=(\\\\\\\\hat{w}_{i},\\\\\\\\hat{\\\\\\\\mu}_{i},\\\\\\\\hat{\\\\\\\\Sigma}_{i})_{i=1}^{k}\\\\\\\\). Let \\\\\\\\(m\\\\\\\\colon(0,1)^{2}\\\\\\\\times\\\\\\\\mathbb{N}^{2}\\\\\\\\to\\\\\\\\mathbb{N}\\\\\\\\). We say \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) learns the parameters of \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) with \\\\\\\\(m\\\\\\\\) samples if for every \\\\\\\\(\\\\\\\\alpha,\\\\\\\\beta\\\\\\\\in(0,1)\\\\\\\\) and every \\\\\\\\(F\\\\\\\\in\\\\\\\\mathcal{F}\\\\\\\\), if \\\\\\\\(S\\\\\\\\) is an i.i.d. sample of size \\\\\\\\(m(\\\\\\\\alpha,\\\\\\\\beta,k,d)\\\\\\\\) from \\\\\\\\(F\\\\\\\\), then \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Gmm}}(F,\\\\\\\\hat{F})<\\\\\\\\alpha\\\\\\\\) with probability at least \\\\\\\\(1-\\\\\\\\beta\\\\\\\\).\\\\n\\\\nFootnote 5: For examples, it is standard to pick \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) to be those GMMs that are separable/identifiable.\\\\n\\\\nPlugging the masking mechanism developed in Section 5 (in particular, Lemma 5.1 and Lemma 5.2) into PPE (Theorem 2.3) gives a private to non-private reduction for GMMs.\\\\n\\\\n**Theorem 6.2** (Private to Non-Private Reduction).: _Let \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) be a subclass of GMMs with \\\\\\\\(k\\\\\\\\) components in \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\). Let \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) be a non-private Algorithm that PAC learns the parameters of \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) with respect to \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Gmm}}\\\\\\\\) using \\\\\\\\(m_{\\\\\\\\textsc{non-private}}(\\\\\\\\alpha,\\\\\\\\beta,k,d)\\\\\\\\) samples. Then for every \\\\\\\\(\\\\\\\\varepsilon<\\\\\\\\ln(2)/3\\\\\\\\), \\\\\\\\(\\\\\\\\delta\\\\\\\\in(0,1)\\\\\\\\), \\\\\\\\(\\\\\\\\gamma\\\\\\\\leq\\\\\\\\frac{\\\\\\\\varepsilon\\\\\\\\alpha}{C_{2}\\\\\\\\sqrt{k\\\\\\\\ln(2/\\\\\\\\delta)}\\\\\\\\sqrt{d(d+\\\\\\\\ln(12k /\\\\\\\\beta))\\\\\\\\cdot\\\\\\\\ln(12k/\\\\\\\\delta)}}\\\\\\\\) for a sufficiently large constant \\\\\\\\(C\\\\\\\\) and \\\\\\\\(t=\\\\\\\\max\\\\\\\\{5,\\\\\\\\lceil\\\\\\\\frac{20}{\\\\\\\\varepsilon}\\\\\\\\ln(1+\\\\\\\\frac{\\\\\\\\varepsilon^{\\\\\\\\prime}-1}{2 \\\\\\\\delta})\\\\\\\\rceil\\\\\\\\}\\\\\\\\), there is a learner \\\\\\\\(\\\\\\\\mathcal{A}_{\\\\\\\\textsc{private}}\\\\\\\\) with the following properties:_\\\\n1. \\\\\\\\(\\\\\\\\mathcal{A}_{\\\\\\\\textsc{private}}\\\\\\\\) _is_ \\\\\\\\((2\\\\\\\\varepsilon,4e^{\\\\\\\\varepsilon}\\\\\\\\delta)\\\\\\\\)_-DP._\\\\n2. \\\\\\\\(\\\\\\\\mathcal{A}_{\\\\\\\\textsc{private}}\\\\\\\\) _PAC learns the parameters of_ \\\\\\\\(\\\\\\\\mathcal{F}\\\\\\\\) _using_ \\\\\\\\(O(m_{\\\\\\\\textsc{non-private}}(\\\\\\\\gamma,\\\\\\\\beta/2t,k,d)\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\varepsilon)\\\\\\\\) _samples._\\\\n3. \\\\\\\\(\\\\\\\\mathcal{A}_{\\\\\\\\textsc{private}}\\\\\\\\) _runs in time_ \\\\\\\\(O((\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\varepsilon)\\\\\\\\cdot T_{\\\\\\\\mathcal{A}}+(\\\\\\\\log(1/\\\\\\\\delta)/ \\\\\\\\varepsilon)^{2}\\\\\\\\cdot(k^{2}d^{3}+k^{3}\\\\\\\\log k))\\\\\\\\)_, where_ \\\\\\\\(T_{\\\\\\\\mathcal{A}}\\\\\\\\) _is the running time for the non-private algorithm._\\\\n\\\\nTo prove Theorem 6.2, we require the following lemma whose proof can be found in Appendix H.\\\\n\\\\n**Lemma 6.3**.: _Let \\\\\\\\(F=(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})_{i=1}^{k}\\\\\\\\) and \\\\\\\\(F^{\\\\\\\\prime}=(w^{\\\\\\\\prime}_{i},\\\\\\\\mu^{\\\\\\\\prime}_{i},\\\\\\\\Sigma^{\\\\\\\\prime}_{i})_{i=1}^{k}\\\\\\\\) be two \\\\\\\\(d\\\\\\\\)-dimensional GMMs where \\\\\\\\(\\\\\\\\Sigma_{i}\\\\\\\\) and \\\\\\\\(\\\\\\\\Sigma^{\\\\\\\\prime}_{i}\\\\\\\\) are positive-definite matrices. Suppose that \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{GMM}}\\\\\\\\left(F,F^{\\\\\\\\prime}\\\\\\\\right)<\\\\\\\\frac{1}{600}\\\\\\\\). Then \\\\\\\\(\\\\\\\\frac{1}{200}\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}(F,F^{\\\\\\\\prime})\\\\\\\\leq\\\\\\\\frac{1}{ \\\\\\\\sqrt{2}}\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}(F,F^{\\\\\\\\prime})\\\\\\\\)._\\\\n\\\\nProof of Theorem 6.2.: Let \\\\\\\\(z=3/2\\\\\\\\), \\\\\\\\(r=1\\\\\\\\), and \\\\\\\\(t\\\\\\\\geq\\\\\\\\frac{20}{\\\\\\\\varepsilon}\\\\\\\\ln\\\\\\\\left(1+\\\\\\\\frac{e^{\\\\\\\\varepsilon}-1}{2\\\\\\\\delta} \\\\\\\\right)=O(\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\varepsilon)\\\\\\\\). We run Algorithm 1 with the following.\\\\n\\\\n* For the non-private algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\), we use the algorithm from Theorem 6.5 with accuracy parameter \\\\\\\\(\\\\\\\\alpha/2z\\\\\\\\) and failure probability \\\\\\\\(\\\\\\\\beta/2t\\\\\\\\).\\\\n* For the masking mechanism, we use the \\\\\\\\((r,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)-masking mechanism \\\\\\\\(\\\\\\\\mathcal{B}_{\\\\\\\\textsc{GMM}}\\\\\\\\) which is defined in Lemma 5.1. Further, this mechanism is \\\\\\\\((\\\\\\\\alpha/2z,\\\\\\\\beta/2)\\\\\\\\)-concentrated.\\\\n* Finally, note that the distance function \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}\\\\\\\\) satisfies the \\\\\\\\(z\\\\\\\\)-approximate \\\\\\\\(r\\\\\\\\)-restricted triangle inequality (Lemma 5.2).\\\\n\\\\nLet \\\\\\\\(F^{*}\\\\\\\\) be the true GMM. Let \\\\\\\\(F_{i}\\\\\\\\) be the estimated GMMs computed by \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) in Line 2 of Algorithm 1. Then the first item above guarantees that \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}(F^{*},F_{i})\\\\\\\\leq\\\\\\\\alpha/2z\\\\\\\\) for all \\\\\\\\(i\\\\\\\\in[t]\\\\\\\\) with probability at least \\\\\\\\(1-\\\\\\\\beta/2\\\\\\\\).\\\\n\\\\nWe thus conclude that we have a private algorithm for learning GMMs that is \\\\\\\\((2\\\\\\\\varepsilon,4e^{\\\\\\\\varepsilon}\\\\\\\\delta)\\\\\\\\)-DP and that returns \\\\\\\\(\\\\\\\\widetilde{F}\\\\\\\\) satisfying \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{Param}}(\\\\\\\\widetilde{F},F^{*})\\\\\\\\leq\\\\\\\\alpha\\\\\\\\) with probability \\\\\\\\(1-\\\\\\\\beta\\\\\\\\). By Lemma 6.3, we further conclude that \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{GMM}}(\\\\\\\\widetilde{F},F^{*})\\\\\\\\leq O(\\\\\\\\alpha)\\\\\\\\) with probability \\\\\\\\(1-\\\\\\\\beta\\\\\\\\).\\\\n\\\\nIt remains to check the sample complexity and computational complexity of our algorithm. Since we run \\\\\\\\(t\\\\\\\\) independent instances of the non-private algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\), we require \\\\\\\\(t\\\\\\\\cdot m_{\\\\\\\\textsc{private}}(\\\\\\\\alpha/2z,\\\\\\\\beta/2t,k,d)=O(m_{\\\\\\\\textsc{private}}( \\\\\\\\alpha/2z,\\\\\\\\beta/2t,k,d)\\\\\\\\cdot\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\varepsilon)\\\\\\\\) samples. Finally, we bound the running time. Lemma 3.4 shows that the running time to apply the masking mechanism is \\\\\\\\(O(k\\\\\\\\cdot d^{3}+k\\\\\\\\log k)\\\\\\\\) and Lemma 3.2 shows that the running time to compute dist is \\\\\\\\(O(k^{2}d^{3}+k^{3}\\\\\\\\log k)\\\\\\\\). The claimed running time now follows from Remark 2.4. \\\\n\\\\n### Application\\\\n\\\\nAs a concrete application, we apply Theorem 6.2 with the algorithm of [14] to obtain the first private algorithm for learning the parameters of a GMM with sample and computational complexity that is polynomial in \\\\\\\\(d\\\\\\\\) (for a fixed \\\\\\\\(k\\\\\\\\)) with minimal separation assumptions. Note that our algorithm does not require any boundedness assumptions on the parameters.\\\\n\\\\n**Definition 6.4** (\\\\\\\\(\\\\\\\\gamma\\\\\\\\)-Statistically Learnable [14]).: We say a GMM \\\\\\\\(F=(w_{i},\\\\\\\\mu_{i},\\\\\\\\Sigma_{i})_{i=1}^{k}\\\\\\\\) is \\\\\\\\(\\\\\\\\gamma\\\\\\\\)-statistically learnable if (i) \\\\\\\\(\\\\\\\\min_{i}w_{i}\\\\\\\\geq\\\\\\\\gamma\\\\\\\\) and (ii) \\\\\\\\(\\\\\\\\min_{i\\\\\\\\neq j}d_{\\\\\\\\mathrm{TV}}\\\\\\\\left(\\\\\\\\mathcal{N}(\\\\\\\\mu_{i},\\\\\\\\Sigma_{i}),\\\\\\\\mathcal{N} (\\\\\\\\mu_{j},\\\\\\\\Sigma_{j})\\\\\\\\right)\\\\\\\\geq\\\\\\\\gamma\\\\\\\\).\\\\n\\\\nIf a GMM is \\\\\\\\(\\\\\\\\gamma\\\\\\\\)-statistically learnable, we will be able to recover its components accurately.\\\\n\\\\n**Theorem 6.5** (Non-private Learning of GMMs [14]).: _There exists an algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) and a function \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\) with the following guarantee. Fix \\\\\\\\(\\\\\\\\alpha,\\\\\\\\beta\\\\\\\\in(0,1)\\\\\\\\), \\\\\\\\(k,d\\\\\\\\in\\\\\\\\mathbb{N}\\\\\\\\)._\\\\n\\\\n* _For fixed_ \\\\\\\\(k\\\\\\\\)_, the sample complexity_ \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\) _is polynomial in_ \\\\\\\\(d/\\\\\\\\alpha\\\\\\\\beta\\\\\\\\)_._\\\\n* _For fixed_ \\\\\\\\(k\\\\\\\\)_,_ \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) _runs in time_ \\\\\\\\(\\\\\\\\operatorname{poly}(d/\\\\\\\\alpha\\\\\\\\beta)\\\\\\\\)_._\\\\n* _Let_ \\\\\\\\(\\\\\\\\mathcal{F}^{*}\\\\\\\\) _be an_ \\\\\\\\(\\\\\\\\alpha\\\\\\\\)_-statistically learnable subclass of GMMs with_ \\\\\\\\(k\\\\\\\\) _components in_ \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\) _and let_ \\\\\\\\(F^{*}\\\\\\\\in\\\\\\\\mathcal{F}^{*}\\\\\\\\)_. Given an i.i.d. sample_ \\\\\\\\(D\\\\\\\\) _of size_ \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta)\\\\\\\\) _drawn from_ \\\\\\\\(F^{*}\\\\\\\\)_, with probability at least_ \\\\\\\\(1-\\\\\\\\beta\\\\\\\\)_,_ \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) _return_ \\\\\\\\(\\\\\\\\hat{F}\\\\\\\\) _such that_ \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{GMM}}(\\\\\\\\hat{F},F^{*})\\\\\\\\leq\\\\\\\\alpha\\\\\\\\)_._\\\\nThe following corollary follows immediately by plugging Theorem 6.5 into Theorem 6.2.\\\\n\\\\n**Corollary 6.6**.: _There exists an algorithm \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) and a function \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\) with the following guarantee. Fix \\\\\\\\(\\\\\\\\alpha,\\\\\\\\beta,\\\\\\\\varepsilon,\\\\\\\\delta\\\\\\\\in(0,1)\\\\\\\\), \\\\\\\\(k,d\\\\\\\\in\\\\\\\\mathbb{N}\\\\\\\\)._\\\\n\\\\n* \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) _is_ \\\\\\\\((\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\)_-DP._\\\\n* _For fixed_ \\\\\\\\(k\\\\\\\\)_, the sample complexity_ \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\) _is polynomial in_ \\\\\\\\(d\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\alpha\\\\\\\\beta\\\\\\\\varepsilon\\\\\\\\)_._\\\\n* _For fixed_ \\\\\\\\(k\\\\\\\\)_,_ \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) _runs in time_ \\\\\\\\(\\\\\\\\operatorname{poly}(d\\\\\\\\log(1/\\\\\\\\delta)/\\\\\\\\alpha\\\\\\\\beta\\\\\\\\varepsilon)\\\\\\\\)_._\\\\n* _Let_ \\\\\\\\(\\\\\\\\mathcal{F}^{*}\\\\\\\\) _be an_ \\\\\\\\(\\\\\\\\alpha\\\\\\\\)_-statistically learnable subclass of GMMs with_ \\\\\\\\(k\\\\\\\\) _components in_ \\\\\\\\(\\\\\\\\mathbb{R}^{d}\\\\\\\\) _and let_ \\\\\\\\(F^{*}\\\\\\\\in\\\\\\\\mathcal{F}^{*}\\\\\\\\)_. Given an i.i.d. sample_ \\\\\\\\(D\\\\\\\\) _of size_ \\\\\\\\(m_{\\\\\\\\mathcal{A}}(d,k,\\\\\\\\alpha,\\\\\\\\beta,\\\\\\\\varepsilon,\\\\\\\\delta)\\\\\\\\) _drawn from_ \\\\\\\\(F^{*}\\\\\\\\)_, with probability at least_ \\\\\\\\(1-\\\\\\\\beta\\\\\\\\)_,_ \\\\\\\\(\\\\\\\\mathcal{A}\\\\\\\\) _return_ \\\\\\\\(\\\\\\\\hat{F}\\\\\\\\) _such that_ \\\\\\\\(\\\\\\\\operatorname{dist}_{\\\\\\\\textsc{GMM}}(\\\\\\\\hat{F},F^{*})\\\\\\\\leq\\\\\\\\alpha\\\\\\\\)_._\\\\n\\'\\n \\'# Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation\\\\n\\\\n###### Abstract\\\\n\\\\nThe quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.\\\\n\\\\nKeywords:Knot segmentation Outer-Inter relationship prediction ConvLSTM\\\\n\\\\n## 1 Introduction\\\\n\\\\nDistribution of knots within logs is one of the most important factor in wood processing chain since it determines how the log will be sliced and used. A knot is defined as a piece of a branch that is lodged in a stem and often starts at the stem path. Knots come in various dimensions, shapes and trajectories inside the trunk, these characteristics often depend on tree specie and environmental factors [15]. In wood processing, the knots are considered as defects that affect the quality of logs; hence, detecting their features such as position, size and angle of inclination are relevant and crucial for foresters and sawyers. Knowing these characteristics before the tree processing could generate a relative gain of 15-18% in value of products [2]. Nowadays, internal prediction of tree trunk density from bark observation is a complex and tedious task that requires a lot of human expertise or cannot be performed without expensive X-rays machines. In recent years, with the advent and success of deep learning, convolutional\\\\nneural networks have achieved great performances on a variety of tasks such as object detection and image classification due to their strong features extraction capabilities [7, 13]. Compared to traditional methods, the data driven deep learning based approaches learn discriminative characteristics from annotated data automatically instead of human engineering. While the era of deep learning led to significant improvements in several areas of computer vision and natural language processing, there are still only a few paper which study the interest of these approaches for forestry and wood processing industry. This is due to the lack of open data, but also due to the lack of transfer of architectures that have demonstrated their efficiency in computer vision to the specific tasks of the forestry and wood processing industry. In this paper, we propose to explore an original task that does not seem to bear resemblance with a task in another domain: predicting the inner structure from the outer appearance of a wood log. The internal knots of a wood log are a consequence of the growth of branches of the tree and there is therefore, at least for some species, a causality between the presence of an inner knot and the growth or scar of an external branch. As we will demonstrate in the paper, the deformation of the outer surface of the tree, which is the consequence of the presence of branches, allows inferring the location and shape of inner knots. Our experiments are carried on conifers for which there is a clear relationship between the growth of branch and the knots. However, for other species such as deciduous trees, this relationship is unclear, and the task remains challenging.\\\\n\\\\nTo solve the task of predicting the inner knots from the outer contour, we consider convolutional neural networks of the encoder-decoder family, where the encoder extracts features for the contour which are then used to decode the presence of a knot as a binary mask. Regularly spaced contour slices of the tree are provided as input to the network. As the presence of a knot is causally linked with a deformation of the contour due to a branch, inferring a knot needs to integrate features from the contour slices further away up or down the tree. To propagate these features between different slices, we consider convolutional LSTM, which are convolutional bidirectional recurrent neural networks [19]. A convolutional recurrent network keeps the spatial structure of the representation and extracts features along the recurrent paths by applying convolutions rather than dense matrix products. This has the benefit of reducing the cost of the net\\\\n\\\\nFigure 1: The recurrent neural network involves a recurrent encoder and feedforward decoder. The context along the slice dimension is propagated with convolutional LSTMs.\\\\nwork. In our task, this makes sense because a knot progressively diffuses within the wood as one moves along the longitudinal axis of the tree. That progressive diffusion induces that relevant features can be extracted locally, without having to resort to longer range interactions. Finally, given knots have various shapes and diffuses along a varying number of slices, using LSTMs lets the neural network learn how many slices need to be integrated to properly recover the shape of the knot. In summary, the main contribution of our work lies in two parts:\\\\n\\\\n* we propose to address an original machine learning task that is also valuable for the forestry industry, namely, the prediction of inner defects given observations of the outer deformation of a wood log,\\\\n* we demonstrate the efficiency of integrating recurrent connections in the segmentation network to solve this task.\\\\n\\\\nThe code used for running all the experiments of this paper are available on the following github repository: [https://github.com/jeremyfix/icvs2023](https://github.com/jeremyfix/icvs2023).\\\\n\\\\n## 2 Related Work\\\\n\\\\n**Semantic segmentation** is a fundamental task in computer vision where the goal is to predict the label of each pixel in an image. Deep learning architectures for this task are typically based on the auto-encoder architecture. An autoencoder consists of an encoder and a decoder. The encoder maps the input data to a lower-dimensional latent space representation, while the decoder maps the latent space back to the original input data dimension [20]. In semantic segmentation, the decoder decodes the target labels instead of reconstructing the input. Fully Convolutional Networks (FCN) [14] is an important approach in semantic segmentation and has influenced the design of modern segmentation network. Other refinements of the encoder-decoder structure, such as U-Net and SegNet, have also been proposed in the literature [18, 1].\\\\n\\\\n**Recurrent Neural Networks** have been introduced to deal with sequence data. They can learn the required size of the temporal window to gather the context required for taking a decision at any given time. The difficulty to integrate and propagate information through time, which is the foundation of the fundamental deep learning problem [10] of the vanishing/exploding gradient, has led authors to design dedicated memory units. Representatives of this family are the Long Short Term Memory networks (LSTMs) [6, 8] and Gated Recurrent Units networks (GRUs) [3].\\\\n\\\\n**Convolutional LSTM** preserves the convolutional nature of the data [19]. Indeed, the recurrent weights in the LSTMs involve dense connections and do not exploit the spatial structure of the data they process. Convolution LSTM, by considering convolutional recurrent ways, do preserve the spatial nature of data and reduces the number of parameters required in the recurrent connections. In the original paper, the convolutional LSTMs have been successfully applied to spatio-temporal sequences for weather forecasting.\\\\nIn our work, we use an encoder-decoder architecture to predict the knot distribution (binary mask) from the slices of contours of the tree. To propagate encoder features through the slices, the encoder involves recurrent connections. In order to keep the convolutional nature of the representations, the encoder involves convolutional LSTM networks. Alternatively, we could have considered a 3D convolutional encoder, but this would have fixed the size of the slice context necessary to form a prediction. Using LSTMs let the network learn which contour features influence which other contour features.\\\\n\\\\n## 3 Methodology\\\\n\\\\n### Data Preprocessing\\\\n\\\\nIn order to learn to transform the knot distribution from the contour of trees, we need aligned pairs of contours and knot masks. To build the input and target, we considered the pipelines of [11] for segmenting knots and identifying the contours by using X-rays data. Note that even though the pipelines of [11] are used to acquire data from X-rays images. The main objective of our approach is to avoid X-ray scanners and recover the external geometry from other modalities such as vision camera or laser profilers. The dataset is built from 27 fir trees and 15 spruce trees, with slices every 1.25 mm for tree sections of 1 meter long in average, which makes a total of 30100 slices. Each image is an \\\\\\\\(512\\\\\\\\times 512\\\\\\\\) that is downscaled to \\\\\\\\(256\\\\\\\\times 256\\\\\\\\) for the extraction of the contour and knot segmentation, and further downscaled to \\\\\\\\(192\\\\\\\\times 192\\\\\\\\) for the sequence models presented in this paper. Every tree is sliced in blocks of 40 consecutive slices. In the following of the paper, the axis along which the slices are stacked will be referred as either the longitudinal axis or the z-axis for short. In the experiments, we used 18 fir tree and 8 spruce tree for the training set, we used 4 fir tree and 2 spruce tree for the validation and 5 tree of each specie for the test set. Note that, each tree is represented with by 800 slices.\\\\n\\\\n### Neural network architectures without recurrent connections\\\\n\\\\nWe trained two feedforward neural networks based on U-Net [18] and SegNet [1] in order to obtain a baseline to compare with the architecture involving recurrent connections along the z-axis. Although the U-Net and SegNet do not involve recurrent connections, these have been trained on the same data as the recurrent networks, e.g., stacks of slices. This allows to guarantee that training has been performed on the same data and the metrics are computed the same way. The U-Net encoder involves fewer channels than the original network to fit with the input data. The upsampling along the decoder path is performed using a nearest-pixel policy. Along the decoding path, the encoder features are concatenated with the decoder features. The SegNet encoder involves less channels and convolutional layers than the original network. The number of blocks and channels is reduced with respect to the original SegNet because our inputs are smaller.\\\\n### Neural network architectures with recurrent connections\\\\n\\\\nIn order to propagate the contextual features of the contours in the encoder, we also consider neural network architectures with recurrent connections along the slice dimension (longitudinal axis of the tree). Recurrent connections are implemented with convolutional LSTMs which allow the network to learn which slice is impacting the features of another slice. We remind that the knots within a log can be causally linked to the presence of a branch. Instead of a fully connected LSTM, the convolutional LSTM involves fewer parameters by exploiting the spatial structure of the input data. In this paper, we consider recurrent connections only in the encoder part and not in the decoder part. The rationale is that introducing recurrent connections in the encoder allows the network to propagate contour features through the slices, and our experiments show that this is already sufficient to get good performances. These recurrent connections are bidirectional to allow information to propagate in both directions along the longitudinal axis. For the decoder, we do not add recurrent connections. That could be helpful but at a higher computational cost, and our experiments already demonstrated good performances with recurrent connections only in the encoder. The neural network architecture is depicted on Figure 1.\\\\n\\\\nThe recurrent encoder is built from 3 consecutive ConvLSTMs bidirectional blocks. Every block has the same number of memory cells than the size of the spatial dimensions times the channel dimension. The input, output, and forget gates compute their values from a convolution with kernel size 3 from the \"previous\" sequence index (here, previous is to be considered along the longitudinal z-axis and be either following upward or downward directions given we consider bidirectional LSTMs). We use the same representation depth than for the SegNet with 32, 48 and 64 channels and a maxpooling layer is placed after every ConvLSTM layer to downscale spatially the representation by a factor of 2. The decoder is not recurrent and is the same as for our SegNet, namely 3 consecutive blocks with an upsampling (nearest) followed by a \\\\\\\\(2\\\\\\\\times[Conv2D(3\\\\\\\\times 3)-BatchNorm-ReLU]\\\\\\\\) block. The final layer is a \\\\\\\\(Conv(1\\\\\\\\times 1)\\\\\\\\) to output the unnormalized scores for the classification of every pixel.\\\\n\\\\n### Evaluation metrics\\\\n\\\\nOur experiments use different quantitative metrics to evaluate the quality and the performance of our method. For the segmentation task, the ground truth output is usually very sparse and there are much more negatives than positives. Hence, we need to use evaluation metrics that are not biased due to this class imbalance. We used the Dice similarity coefficient (Dice) [5], which is also known as F1-score as overlap metric, the Hausdorff Distance (HD) [9] as distance-based metric, and the Cohen\\\\\\'s Kappa \\\\\\\\(\\\\\\\\kappa\\\\\\\\)[4, 17] as counting-based metric to evaluate the segmentation results.\\\\n\\\\nThe Hausdorff distance complements the Dice similarity because it indicates if false positives are close to a patch of positives or further away, while the Cohen\\\\\\'s Kappa indicates the agreement between ground truth and the prediction.\\\\nFor each pixel, Cohen\\\\\\'s Kappa compares the labels assigned by the model with the ground truth and measures the degree of agreement between them. The Cohen\\\\\\'s Kappa ranges from -1 to 1 where a value of 1 indicates perfect agreement between the prediction and ground truth, whereas 0 indicates a prediction which is not better than random guessing and a negative value indicates less agreement than expected by chance. The advantage of using Cohen\\\\\\'s Kappa is that it takes into account the possibility of chance agreement and provides a more accurate measure of agreement between prediction and ground truth, this is important in cases where the number of pixels assigned to each class is imbalanced.\\\\n\\\\nFor the different equations, we denote FN, FP, TP, TN respectively the number of false negatives, false positives, true positives and true negatives, where \\\\\\\\(\\\\\\\\hat{y}\\\\\\\\) is defined as final prediction computed by thresholding the output probability computed by the network (the threshold is set to 0.5 for all the experiments), and \\\\\\\\(y\\\\\\\\) the true value to be predicted (a mask, made of either 1 for a pixel belonging to a knot, or 0 otherwise). The metrics are always evaluated on the whole volume of 40 slices. As mentioned in section 3.2, even the feedforward neural networks (SegNet and UNet) are trained on the volumes. Although these networks do not propagate informations throught the longitudinal axis, training and evaluating these networks on the volume allow to have comparable measures (averaged on the same data). The value of the Hausdorff Distance is reported in millimeters. The metrics reported in the result section are averaged over the total number of volumes in the considered fold.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l||c c} Method & Dice/F1 \\\\\\\\(\\\\\\\\uparrow\\\\\\\\) HD \\\\\\\\(\\\\\\\\downarrow\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline SegNet & 0.68 & 26.18 \\\\\\\\\\\\\\\\ U-Net & 0.72 & 47.80 \\\\\\\\\\\\\\\\ \\\\\\\\hline ConvLSTM & **0.84** & **17.34** \\\\\\\\\\\\\\\\ \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 1: Left) Comparison of the segmentation methods on Dice score and HD using the validation fold. Right) Results of the SegNet and ConvLSTM models for a Fir tree specie. The first row corresponds to the input images, the second row is the associated ground truth and the bottom ones are the predictions. These samples all belong to the validation fold. Every column corresponds to one of 5 slices from different volumes.\\\\n### Other experimental hyperparameters\\\\n\\\\nFor all the experiments presented in the paper, the optimization followed the same schedule. The networks have been trained for 150 epochs with a batch size of either 10 for U-Nets and ConvLSTMs, reduced to 4 for SegNet. The parameters have been optimized with Adam [12], a base learning rate of 0.0001. The loss is the binary cross entropy. ConvLSTMs trained for one week, the U-Net and SegNet trained for almost 10 days, using two RTX 3090. The experiments were coded either with Tensorflow 2.45 or Pytorch 1.9. We used Tensorboard6 to track the experiments and log the curves (loss and the different metrics). For regularizing the ConvLSTM encoder-decoder, a dropout layer is inserted between the encoder and decoder parts with a probability of 10% to mask a neuron. Following the original papers of U-Net and SegNet, we did not insert dropout layers in these networks. In all the trainings, data augmentation is applied to the input data with a random rotation out of 8 possible angles, and horizontal flip with a probability of 0.5.\\\\n\\\\nFootnote 5: [https://www.tensorflow.org](https://www.tensorflow.org)\\\\n\\\\nFootnote 6: [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)\\\\n\\\\n## 4 Results\\\\n\\\\nIn this section, we present both quantitatively and qualitatively the performances of the various models on the prediction of knots. The results on the validation fold and test folds are provided respectively in table 1 and table 2.\\\\n\\\\nFor all the metrics, the ConvLSTM model performs better than the neural networks without recurrent connections. Looking only at the DICE and HD metrics, it seems that even without the recurrent connections, both the SegNet and U-Net perform reasonably well on the task. However, we observed qualitatively that this is not really the case as several knots are not predicted by these models. In that respect, the kappa metric seems to reflect more the difference in performance between the feedforward and recurrent networks.\\\\n\\\\nIncluding the context with the recurrent connections in the encoder provides a boost in performance. The quality of the segmentation of the recurrent network is better if we look at the Hausdorff distance, which means that the predicted masks with the ConvLSTM are closer in distance to the ground truth than with the non-recurrent segmentation networks. The Hausdorff distance is given in millimeters, and we remind that the slices are \\\\\\\\(192\\\\\\\\times 192\\\\\\\\) pixels which correspond to \\\\\\\\(192\\\\\\\\mathrm{mm}\\\\\\\\times 192\\\\\\\\mathrm{mm}\\\\\\\\). Additionally, we computed on the test set the Cohen\\\\\\'s Kappa to evaluate the agreement between the predicted masks and the ground truth. The results show that the ConvLSTM achieves a score of 0.41 for fir trees and 0.21 for spruce indicating respectively moderate agreement and fair agreement, while the non-recurrent networks score lower with Kappa values between 0.05 and 0.12 for both species indicating very weak agreement. These findings demonstrate the boost provided by the recurrent networks.\\\\nIn table 2, right, we provide the metrics of the ConvLSTM model on the different trees of the test fold, either fir or spruce. The metrics computed on individual trees are consistent with the averaged metrics computed over all the volumes and reported in table 2, left. However, some spruce trees are particularly challenging. That\\\\\\'s the case for example for the trees 4327 and 4948 which have a really unconventional contours, strongly distorted for some reason unknown to the authors. This out-of-distribution contours probably explains why the model fails to correctly predict all the knots. In addition to these averaged metrics, we provide in Figure 3 the distribution of Cohen\\\\\\'s Kappa metric computed on the test fold for both fir and spruce trees, for both the ConvLSTM and SegNet networks. We observe that the ConvLSTM model outperforms the SegNet for all trees for both species, with a clear separation between the distributions. Specifically, the ConvLSTM model achieves nearly a twofold improvement over the SegNet for almost all trees.\\\\n\\\\nAs the SegNet performs better on the test set than the U-Net, further comparison will only be made between SegNet and the ConvLSTM network. To better appreciate the difference in segmentation quality between the SegNet and ConvLSTM networks, the prediction masks of both networks on individual slices from different volumes are given in Table 1, right. On this figure, every column is a slice from a different volume of a fir tree and consecutive rows represent the input contour, the ground truth, the prediction of SegNet and the prediction of the ConvLSTM. From these 5 samples, it appears that SegNet usually underestimates knots and sometimes, knots may be even not predicted at all. For the ConvLSTM, most knots are predicted, although the knots might be overestimated in shape.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c||c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Specie} & Tree & \\\\\\\\multicolumn{4}{c}{Metrics} \\\\\\\\\\\\\\\\  & ID & Dice \\\\\\\\(\\\\\\\\uparrow\\\\\\\\) & HD \\\\\\\\(\\\\\\\\downarrow\\\\\\\\) & Kappa \\\\\\\\(\\\\\\\\uparrow\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\multirow{4}{*}{**Fir**} & 4392 & 0.72 & 14.6 & 0.28 \\\\\\\\\\\\\\\\  & 4394 & 0.75 & 16.3 & 0.29 \\\\\\\\\\\\\\\\  & 4396 & 0.78 & 8.0 & 0.52 \\\\\\\\\\\\\\\\  & 5027 & 0.84 & 6.5 & 0.50 \\\\\\\\\\\\\\\\  & 5028 & 0.78 & 8.4 & 0.53 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\multirow{4}{*}{**Spruce**} & 4327 & 0.70 & 29.0 & 0.12 \\\\\\\\\\\\\\\\  & 4328 & 0.72 & 19.2 & 0.12 \\\\\\\\\\\\\\\\ \\\\\\\\cline{1-1}  & 4329 & 0.73 & 9.1 & 0.25 \\\\\\\\\\\\\\\\ \\\\\\\\cline{1-1}  & 4948 & 0.70 & 31.0 & 0.11 \\\\\\\\\\\\\\\\ \\\\\\\\cline{1-1}  & 4990 & 0.73 & 13.6 & 0.26 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 2: Left) Comparison of the segmentation methods on Dice, HD and Kappa metrics on the test fold. Right) Quantitative results of the ConvLSTM model for the different trees of the test set. These are the same trees than the ones used for table on the left. The metrics are averaged over all the volumes of the same tree. All the trees had almost the same number of slices (from 800 to 810 slices).\\\\nThe predictions on some consecutive slices of the same volume of a tree are shown on Figures 2 for respectively a fir tree and a spruce tree. On the fir tree (left), we see part of the branch getting out from the tree, which is the anchor feature from which a network could learn the presence of an inner knot. Indeed, the ConvLSTM seems to be able to propagate information through the slices with its recurrent connections, as it is able to predict the location of a knot on the first of the five slices. It seems unlikely a network could predict the presence of a knot solely based on the first slice, given the deformation of the latter is barely visible on the contour of this first slice.\\\\n\\\\nFigure 3: Distribution of the Kappa metric on the fir and spruce trees of the test fold for both the SegNet and ConvLSTM neural networks. These are the same trees than used in table 2.\\\\n\\\\nFigure 2: Results of the SegNet and ConvLSTM models for a fir tree (left) or spruce tree specie (right) on 5 consecutive slices from the same volume. The first row corresponds to the input contours, the second row is the associated ground truth, and the two bottom rows are the predictions. These slices belong to a tree from the test set.\\\\nThe figure 4 shows a 3D representation of the contour of a fir tree from the test set, as well as the ground truth and the prediction produced by the ConvLSTM. The full tree represents a set of 803 slices, and all these slices are processed by sequences of 40 slices, with a stride of 1. From this full tree 3d representation, we observe that every knot present in the ground truth is also predicted by the ConvLSTM. It seems also that some knots may not have been correctly labeled as knots in the ground truth. This 3D representation also highlights the consistency of the knot predictions. From this representation, we also better see that there are various types of branch scars, some being clearly visible while others are more like little bumps on the surface of the tree. The smallest scars are certainly the ones for which it is the most challenging for the network to infer the location of knots, but even in some of these difficult cases, we see that the ConvLSTM model succeeds in predicting knots.\\\\n\\\\n## 5 Discussion\\\\n\\\\nIn this paper, we investigated a machine learning task that is highly valuable for the forestry industry : predicting the location of inner defects, knots, from the outside appearance of the tree. From the machine learning perspective, this task is original. We addressed this problem by training various neural network architectures of the encoder/decoder family and the most promising tested architectures are the convolutional LSTMs which benefit from recurrent connections along the longitudinal axis of the tree to propagate contour features reflecting the scars of a branch to the slices where the knots must be predicted. Although from the averaged metrics (DICE and Hausdorff), the feedforward networks (SegNet, U-Net) seem to perform well, it turns out that their predictions are pretty bad when we observe them qualitatively. This is not the case for the convolutional LSTM model, which have better metrics and clearly better segmentation of the knots when we check them visually. This discrepancy needs further investigation, and it is unclear why good classification metrics would lead to bad segmentation. The performances of the networks appear to be more contrasted by the Cohen\\\\\\'s Kappa.\\\\n\\\\nThe data used by the proposed machine learning pipeline relies on the work of [11] that extract contour and inner knots of tree logs from X-ray scans. X-ray\\\\n\\\\nFigure 4: 3D representation of the ground truth (left) and prediction of the ConvLSTM (right) viewed from the side of the tree or the top on both sides. Generated with Paraview.\\\\nscans are only essential to produce the targets but are not used by our proposed approach. The required contours for the model can be obtained using laser scanners. We have a work in progress to create a platform with calibrated lasers to extract the contour of a tree log. From a machine learning perspective, the input contours are sparse, but dense representations are used for encoding. There is room for improvement in encoding and decoding methods. Instead of using a binary mask, encoding the contour as a polygon and utilizing graph neural networks for differentiable feature learning could be more efficient. Furthermore, recent research on neural radiance fields [16] suggests the possibility of encoding a 3D volume as a parameterized function, eliminating the need to explicitly construct the 3D volume of knots. Although these ideas require experimentation, a lightweight recurrent encoding of contours that parameterizes a 3D knot density function holds promise.\\\\n\\\\n## Acknowledgment\\\\n\\\\nThis research was made possible with the support from the French National Research Agency, in the framework of the project WoodSeer, ANR-19-CE10-011.\\\\n\\'\\n \\'# Attention-based Multi-task Learning for Base Editor Outcome Prediction\\\\n\\\\n###### Abstract\\\\n\\\\nHuman genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model\\\\\\'s predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models\\\\\\' capacity to enhance and accelerate the process of refining base editing designs.\\\\n\\\\nMachine Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning Multi-task Learning, Multi-task Learning, Multi-task Learning, Multi-task Learning Multi-task\\\\nwhere we directly learn the probability distribution over all possible outcome sequences for a given target sequence. The second one is a two-stage model where we first estimate the probability of the given target sequence being edited, acknowledging that in many cases, the editor fails and no changes are observed which is often referred to as wild-type outcome. We then proceed to estimate the probability distribution of edited outcomes.\\\\n\\\\nDifferent editors exhibit varying behaviors on the same target sequences due to factors like binding affinities and editing window sizes, introducing distributional shifts. In response to this challenge, we introduce a multi-task learning framework. Rather than training individual models for each editor, as current models do, we propose a unified model capable of simultaneously accommodating multiple editors.\\\\n\\\\nIn this work, we study the different modeling strategies for training machine learning models for the base editor outcome prediction task. We explore the spectrum of modeling choices evaluated on multiple datasets and base editors. A key highlight is the proposed unified multi-task model that is capable of learning from various base editors without necessitating training separate models for each setup. We train our models on six libraries corresponding to the outcomes of six base editors applied on thousands of target sites (Table 2). Our models\\\\\\' predictions show a good correlation with the ground truth across all datasets demonstrating the potential of machine learning in guiding and exploring genome editing space.\\\\n\\\\n## 2 Related Work\\\\n\\\\nIn recent years, the intersection of deep learning and CRISPR-Cas9 systems has witnessed substantial interest from the bioinformatics community. Researchers have explored the applications of deep learning in predicting various aspects of CRISPR-Cas9 systems, including predicting gRNA activities (Amenen et al., 2021; Xie et al., 2023; Zhang et al., 2021) and editing outcomes for both base editing and prime editing scenarios (Mathis et al., 2023).\\\\n\\\\nAmong those, one notable approach is the BE-Hive proposed by (Arbab et al., 2020), which aims to predict base editing outcomes and efficiencies while considering sequence context, PAM compatibility, and cell-type-specific factors. The model employs a gradient boosting tree for predicting overall editing efficiency and a deep conditional autoregressive model for predicting probability of edited outcome sequences (denoted by bystander efficiency). Similarly, (Song et al., 2020) presented DeepABE and DeepCBE, that is based on convolutional neural networks to model both overall editing efficiency and bystander efficiency of adenine and cytosine base editors.\\\\n\\\\nRecently, Marquart et al. (2021) proposed BE-DICT, which predicts per-base editing efficiency (i.e. editing efficiency of each target base in a sequence) and bystander base-editing efficiency using attention-based deep learning. In a latest comprehensive study, (Kim et al., 2023) developed DeepCas9variants and DeepBEs to predict editing efficiencies and outcomes of various BEs, taking into account different Cas9 variants. They build on and adapt the models proposed in (Song et al., 2020) (i.e. convolutional networks) to generate predictions for a range of CRISPR-Cas9 systems.\\\\n\\\\nWhile the surge of interest in applying machine learning to CRISPR-Cas9 systems is clear in recent literature, it\\\\\\'s noteworthy that many of these works have a primary emphasis on designing CRISPR-Cas9 systems under various conditions and less focused on the analysis of ML models without offering a holistic and systematic analysis of model design. Given the intricate nature of CRISPR-Cas9 systems and the multitude of model paradigms adopted, deriving concrete conclusions about optimal model design strategies remains elusive. In this context, our work aims to serve as model-first work that presents the base editing outcome prediction through a modeling lens. We focus on model development and provide a systematic analysis of each component of the models, providing a structured framework for problem formulation and model design specifically tailored to the prediction of base editing outcomes. Through this structured examination of these critical aspects, our aim is to lay the groundwork for more informed and refined approaches for using deep learning models to assist the design of base editors.\\\\n\\\\n## 3 Method\\\\n\\\\nBase editor and related conceptsBase editors (BEs) are created by fusing the Cas9 protein with DNA-modifying enzymes. They are directed by a 20-base pair guiding RNA molecule (sgRNA) that acts as a GPS to locate and bind to a matching DNA segment known as the _protospacer_. The effectiveness of BEs largely depends on the composition of this protospacer sequence. BEs, in tandem with the sgRNA, can only bind to the DNA if there\\\\\\'s a _protospacer adjacent motif_ (PAM) - a sequence consisting of 2-6 nucleotides - present adjacent to the protospacer. This PAM sequence further influences the activity of BEs. There are two primary types of base editors: adenine base editors (ABEs) (presented in figure 1), which convert adenine (A) to guanine (G), and cytosine base editors (CBEs) that chemically convert cytosine (C) to thymine (T). A detailed description of the base editor is provided in the appendix section 6.1.1.\\\\n\\\\n### Data representation\\\\n\\\\nAssume we have a target (reference) DNA sequence denoted as \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}=[x_{1},x_{2},\\\\\\\\ldots,x_{T}]\\\\\\\\) where \\\\\\\\(x_{i}\\\\\\\\in\\\\\\\\{A,C,G,T\\\\\\\\}\\\\\\\\), and a set of DNA sequences \\\\\\\\(\\\\\\\\mathbf{X}_{\\\\\\\\text{out}}=[\\\\\\\\mathbf{x}_{\\\\\\\\text{out},1},\\\\\\\\mathbf{x}_{\\\\\\\\text{out},2}, \\\\\\\\ldots,\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}]\\\\\\\\in\\\\\\\\mathbb{R}^{T}\\\\\\\\). The _target_ of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},1}\\\\\\\\) is the target of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}\\\\\\\\), and the target of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}\\\\\\\\) is the target of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}\\\\\\\\). The target of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}\\\\\\\\) is the target of the DNA sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},M}\\\\\\\\).\\\\n\\\\\\\\(\\\\\\\\mathbb{R}^{M\\\\\\\\times T}\\\\\\\\) representing corresponding outcomes when a specific base editor is applied to the reference sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\). The associated probabilities for these outcomes are given by \\\\\\\\(\\\\\\\\mathbf{y}=[y_{1},y_{2},\\\\\\\\dots,y_{M}]\\\\\\\\) where \\\\\\\\(y_{i}=P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\in[0,1]\\\\\\\\), _for_\\\\\\\\(i=1,2,\\\\\\\\dots,M\\\\\\\\), indicating the likelihood of obtaining outcome \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\) through editing of \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\). Here, \\\\\\\\(T\\\\\\\\) is the length of the reference sequence, and \\\\\\\\(M\\\\\\\\) represents the total number of possible outcomes for a given reference sequence. The count of outcomes can vary depending on the reference sequence. An example of a reference sequence and associated outcome sequences is represented in Figure 2.\\\\n\\\\nIn this paper, we use bold uppercase letters for matrices (\\\\\\\\(\\\\\\\\mathbf{X}\\\\\\\\)), bold lowercase letters for vectors or sequences (\\\\\\\\(\\\\\\\\mathbf{x}\\\\\\\\)), and regular non-bold letters for scalars or tokens. We use \\\\\\\\(P\\\\\\\\) for probability distributions and non-bold uppercase letters (\\\\\\\\(X\\\\\\\\)) for random variables. To represent the reference sequence, we consider protospacer, PAM, and overhangs. Here, \"overhangs\" refer to adjacent nucleotides on both sides of the protospacer.. To declutter the notation, we will mainly use \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\) to denote the reference sequence which could refer to one of these representations: (a) protospacer, (b) protospacer + PAM, or a (c) left overhangs + protospacer + PAM + right overhangs where + is the concatenation operator. Respectively, the outcome sequences are the DNA sequences with the same length as the reference sequence and with a modification of the target bases at the protospacer. The outcome sequence identical to the reference sequence (no edits) is referred as the wild-type.\\\\n\\\\nThe training dataset comprises \\\\\\\\(N\\\\\\\\) pairs, each containing a reference sequence, its associated outcomes, and the corresponding probabilities, denoted as \\\\\\\\(D=\\\\\\\\{\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^{i},\\\\\\\\mathbf{X}_{\\\\\\\\text{out}}^{i},\\\\\\\\mathbf{y}^{i}\\\\\\\\}_ {i=1}^{N}\\\\\\\\). For simplicity, when referring to a specific reference sequence and its outputs, we omit the instance-level indexing and use only \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\).\\\\n\\\\n### Problem formulation\\\\n\\\\nOur objective is to predict the likelihood of potential outcomes resulting from a specific base editor applied to a reference sequence.One approach would be formulating it as a generative model where we directly model the condition distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) that we can both sample different outcomes for a given reference sequence and calculate the probability of each outcome. However, unlike typical generative models that must learn to generate entire output sequences, our scenario benefits from already knowing a portion of the output sequences. Due to the base editor\\\\\\'s specific targeting of A-to-G or C-to-T transformations, a substantial portion of the output sequence remains consistent with the reference sequence, with only a few positions undergoing alteration.\\\\n\\\\nIn the inference phase, for a given reference sequence, we can efficiently generate all possible outcomes by considering only the edit combination of target bases (A/G) within the protospacer. By traversing through a range of possible edits, we cover the entire landscape of potential outcome sequences. Therefore, we only need to learn the distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) such that we can evaluate the probability of a specific outcome for a given reference sequence \\\\\\\\(P(X_{\\\\\\\\text{out}}=\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\).\\\\n\\\\nOne-stage ModelIn this setup, we tackle the problem by learning a function \\\\\\\\(f(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i})\\\\\\\\rightarrow\\\\\\\\hat{y}_{i}\\\\\\\\) where \\\\\\\\(i=1,\\\\\\\\dots,M\\\\\\\\), and \\\\\\\\(\\\\\\\\sum_{i=1}^{M}\\\\\\\\hat{y}_{i}=1\\\\\\\\), that takes as input the reference sequence and one of its corresponding outcome and learns to approximate the probability of obtaining that specific outcome. Notably, this function \\\\\\\\(f\\\\\\\\) characterizes a categorical distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}=\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\sim Cat(M, \\\\\\\\hat{\\\\\\\\mathbf{y}})\\\\\\\\), where \\\\\\\\(\\\\\\\\hat{\\\\\\\\mathbf{y}}\\\\\\\\) is the vector containing probabilities for M outcomes. To learn the function \\\\\\\\(f\\\\\\\\), we propose to use attention-based encoder blocks to learn the encoding of both the\\\\n\\\\nFigure 1: Adenine base editor\\\\n\\\\nFigure 2: An example of a reference sequence of 20 bases (i.e. nucleotides) and associated outcome sequences when applying ABEmax base editor. The first row represents the reference (target) sequence, and the second row is the outcome sequence with no modification (i.e. wild-type) with a probability of occurrence of 0.52. The third row represents a possible outcome sequence where the letter A is changed to G at position 5 with a probability of 0.35. The rest of the rows represent all possible changes of the reference sequence targeting letters A to G with their associated probabilities.\\\\nreference sequence and output sequence. Subsequently, we apply a prediction model on the learned encoded representation to output the probability of obtaining the outcome. The network architecture to learn \\\\\\\\(f\\\\\\\\) is reported in figure 3 (B: proportion model). However, there is a relatively higher probability often associated with the wild-type outcome (\\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}=\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\)), while the probabilities linked to the edited outcome sequences are often very small. This situation presents a challenge when directly modeling \\\\\\\\(P(X_{\\\\\\\\text{out}}|x_{\\\\\\\\text{ref}})\\\\\\\\)--as the model might easily learn the wild-type probability but struggle with outcomes that have extremely low probabilities.\\\\n\\\\n### Two-stage model\\\\n\\\\nTo address this, we propose a two-stage model where we break down \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) as the product of two probabilities:\\\\n\\\\n\\\\\\\\[P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})=\\\\\\\\begin{cases}P(\\\\\\\\mathbf{x} _{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\text{edited})P(\\\\\\\\text{edited}|\\\\\\\\mathbf{ x}_{\\\\\\\\text{ref}}),\\\\\\\\\\\\\\\\ \\\\\\\\text{if }\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\neq\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\\\\\\\\\ 1-P(\\\\\\\\text{edited}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}),\\\\\\\\text{if }\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}= \\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\end{cases} \\\\\\\\tag{1}\\\\\\\\]\\\\n\\\\nFor a given reference sequence, we first predict the probability of overall efficiency which is defined in Eq. 2. It provides the probability of the target sequence being edited, \\\\\\\\(P(edited|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\), which in turn gives the probability of the wild-type. Next, we predict the probability of all possible edited outcomes, \\\\\\\\(P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},edited)\\\\\\\\). We refer to the first as _overall efficiency_ and the second as _proportion_\\\\n\\\\n\\\\\\\\[P(edited|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})=\\\\\\\\frac{\\\\\\\\text{Sum of the read count of all edited reads for the target}}{\\\\\\\\text{Total read count of the target sequence}} \\\\\\\\tag{2}\\\\\\\\]\\\\n\\\\nWe estimate the overall efficiency of the given reference sequence using \\\\\\\\(f_{\\\\\\\\theta_{1}}(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) (Eq. 3), denoted by the overall efficiency model, and subsequently, we predict the conditional probabilities of all non wild-type outcomes using \\\\\\\\(f_{\\\\\\\\theta_{2}}(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i})\\\\\\\\) (Eq. 4) which we denote by the proportion model.\\\\n\\\\n\\\\\\\\[f_{\\\\\\\\theta_{1}}(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})=P(edited|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}) \\\\\\\\tag{3}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(P(\\\\\\\\text{\\\\\\\\emph{wild-type}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})=1-P(edited|\\\\\\\\mathbf{x}_{ \\\\\\\\text{ref}})\\\\\\\\)\\\\n\\\\n\\\\\\\\[f_{\\\\\\\\theta_{2}}(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i})=P(\\\\\\\\mathbf{ x}_{out,i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\text{\\\\\\\\emph{edited}}), \\\\\\\\tag{4}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\neq\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\)\\\\n\\\\nOnce \\\\\\\\(f_{\\\\\\\\theta_{1}}\\\\\\\\) and \\\\\\\\(f_{\\\\\\\\theta_{2}}\\\\\\\\) are learned, we can calculate \\\\\\\\(P(X=\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) where \\\\\\\\(i=1,\\\\\\\\ldots M\\\\\\\\) for all outcome sequences, including wild-type and edited sequences using Eq 1.\\\\n\\\\n#### 3.3.1 Overall efficiency model\\\\n\\\\nWe formulate the overall efficiency model as a probabilistic classification task where \\\\\\\\(f_{\\\\\\\\theta_{1}}\\\\\\\\) parameterizes a binomial distribution \\\\\\\\(P(C|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) of a random variable \\\\\\\\(C\\\\\\\\in\\\\\\\\{\\\\\\\\textit{edited},\\\\\\\\textit{not edited}\\\\\\\\}\\\\\\\\) with the aim to learn to output the \\\\\\\\(P(C=edited|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) for a given reference sequence. To learn \\\\\\\\(f_{\\\\\\\\theta_{1}}\\\\\\\\), we first computed the overall editing efficiency for each reference sequence by summing all probabilities attributed to the non wild-type outcomes as given in Eq 2, or equivalently, \\\\\\\\(1-P(\\\\\\\\textit{wild-type}|\\\\\\\\mathbf{x}_{ref})\\\\\\\\). Then we use multiple 1D-Convolutional layers (LeCun et al., 1995) on the one-hot-encoded representation of \\\\\\\\(\\\\\\\\mathbf{x}_{ref}\\\\\\\\) to learn discriminative feature embedding that is passed to the multi-layer perceptron (MLP) layer to approximate the distribution \\\\\\\\(P(C|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\). The model architecture is presented in Figure 3 (A). We trained \\\\\\\\(f_{\\\\\\\\theta_{1}}\\\\\\\\) using KL-divergence loss that is applied on the true distribution \\\\\\\\(P(C|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) and learned distribution \\\\\\\\(\\\\\\\\hat{P}_{\\\\\\\\theta_{1}}(C|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) for each reference sequence.\\\\n\\\\n\\\\\\\\[\\\\\\\\mathcal{L}_{\\\\\\\\textit{efficiency}}(\\\\\\\\theta_{1},D)=\\\\\\\\sum_{i=1}^{N}D_{kl}(P(C| \\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^{i})\\\\\\\\|\\\\\\\\hat{P}_{\\\\\\\\theta_{1}}(C|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^ {i})) \\\\\\\\tag{5}\\\\\\\\]\\\\n\\\\n#### 3.3.2 Proportion model\\\\n\\\\nThis model is designed to approximate the conditional distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited})\\\\\\\\). To achieve this, we first remove the wild-type from each reference sequence\\\\\\'s corresponding output \\\\\\\\(X_{\\\\\\\\text{out}}\\\\\\\\). Then, we normalize the probabil\\\\n\\\\nFigure 3: Two-stage Model overview\\\\nities of the remaining outcomes to ensure a valid distribution effectively converting \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) into the distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited})\\\\\\\\). The proportion model \\\\\\\\(f_{\\\\\\\\theta_{2}}\\\\\\\\) is designed to learn the parameters governing the distribution \\\\\\\\(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited})\\\\\\\\). Similar to the one-stage model, \\\\\\\\(f_{\\\\\\\\theta_{2}}\\\\\\\\) is provided with both the reference sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}\\\\\\\\) and its associated outcome sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\). The model is then trained to estimate the likelihood \\\\\\\\(P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\mid\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited})\\\\\\\\), representing the probability of reference sequence being edited, and result in the outcome sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\).\\\\n\\\\nAs illustrated in Figure 3 (B), \\\\\\\\(f_{\\\\\\\\theta_{2}}\\\\\\\\) uses attention-based model comprised of two encoder networks, \\\\\\\\(\\\\\\\\text{Enc}^{1}(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\), \\\\\\\\(\\\\\\\\text{Enc}^{2}(\\\\\\\\mathbf{x}_{\\\\\\\\text{out}})\\\\\\\\), and one output network \\\\\\\\(g\\\\\\\\). The design of the encoder networks adapts the transformer encoder blocks architecture (Vaswani et al., 2017), characterized by multiple layers of multi-head self-attention modules. The two encoder networks process the reference sequence and one of its corresponding output sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\), leading to the extraction of their respective latent representations, namely \\\\\\\\(\\\\\\\\mathbf{Z}_{\\\\\\\\text{ref}}\\\\\\\\in\\\\\\\\mathbb{R}^{T\\\\\\\\times d}\\\\\\\\) and \\\\\\\\(\\\\\\\\mathbf{Z}_{\\\\\\\\text{out}}\\\\\\\\in\\\\\\\\mathbb{R}^{T\\\\\\\\times d}\\\\\\\\). Both vectors are then concatenated to form a unified learned representation \\\\\\\\(\\\\\\\\mathbf{Z}\\\\\\\\in\\\\\\\\mathbb{R}^{T\\\\\\\\times 2d}\\\\\\\\). Subsequently, the output network \\\\\\\\(g\\\\\\\\) embeds this unified representation \\\\\\\\(\\\\\\\\mathbf{Z}\\\\\\\\) to compute the probability of obtaining the output sequence given the reference sequence, \\\\\\\\(P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\mid\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited})\\\\\\\\).\\\\n\\\\nPrecisely, the output network \\\\\\\\(g(\\\\\\\\mathbf{Z})\\\\\\\\) takes as input the final representation \\\\\\\\(\\\\\\\\mathbf{Z}\\\\\\\\in\\\\\\\\mathbb{R}^{T\\\\\\\\times 2d}\\\\\\\\) and performs an affine transformation followed by softmax operation to compute the probability of conversion of every target base (i.e. base A or C depending on the chosen base editor) as it is shown below:\\\\n\\\\n\\\\\\\\[\\\\\\\\hat{y}_{it}=\\\\\\\\sigma(\\\\\\\\mathbf{W}\\\\\\\\mathbf{z}_{it}+\\\\\\\\mathbf{b}_{t}) \\\\\\\\tag{6}\\\\\\\\]\\\\n\\\\nwhere \\\\\\\\(\\\\\\\\mathbf{W}\\\\\\\\in\\\\\\\\mathbb{R}^{2\\\\\\\\times 2d}\\\\\\\\), \\\\\\\\(\\\\\\\\mathbf{b}_{t}\\\\\\\\in\\\\\\\\mathbb{R}^{2}\\\\\\\\) and \\\\\\\\(\\\\\\\\sigma\\\\\\\\) is softmax function. \\\\\\\\(\\\\\\\\hat{y}_{it}\\\\\\\\) represents the probability of editing occurring at the \\\\\\\\(t\\\\\\\\)-th position in the \\\\\\\\(i\\\\\\\\)-th outcome sequence. The un-normalized probability for the whole \\\\\\\\(i\\\\\\\\)-th output sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\) given its reference sequence is computed by \\\\\\\\(\\\\\\\\hat{y}_{i}=\\\\\\\\prod_{t=1}^{T}\\\\\\\\hat{y}_{i,t}\\\\\\\\), which is then normalized across all the outcomes to make it valid probability distribution (Eq. 7). Therefore, the approximated probability for obtaining \\\\\\\\(i\\\\\\\\)-th edited (non-wild type) outcome sequence is given by:\\\\n\\\\n\\\\\\\\[\\\\\\\\hat{P}(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},i}\\\\\\\\mid\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},\\\\\\\\textit{edited} )=\\\\\\\\frac{\\\\\\\\hat{y}_{i}}{\\\\\\\\sum_{i=1}^{M}\\\\\\\\hat{y}_{i}} \\\\\\\\tag{7}\\\\\\\\]\\\\n\\\\nObjective FunctionWe used the Kullback-Leibler (KL) divergence on the model\\\\\\'s estimated distribution over all outcome sequences for a given reference sequence \\\\\\\\(\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^{i}\\\\\\\\) and the actual distribution:\\\\n\\\\n\\\\\\\\[D_{\\\\\\\\text{KL}}^{i}(P(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^{i}, \\\\\\\\textit{edited})||\\\\\\\\hat{P}_{\\\\\\\\theta_{2}}(X_{\\\\\\\\text{out}}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^ {i},\\\\\\\\textit{edited})) \\\\\\\\tag{8}\\\\\\\\] \\\\\\\\[=\\\\\\\\sum_{j=1}^{M_{i}}P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},j}|\\\\\\\\mathbf{x}_{\\\\\\\\text{ ref}}^{i},\\\\\\\\textit{edited})\\\\\\\\log\\\\\\\\frac{P(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},j}|\\\\\\\\mathbf{x}_{ \\\\\\\\text{ref}}^{i},\\\\\\\\textit{edited})}{\\\\\\\\hat{P}_{\\\\\\\\theta_{2}}(\\\\\\\\mathbf{x}_{\\\\\\\\text{out},j }|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}}^{i},\\\\\\\\textit{edited})}\\\\\\\\]\\\\n\\\\nLastly, the objective function for the whole training set is defined by the average loss across all the reference sequences as follows:\\\\n\\\\n\\\\\\\\[\\\\\\\\mathcal{L}_{\\\\\\\\text{proportion}}(\\\\\\\\theta_{\\\\\\\\mathbf{2}};D)= \\\\\\\\tag{9}\\\\\\\\] \\\\\\\\[\\\\\\\\sum_{i=1}^{N}D_{KL}^{i}(P(X_{\\\\\\\\text{out}}\\\\\\\\mid\\\\\\\\mathbf{x}_{\\\\\\\\text{ ref}}^{i},\\\\\\\\textit{edited})||\\\\\\\\hat{P}_{\\\\\\\\theta_{2}}(X_{\\\\\\\\text{out}}\\\\\\\\mid\\\\\\\\mathbf{x}_{ \\\\\\\\text{ref}}^{i},\\\\\\\\textit{edited})\\\\\\\\]\\\\n\\\\nThe final objective is composed of both the overall efficiency model loss and the proportion model loss with a weight regularization term (i.e. \\\\\\\\(l_{2}\\\\\\\\)-norm regularization) applied to the model parameters represented by \\\\\\\\(\\\\\\\\theta=\\\\\\\\{\\\\\\\\theta_{\\\\\\\\mathbf{1}},\\\\\\\\theta_{\\\\\\\\mathbf{2}}\\\\\\\\}\\\\\\\\) (Eq. 10)\\\\n\\\\n\\\\\\\\[\\\\\\\\mathcal{L}_{\\\\\\\\text{proportion}}(\\\\\\\\theta_{\\\\\\\\mathbf{1}};D)+\\\\\\\\mathcal{L}_{\\\\\\\\text{ efficiency}}(\\\\\\\\theta_{\\\\\\\\mathbf{2}},D)+\\\\\\\\frac{\\\\\\\\lambda}{2}\\\\\\\\|\\\\\\\\theta\\\\\\\\|_{2}^{2} \\\\\\\\tag{10}\\\\\\\\]\\\\n\\\\n### Multi-task learning with multiple base editors\\\\n\\\\nMulti-task learningThere is a diverse set of base editors, each distinguished by its unique design attributes. These distinctions, including variations in binding affinities, editing window sizes, and deaminase activities, result in differing editing efficiencies even when targeting the same sequence. The variations in editing efficiency across different editors emphasize the complexity of the base editing landscape. Conventional approaches have often proposed training separate models for each editor. However, this approach not only demands additional effort but also fails to leverage the shared structural similarities among editors. To leverage common patterns and relationships present across various libraries derived from different base editors, and optimize predictive capability while also reducing computational time, we propose a more efficient solution based on multi-task learning. Instead of training separate models for each editor, we train a single model capable of predicting the efficiency of all editors when applied to the same reference sequence.\\\\n\\\\nGiven a total number of \\\\\\\\(D\\\\\\\\) editors where each editor has its own dataset \\\\\\\\(B_{i}\\\\\\\\) (denoted by screening libraries), we developed a multi-task learning model that uses shared encoding\\\\n\\\\nFigure 4: Multi-task learning model overview\\\\nlayers to extract a common representation across all the libraries as well as individual branches that fine-tune the model specifically for each library, ensuring a better fit to their respective data. This approach implicitly models \\\\\\\\(P(X_{\\\\\\\\text{out}}\\\\\\\\mid\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}},B_{i})\\\\\\\\) where \\\\\\\\(B_{i}\\\\\\\\) represents the base editor type applied on the reference sequence. To implement one universal model across all datasets, we extend our proposed two-stage model architecture (illustrated in Figure 3) for multi-task learning, as depicted in Figure 4.\\\\n\\\\nSpecifically, we modify the overall efficiency model by initially employing two convolutional layers as a shared building block across all datasets/editors, enabling the learning of a common representation for the reference sequence. Then a set of output blcoks is used to represent editor-specific transformation. Each editor type has its own output block consisting of two layers of convolutional network followed by MLP layers to predict the probability \\\\\\\\(P(edited|\\\\\\\\mathbf{x}_{\\\\\\\\text{ref}})\\\\\\\\) for each editor/dataset accordingly.\\\\n\\\\nWe adapt the proportion model by using a common encoder network across all editors/datasets to establish a unified representation \\\\\\\\(\\\\\\\\mathbf{Z}_{\\\\\\\\text{ref}}\\\\\\\\) for the reference sequence while using separate encoders and output blocks for each distinct editor. To counterbalance any bias towards larger datasets, we implemented a data loader that uniformly samples the same number of data samples in each mini-batch throughout the training phase.\\\\n\\\\n## 4 Experiments\\\\n\\\\n### Dataset and Experiment setup\\\\n\\\\nDatasetTo comprehensively assess base editors\\\\\\' efficiency across thousands of genomic sequences, we conducted high-throughput screening, resulting in the creation of six distinct datasets. Each dataset corresponds to the application of one of the following base editors: SpRY-ABESe, SpCas9-ABESe SpG-ABE8e, SpRY-ABEmax, SpCas9-ABEmax, and SpG-ABEmax, as listed in Table 1. Detailed descriptions of the used editors are provided in Appendix Section 6.2. Each dataset encompasses approximately 11,000 reference sequences and their corresponding output sequences. In each dataset, we leveraged 193 distinct PAM sites, each comprising four nucleotide bases.\\\\n\\\\nExperiment setupWe divided every dataset into training, testing, and validation sets, maintaining a ratio of 80%, 10%, and 10%. This procedure is repeated three times to ensure robust performance reporting. All reported results are based on the average performance over the three runs(indicated by \\\\\\\\(mean\\\\\\\\pm std\\\\\\\\)). First, we use the one-stage model to identify the best features to represent reference sequence for predicting the base editing outcomes (i.e. determine reference sequence representation option as explained in section 3.2). Using the selected features (i.e., protospacer + PAM), we proceed to evaluate the performance between the one-stage and two-stage models. Finally, using the two-stage model, we compare the multi-task learning (i.e. a unified model training for all editors) to the single-task learning setup where separate models are trained for the different editors.\\\\n\\\\nThroughout model training, we track the epoch at which the best validation scores are attained. Evaluation of the trained models for each base editor is based on their average performance on the test sets across the three runs. Pearson and Spearman correlation were used as performance measures for all tested models. More details about the network structure, optimization, and hyperparameters are presented in the Appendix Section 6.3.\\\\n\\\\n### Experiment results\\\\n\\\\nReference sequence representationExisting models have explored different factors that could affect the base editor\\\\\\'s efficiency, which we categorize into three scenarios: 1) the protospacer, 2) the protospacer along with its PAM, and 3) an extended range including left overhangs, protospacer, PAM, and right overhangs. We investigate all three scenarios with the one-stage model to identify the best features to represent the reference sequence. As shown in Table 2, we observe that incorporating PAM information significantly enhances performance, whereas the inclusion of overhangs demonstrates minimal impact. Besides, adding overhangs increases the computational complexity drastically. Consequently, we opt to employ protospacer and PAM information to represent reference sequences in all the subsequent model results presented below.\\\\n\\\\nComparing One-stage with Two-stage ModelAs detailed in Section 3.2, our model can be conceptualized as either a one-stage model, directly capturing the distribution across all potential outcomes for a given reference, or as a two-stage model. The latter approach involves initially\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c c} \\\\\\\\hline Editor & \\\\\\\\#ins & \\\\\\\\#refseq & \\\\\\\\#outcome & mean & std \\\\\\\\\\\\\\\\ \\\\\\\\hline SpRY-ABESe & 110141 & 11291 & 9.7 & 0.102 & 0.211 \\\\\\\\\\\\\\\\ SpCas9-ABESe & 43054 & 11337 & 4.6 & 0.217 & 0.323 \\\\\\\\\\\\\\\\ SpG-ABESe & 80873 & 11307 & 7.1 & 0.139 & 0.263 \\\\\\\\\\\\\\\\ SpRY-ABEmax & 70851 & 11347 & 6.2 & 0.159 & 0.301 \\\\\\\\\\\\\\\\ SpCas9-ABEmax & 39606 & 11302 & 3.5 & 0.285 & 0.417 \\\\\\\\\\\\\\\\ SpG-ABEmax & 70851 & 11347 & 6.2 & 0.159 & 0.301 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 1: Data statistics: “#ins” refers to the number of reference and output sequence pairs, “#refseq” denotes the number of distinct reference sequences, “#outcome” denotes the average number of outcomes per reference sequence, the mean and std refers to the mean and standard deviation of the probability across all the outcomes.\\\\npredicting the probability of an edit occurring in the reference sequence, followed by predicting the probabilities of individual edited outcomes. In this section, we present results for both models to illustrate the advantages of the two-stage approach over the one-stage counterpart. For the one-stage model, we use exactly the same architecture as the proportion model from the two-stage model on the original data without preprocessing where we remove the wild type and renormalize the probability for each reference.\\\\n\\\\nTable 3 shows the two-stage model has slightly superior results (Spearman correlation) over the one-stage model. This improvement can be attributed to the model\\\\\\'s two-step prediction approach, which first predicts the wild-type alone and subsequently refines predictions for various edited outcomes. To better understand the difference between the two models\\\\\\' ability to predict the wild-type and edited outcome sequences, we rigorously evaluated each model\\\\\\'s performance separately on both types of outcome. The two-stage model outperforms the one-stage model in most of the datasets when considering both wild type and edited outcomes as presented in Table 4 and 5.\\\\n\\\\nMulti-task learningGiven this conclusion, we proceed with the multi-task learning with the two-stage mode (see Figure 4). We compared the performance of multi-task learning across all the datasets/editors with a single-task setup where we trained one model per dataset/editor. Table 6 reports similar performance for both models. Although there wasn\\\\\\'t a substantial performance difference, adopting a unified multi-task model offers advantages such as reduced run-time (for training and inference) and smaller model size (fewer parameters) while maintaining consistent performance across all datasets. Moreover, with a unified model, we can simultaneously predict the editing outcomes of all six editors at once for a given target sequence.\\\\n\\\\nComparing to baselines in the literatureWe also compared our model with BE-DICT (Marquart et al., 2021) which is one of the relevant existing models that tackle base editing outcome prediction. BE-DICT is a sequence-to-sequence model where the decoding happens in an auto-regressive manner, it is computationally heavy compared to our proposed method. Moreover, it is trained as single-task model (i.e. one model for each editor) and use only the protospacer to represent the target sequence. We extended and retrained BE-DICT on two of the datasets (randomly chosen) and compared the prediction results with ours. For a fair comparison, we first used our one-stage model trained in the single task setting (one model per dataset) using only the protospacer. The results of this experiment reveal the advantages of the architectural changes, particu\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{Single task learning} & \\\\\\\\multicolumn{2}{c}{Multi task learning} \\\\\\\\\\\\\\\\ Libraries & Spearman & Pearson & Spearman & Pearson \\\\\\\\\\\\\\\\ \\\\\\\\hline SpKP-ABEmax & \\\\\\\\(0.872\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.986\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.872\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.986\\\\\\\\pm 0.0002\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEmax & \\\\\\\\(0.879\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.864\\\\\\\\pm 0.0019\\\\\\\\) & \\\\\\\\(0.992\\\\\\\\pm 0.0001\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEmax & \\\\\\\\(0.882\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.991\\\\\\\\pm 0.0006\\\\\\\\) & \\\\\\\\(0.889\\\\\\\\pm 0.0016\\\\\\\\) & \\\\\\\\(0.992\\\\\\\\pm 0.0004\\\\\\\\) \\\\\\\\\\\\\\\\ SpKP-ABEbe & \\\\\\\\(0.861\\\\\\\\pm 0.0029\\\\\\\\) & \\\\\\\\(0.974\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.863\\\\\\\\pm 0.0011\\\\\\\\) & \\\\\\\\(0.975\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEbe & \\\\\\\\(0.856\\\\\\\\pm 0.008\\\\\\\\) & \\\\\\\\(0.938\\\\\\\\pm 0.0005\\\\\\\\) & \\\\\\\\(0.852\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.937\\\\\\\\pm 0.003\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEbe & \\\\\\\\(0.856\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.980\\\\\\\\pm 0.0008\\\\\\\\) & \\\\\\\\(0.871\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.979\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 6: Performance comparison between the multi-task and single task learning models\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{} & \\\\\\\\multicolumn{2}{c}{One-stage Model} & \\\\\\\\multicolumn{2}{c}{Two-stage Model} \\\\\\\\\\\\\\\\ Libraries & Spearman & Pearson & Spearman & Pearson \\\\\\\\\\\\\\\\ \\\\\\\\hline SpKP-ABEmax & \\\\\\\\(0.745\\\\\\\\pm 0.015\\\\\\\\) & \\\\\\\\(0.711\\\\\\\\pm 0.011\\\\\\\\) & \\\\\\\\(0.798\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.872\\\\\\\\pm 0.012\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEmax & \\\\\\\\(0.82\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.851\\\\\\\\pm 0.014\\\\\\\\) & \\\\\\\\(0.838\\\\\\\\pm 0.009\\\\\\\\) & \\\\\\\\(0.890\\\\\\\\pm 0.030\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEmax & \\\\\\\\(0.807\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.752\\\\\\\\pm 0.014\\\\\\\\) & \\\\\\\\(0.845\\\\\\\\pm 0.011\\\\\\\\) & \\\\\\\\(0.822\\\\\\\\pm 0.014\\\\\\\\) \\\\\\\\\\\\\\\\ SpKP-ABEbe & \\\\\\\\(0.393\\\\\\\\pm 0.021\\\\\\\\) & \\\\\\\\(0.508\\\\\\\\pm 0.025\\\\\\\\) & \\\\\\\\(0.547\\\\\\\\pm 0.056\\\\\\\\) & \\\\\\\\(0.699\\\\\\\\pm 0.051\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEbe & \\\\\\\\(0.855\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.840\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.806\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.858\\\\\\\\pm 0.0021\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEbe & \\\\\\\\(0.712\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.732\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.774\\\\\\\\pm 0.005\\\\\\\\) & \\\\\\\\(0.810\\\\\\\\pm 0.009\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 3: Prediction performance all outcomes (i.e. including wild-type sequences).\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{} & \\\\\\\\multicolumn{2}{c}{Protospacer \\\\\\\\& PAM} & \\\\\\\\multicolumn{2}{c}{Protospacer \\\\\\\\& PAM \\\\\\\\& Overhang} \\\\\\\\\\\\\\\\ Libraries & Spearman & Pearson & Spearman & Pearson & Pearson \\\\\\\\\\\\\\\\ \\\\\\\\hline SpKP-ABEmax & \\\\\\\\(0.835\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.981\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.854\\\\\\\\pm 0.006\\\\\\\\) & \\\\\\\\(0.983\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.854\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.983\\\\\\\\pm 0.002\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap89-ABEmax & \\\\\\\\(0.786\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.978\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.881\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.0005\\\\\\\\) & \\\\\\\\(0.891\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEmax & \\\\\\\\(0.841\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.985\\\\\\\\pm 0.0007\\\\\\\\) & \\\\\\\\(0.866\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.0003\\\\\\\\) & \\\\\\\\(0.878\\\\\\\\pm 0.008\\\\\\\\) & \\\\\\\\(0.991\\\\\\\\pm 0.0009\\\\\\\\) \\\\\\\\\\\\\\\\ SpRY-ABEbe & \\\\\\\\(0.776\\\\\\\\pm 0.019\\\\\\\\) & \\\\\\\\(0.965\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.779\\\\\\\\pm 0.0036\\\\\\\\) & \\\\\\\\(0.968\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.803\\\\\\\\pm 0.008\\\\\\\\) & \\\\\\\\(0.967\\\\\\\\pm 0.0003\\\\\\\\) \\\\\\\\\\\\\\\\ SpCas9-ABEbe & \\\\\\\\(0.762\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.883\\\\\\\\pm 0.005\\\\\\\\) & \\\\\\\\(0.857\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.945\\\\\\\\pm 0.0006\\\\\\\\) & \\\\\\\\(0.862\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.945\\\\\\\\pm 0.003\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEbe & \\\\\\\\(0.803\\\\\\\\pm 0.005\\\\\\\\) & \\\\\\\\(0.963\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.820\\\\\\\\pm 0.005\\\\\\\\) & \\\\\\\\(0.974\\\\\\\\pm 0.0009\\\\\\\\) & \\\\\\\\(0.819\\\\\\\\pm 0.006\\\\\\\\) & \\\\\\\\(0.9771\\\\\\\\pm 0.0008\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 2: Pearson and Spearman correlation using one-stage Model across the three different reference sequence representations. In our experiment, we chose 5 neighboring nucleotides for both sides to represent the overhangs.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{} & \\\\\\\\multicolumn{2}{c}{One-stage Model} & \\\\\\\\multicolumn{2}{c}{Two-stage Model} \\\\\\\\\\\\\\\\ Libraries & Spearman & Pearson & Spearman & Pearson \\\\\\\\\\\\\\\\ \\\\\\\\hline SpKP-ABEmax & \\\\\\\\(0.851\\\\\\\\pm 0.006\\\\\\\\) & \\\\\\\\(0.983\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.873\\\\\\\\pm 0.001\\\\\\\\) & \\\\\\\\(0.986\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEmax & \\\\\\\\(0.881\\\\\\\\pm 0.006\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.0005\\\\\\\\) & \\\\\\\\(0.879\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.991\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEmax & \\\\\\\\(0.865\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.989\\\\\\\\pm 0.0003\\\\\\\\) & \\\\\\\\(0.887\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.991\\\\\\\\pm 0.0006\\\\\\\\) \\\\\\\\\\\\\\\\ SpK-ABEbe & \\\\\\\\(0.779\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.968\\\\\\\\pm 0.002\\\\\\\\) & \\\\\\\\(0.862\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.974\\\\\\\\pm 0.001\\\\\\\\) \\\\\\\\\\\\\\\\ SpCap-ABEbe & \\\\\\\\(0.857\\\\\\\\pm 0.007\\\\\\\\) & \\\\\\\\(0.945\\\\\\\\pm 0.0006\\\\\\\\) & \\\\\\\\(0.856\\\\\\\\pm 0.003\\\\\\\\) & \\\\\\\\(0.597\\\\\\\\pm 0.002\\\\\\\\) \\\\\\\\\\\\\\\\ SpG-ABEbe & \\\\\\\\(0.820\\\\\\\\pm 0.005\\\\\\\\) & \\\\\\\\(0.974\\\\\\\\pm 0.0009\\\\\\\\) & \\\\\\\\(0.865\\\\\\\\pm 0.004\\\\\\\\) & \\\\\\\\(0.978\\\\\\\\pm 0.0008\\\\\\\\) \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 3: Prediction performance all outcomes (i.e. including wild-type sequences).\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{} & \\\\\\\\multicolumn{\\\\nlarly in adopting an encoder-encoder architecture over the traditional sequence-to-sequence (encoder-decoder) model. Then we extended their model to use both the protospacer and PAM as the reference sequence and compared it with our proposed multi-task model (trained on eight data sets using protospacer and PAM information as target sequence).\\\\n\\\\nResults in Table 7 show that our model consistently outperforms BE-DICT. Furthermore, considering computational efficiency during model training (on SpRY-ABE8e data using the protospacer and PAM reference sequence representation), BE-DICT takes in the order of minute per epoch (wall time), while our single-task model accomplishes the same task in the order of seconds ( 15 seconds). Notably, the multi-task learning model trained jointly on all six datasets takes 21 seconds per epoch.\\\\n\\\\nThis highlights the benefits of avoiding the complex sequence-to-sequence architecture in favor of a streamlined encoder-encoder structure. This choice not only improves the computational efficiency but also leads to model predictive performance improvements. Moreover, the introduction of a two-stage model and a multi-task framework amplifies these performance gains even further. We present additional results for comparisons with other baselines in Table 8 in the appendix.\\\\n\\\\nTo assess our model\\\\\\'s performance against other state-of-the-art models, we conducted evaluations using the test sets provided by these models. Table 8 displays our findings, which include three most recent models: BE-HIVE (Arbab et al., 2020), DeepABE (Song et al., 2020), and BEDICT (Marquart et al., 2021), along with their respective test sets labeled as A. et al., S. et al., and M. et al.\\\\n\\\\nThe idea is to take the published trained model and evaluate their performance on those various test sets. For the three baseline models, we refer to the results reported in the BEDICT paper. As for our model, to ensure fairness in comparison, we used our single-stage model trained on SpG-ABEmax libraries since most baselines, except DeepABE, do not incorporate the PAM as input. The results correspond to two scenarios: 1) considering all possible outcomes, and 2) only considering non-wild type outcomes. The results for the non-wild type outcomes correspond to the model prediction where we only consider non-wild outcomes. In the case of non-wild-type outcome prediction, we mention that other models were trained exclusively on non-wild outcomes, with outcomes per sequence being renormalized. Our one-stage model, however, was trained on data encompassing all outcomes, so we report non-wild-type results with outcomes renormalized for a fair comparison.\\\\n\\\\n## 5 Conclusion\\\\n\\\\nOur work provides a detailed assessment of the modeling approaches for base editor outcome prediction. Through the development of a unified model, we transcend the limitations of single-editor models and pave the way for more versatile and comprehensive tools. By combining self-attention mechanisms and multi-task learning, we capture the nuances of editing outcomes across various editors, enhancing the accuracy and applicability of our predictions.\\\\n\\\\nAs the first machine learning-focused paper in the domain of base editor outcome prediction, our work represents a stepping stone toward a more systematic and informed modeling approach to genome editing. We explored the different modeling decisions from one-stage to two-stage models, and from single-task to multi-task learning. We evaluated the different sequence representations and benchmarked our best model with one of the main models developed for base editing outcome prediction. We believe that further work studying systematically the different modeling decisions for genome editing will help guide researchers toward more promising editing strategies that in turn will bring advancements in gene therapy and disease modeling.\\\\n\\\\nFor the future, given the current absence of standardized and systematic benchmark datasets in the field, we aim to bridge this gap by creating standard benchmark datasets, establishing baseline models, and proposing better performance metrics. This initiative will provide the machine-learning community with a solid foundation for testing a wide array of innovative ideas and approaches.\\\\n\\\\n## Acknowledgements\\\\n\\\\nWe thank G. Schwank, K. Marquart, L. Kissling and S. Janjuha for input on the CRISPR-Cas and base editing technology and for data sharing and preprocessing. This work was supported by the URPP \\\\\\'Human Reproduction Reloaded\\\\\\' and \\\\\\'University Research Priority Programs\\\\\\'.\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l l l l l l} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multicolumn{2}{c}{} & \\\\\\\\multicolumn{3}{c}{BE-DICT} & \\\\\\\\multicolumn{3}{c}{Ours} \\\\\\\\\\\\\\\\ reference sequence & Libraries & Spearman & Person & Spearman & Person \\\\\\\\\\\\\\\\ \\\\\\\\hline protospacer & SpRY-ABE8e & 0.801 & 0.943 & 0.835 & 0.981 \\\\\\\\\\\\\\\\  & SpRY-ABE8e & 0.746 & 0.861 & 0.776 & 0.965 \\\\\\\\\\\\\\\\  & SpRY-ABE8e & 0.804 & 0.951 & 0.870 & 0.987 \\\\\\\\\\\\\\\\  & SpRY-ABE8e & 0.762 & 0.850 & 0.860 & 0.975 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 7: Performance comparison with the baselines\\\\n\\\\n\\\\\\\\begin{table}\\\\n\\\\\\\\begin{tabular}{l c c c c c c} \\\\\\\\hline \\\\\\\\hline  & \\\\\\\\multicolumn{3}{c}{All Outcomes} & \\\\\\\\multicolumn{3}{c}{Non wild-types} \\\\\\\\\\\\\\\\ Datasets & A.et al & S. et al & M. et al & A.et al & S. et al & M. et al \\\\\\\\\\\\\\\\ \\\\\\\\hline BEDICT & 0.96 & 0.94 & 0.86 & 0.81 & 0.90 & 0.82 \\\\\\\\\\\\\\\\ DeepABE & 0.86 & 0.93 & 0.8 & 0.86 & 0.96 & 0.84 \\\\\\\\\\\\\\\\ BE-HIVE & 0.71 & 0.88 & 0.74 & 0.92 & 0.93 & 0.81 \\\\\\\\\\\\\\\\ Our model & 0.972 & 0.974 & 0.972 & 0.939 & 0.945 & 0.953 \\\\\\\\\\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\\\n\\\\\\\\end{table}\\\\nTable 8: Model performance on the test set from the different published studies. Columns represent test sets, rows represent models used\\']'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df['markdown'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \".\"]\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=separators, chunk_size=1000, chunk_overlap=5)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts_paper = text_splitter.create_documents(df['markdown'].values)\n",
    "\n",
    "# Embeddings\n",
    "hg_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma_rag/'\n",
    "economic_langchain_chroma = Chroma.from_documents(\n",
    "    documents=texts_paper,\n",
    "    collection_name=\"humanizer_data\",\n",
    "    embedding=hg_embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
